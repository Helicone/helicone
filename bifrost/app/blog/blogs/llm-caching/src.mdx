LLM inference can be a bottleneck in production environments. It requires careful optimization in production environments, as it often leads to high computational costs, long latency, and inefficiency.

One of the most effective ways to tackle this challenge is through caching—an approach that reduces the need to recompute repetitive operations, saves on API calls, and minimizes latency.

![LLM Caching](/static/blog/llm-caching/llm-caching-strategy-cover.png)

In this post, we'll dive into the caching strategies that can have a major impact on the performance of your LLM-powered systems.

You will learn about:

- How caching eliminates patterns of redundancy in LLM systems
- KV-Caching to reduce inference costs
- Prompt caching to avoid recomputing common inputs
- Solutions from LLM providers (OpenAI, Google, and Anthropic), and how to use their built-in caching mechanisms
- Caching LLM responses (exact vs semantic caches)
- Monitoring and best practices

## Redundant Patterns in LLM Systems

LLM systems inherently involve repetitive computations that contribute to inefficiencies and higher resource usage. These redundancies can span different layers of the system's architecture, each adding to the overall computational load.

Understanding these redundancies is key to optimizing LLM performance, and caching provides an effective solution to reduce unnecessary computations.

Let's break down the common sources of redundancy at each level of the LLM workflow:

### 1. Autoregressive Token Generation (Recompute on Token)

In autoregressive models, each new token generated depends on the preceding tokens. As the model generates tokens one by one, it needs to recalculate the context for every new token, leading to redundant computation. This can be particularly inefficient when generating long sequences where earlier tokens do not change.

<BottomLine
  title="Solution"
  description="Caching token-level context (such as KV pairs) can prevent the need to recompute the same information repeatedly for each token, significantly reducing the computational burden."
/>

### 2. Stateless Architecture (Recompute on Prompt)

Many LLMs operate without persistent memory, meaning they do not retain previous interactions or data once a session ends. As a result, every new prompt includes the entire conversation history or context, requiring the model to recompute the entire sequence each time.

<BottomLine
  title="Solution"
  description="By using prompt caching or context caching, you can store and reuse previously seen context, avoiding the need to reprocess the same conversation history across multiple interactions. This leads to faster processing and lower compute costs."
/>

### 3. Application-Specific Behavior (Recompute on Functionality)

LLMs in applications often perform repetitive operations with well-defined behaviors, such as generating responses to similar queries or applying the same set of instructions across multiple invocations. These tasks often require recalculating the same functions or processing the same data multiple times.

<BottomLine
  title="Solution"
  description="Caching repeated queries or responses at the application layer allows the system to bypass redundant computation. This can significantly reduce the number of API calls and lower operational costs."
/>

We can remedy the issue at each level through caching, and a combination of such efforts at all levels would help minimize our applications' inference costs.

Next, we will explore the token level first, with an introduction to KV-Cache.

## KV Caching: Token-Level Solution

When generating a new token, LLMs have to consider all the tokens that came before to determine how well the new piece will fit into the semantic, syntactic, and pragmatic context. The calculated context becomes obsolete, however, right after a new addition, requiring the LLM to recalculate all the ways each token affects the others.

### How KV Caching Works

In transformers, for each token, two main pieces of information are kept:

1. The individual meaning (as embedding vectors in multidimensional space)
2. Ways in which the token "may" relate to other tokens (as sets of weight matrices to adjust vectors on the "contextual space")

There are three sets of weight matrices:

- **Key (K):** encodes what makes a token influenced by others
- **Value (V):** represents how a token influences others
- **Query (Q):** determines how much attention a token pays to others

(Note that these are rough, high-level descriptions.)

The idea behind KV caching is to store each token's "potentiality" (the Key and Value matrices) for reuse when calculating actualized effects at iterations.

There are optimization methods and different types of KV-caching mechanisms (such as KCache or FastGen), but here, we will assume the LLM service handles these internals, as major providers currently offer them under various names such as prompt or context caching.

While KV caching optimizes token-level computations, prompt caching extends this efficiency to entire conversation histories or repeated system instructions.

## Prompt Caching: Expanding to Entire Conversations

### Use Cases & Practical Examples

There are certain scenarios where we embed a lot of the same information in repeated prompts. These could be:

- **To keep session history:** In multi-turn conversations (e.g., for a chatbot), the conversation history has to be kept as a prefix to the prompt input.
- **To dictate nuanced behavior:** We may need to pass lengthy system instructions or tool definitions in each prompt—for example, to ensure consistent and appropriate behavior for a chatbot or when instructing methods of sentiment analysis for a movie review platform.
- **To provide referral text:** We may need to include a large corpus of data (e.g., product manuals for customer support bots, or codebase summaries for programming assistants).

Caching provides significant improvements in these scenarios. Instead of repetitively processing instruction tokens, system prompts, or user manuals, the LLM can consult the cache for the K-V values of these tokens, reducing compute, latency, and cost.

This can also be a great way to improve RAG systems. It functions as a lightweight alternative to fine-tuning since it lets us include many few-shot examples in the prompt.

## LLM Service Options

Major LLM providers (OpenAI, Google Gemini, and Anthropic Claude) offer built-in caching support for their models through their APIs, with significantly reduced costs for cache reads.

While this is the case, there are certain constraints, such as:

### Minimum Token Requirements:

- Gemini: 32,768 tokens
- Claude 3.5 Sonnet and Claude 3 Opus: 1,024 tokens
- Claude 3.5 Haiku and Claude 3 Haiku: 2,048 tokens

### Time-to-Live (TTL) Considerations:

- Claude enforces a 5-minute TTL
- Gemini offers configurable TTL settings

It is important to read the latest documentation for these constraints and consider your application’s needs. For example, at their current status:

#### Gemini’s Approach Suits:

- Long-form content caching (e.g., entire knowledge bases or documentation)
- Scenarios where content updates are infrequent
- Applications that benefit from persistent caching (e.g., customer support systems with stable product documentation)
- Cases where the overhead of caching large chunks is justified by frequent access

#### Claude’s Approach Works Better For:

- Short-lived, intensive processing periods
- Rapid iteration scenarios (e.g., development and testing)
- Bursty workloads where the same context is used multiple times in quick succession
- Applications that can refresh cache entries programmatically every few minutes

Note that this involves a trade of compute for storage. The provider may charge less for cache reads yet more for cache writes. In addition, storage durations can increase costs.

## Caching LLM Responses

While prompt caching significantly reduces the cost per API call, creating a cache reserve of common queries and their inferred responses at the application layer can further minimize these calls, eliminate costs, reduce latency, and support scalability.

There are many "caching frameworks" that store prompts and responses as key-value pairs. LangChain has an extensive list [here](https://python.langchain.com/docs/integrations/llm_caching/).

Most of these frameworks support two distinct cache types:

- **Standard Cache:** Matches on the exact string (or the hash value) of the prompt.
- **Semantic Cache:** Matches based on the “semantic similarity” of prompts.

## Best Practices for Context Caching

To best utilize context caching, the system should:

- **Adapt prompts to “trigger” caching:** Usually, this means starting prompts with similar content with identical strings of text. Providers typically check the prompt prefix up to a certain breakpoint. For example, if you’re doing RAG, place the retrieved context at the end of the prompt and keep instructions at the top.
- **Track cache performance:** LLM services report when cached tokens are used. Keep track of cache hits and associated costs to ensure the benefit is measurable.
- **Provide its own caching capabilities:** This, we will discuss next.

### Why Implement an Additional Cache Layer?

- **Resilience to Minor Prompt Variations:** Even a single-character difference can cause a cache miss with standard prompt caching. Semantic caches help mitigate small discrepancies.
- **Further Savings on Output Tokens:** While providers charge less for cached input tokens, output tokens often cost the same. By caching entire responses, additional model calls are eliminated, reducing overall costs.
- **More Sophisticated Cache-Invalidation Mechanisms:** Updates can synchronize with data to keep responses up to date—going beyond a simple TTL. Context-aware triggers can invalidate or refresh specific entries whenever relevant information changes.
- **Ad Hoc Patches for Common Queries:** For frequently asked questions, serving a polished, pre-composed response may be preferable to generating one.
- **Improved Intent Recognition with Semantic Caching:** By analyzing embeddings, user intent can be understood more precisely.

## Best Practices and Monitoring

Optimizing LLM systems through caching requires understanding the application's specific needs and usage patterns. Caching strategies need to evolve alongside the application, with continuous monitoring of key performance indicators such as cache hit rates, latency improvements, and cached token percentages across system layers.

### Key Steps for Optimization:

- Adjusting cache sizes based on usage patterns
- Fine-tuning TTL settings for optimal cache freshness
- Restructuring prompts to maximize cache utilization
- Balancing cache storage costs against compute savings

Tools like **Helicone.ai** provide comprehensive insights into cache performance and other LLM metrics, supporting data-driven optimization. This ensures caching mechanisms enhance rather than hinder overall performance, maintaining efficiency across token-, prompt-, and application-level caches.

## Key Takeaways (AI generated)

- Caching is a powerful tool for optimizing LLM performance across various system layers.
- KV caching optimizes token-level computations, while prompt caching extends this efficiency to entire conversation histories or repeated system instructions.
- Major LLM providers offer built-in caching support through their APIs, with significant cost savings for cache reads.
- Implementing an additional cache layer can further minimize costs, reduce latency, and support scalability.
- Best practices include adapting prompts to trigger caching, tracking cache performance, and providing its own caching capabilities.
- Tools like Helicone.ai provide comprehensive insights into cache performance and other LLM metrics, supporting data-driven optimization.

### You might be interested in:

- [5 Powerful Ways to Optimize LLM Costs](https://www.helicone.ai/blog/slash-llm-cost)
- [A Developer's Guide to Preventing Prompt Injection](https://www.helicone.ai/blog/preventing-prompt-injection)
- [Best Practices for AI Developers](https://www.helicone.ai/blog/ai-best-practices)

<CallToAction
  title="Get Started with Helicone"
  description="Helicone is a platform that helps you optimize LLM performance through caching and other techniques. Get started today and see the difference it can make for your application."
  primaryButtonText="Get Started"
  primaryButtonLink="/"
  secondaryButtonText="Learn More"
  secondaryButtonLink="/"
/>

<Questions />
