Crafting high-quality prompts and evaluating them has become a pressing issue for AI teams. A well-designed prompt can elicit highly relevant, coherent, and useful responses, while a suboptimal one can lead to irrelevant, incoherent, or even harmful outputs. To create high-performing prompts, teams need both high-quality input variables and clearly defined tasks. However, since LLMs are inherently unpredictable, quantifying the impact of even small prompt changes is extremely difficult.

![Prompt Evaluation for Large Language Models: Golden Datasets vs. Random Sampling](/static/blog/prompt-evaluation-for-llms/cover.webp)


In a recent QA Wolf webinar, Nishant Shukla, the senior director of AI at QA Wolf, and Justin Torre, the CEO and co-founder of Helicone, shared their insights on how they tackled the challenge of effective prompt evaluation.


## The Old Approach: Curating Golden Datasets

To address this challenge, teams have traditionally turned to the use of Golden Datasets. Golden Datasets are widely used because they are great for benchmarking and evaluating problems that are well-defined and easily reproducible.

Golden Datasets are meticulously cleaned, labeled, and verified. Teams use Golden Datasets to make sure their applications perform optimally under controlled conditions. 


## The Limitation of the Golden Dataset Approach

While using Golden Datasets to evaluate prompts can work well for certain applications, it’s not the best suited for today's generative AI products that demands speed. In the webinar, Nishant and Justin explained some significant drawbacks:

### 1. Rapid prompt iteration outpaces dataset m**aintenance**

In fast-moving AI companies like QA Wolf, prompts are constantly being updated and improved. By the time a Golden Dataset is created, the prompts have often already changed, rendering the dataset obsolete.

### 2. **Overfitting due to lack of g**eneralization and performance drift

Golden Datasets tend to be simplified or idealized examples that may not accurately reflect the true complexity and variability of real-world data. This can lead to overfitting and poor generalization to production scenarios.

### 3. Curation and m**aintenance results in high** costs and scaling challenges

Keeping a Golden Dataset up-to-date and representative of the evolving data and prompt structures is an ongoing and resource-intensive task.


## The New Approach: Random Sampling for Prompt Evaluation

To overcome these limitations, QA Wolf and Helicone have turned to a more innovative approach: **random sampling of production data**. By randomly selecting a small subset of actual user interactions and using that as the evaluation benchmark, they've been able to achieve several key benefits:

### 1. Agility and Faster Iteration

The ability to quickly test prompt changes against real-world data allows QA Wolf to iterate and improve their multi-agent system more efficiently, without being bogged down by the maintenance overhead of a Golden Dataset.

### 2. Real-world Representation

Random sampling ensures that the evaluation data accurately reflects the complexity and diversity of actual user interactions, reducing the risk of overfitting and improving the generalization of the AI agents.

### 3. Cost-effectiveness

By sampling a subset of production data rather than evaluating against the entire dataset, QA Wolf has been able to significantly reduce the computational resources and costs associated with prompt evaluation.


## Implement Random Sampling in Helicone

To implement this random sampling approach, QA Wolf has partnered with Helicone, a platform that specializes in managing and optimizing LLM-powered workflows. Helicone's ability to log and track all production data, combined with its experimentation capabilities, has been a key enabler for QA Wolf's success.

![Prompt Evaluation for Large Language Models: Golden Datasets vs. Random Sampling](/static/blog/prompt-evaluation-for-llms/cover.webp)

As Justin explained, Helicone allows QA Wolf to randomly sample production data, use it to iterate on prompt definitions, and then evaluate the impact of those changes against the real-world benchmark. This iterative process helps the QA Wolf team converge on prompts that are well-aligned with actual user needs and behaviors.


## Conclusion

| **Golden Datasets** | **Random Sampling** |
| --- | --- |
| • Meticulously cleaned and labeled data
• Controlled conditions
• High maintenance costs
• Risk of overfitting (Model performing well on training data but poorly on new data)  
• Slower iteration due to dataset updates | • Real-world production data
• Reflects actual user interactions
• Cost-effective
• Better generalization
• Faster iteration cycles |
|  |  |

The QA Wolf and Helicone collaboration showcases how taking different approaches to prompt evaluation can enable progression for AI teams. By using random sampling of production data instead of using Golden Datasets, teams can become more agile and more cost-effective. This case study is a valuable example as the AI landscape evolves as companies building better observability tools continue to improve developer’s workflow for prompt engineering and evaluation.

---

### Questions or feedback?

Are the information out of date? Do you have additional platforms to add? Please raise an issue and we’d love to share your insights!
