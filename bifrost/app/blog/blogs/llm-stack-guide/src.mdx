As seassoned Developers we are all familiar with the rise and falls of many tech stacks.
Whether it's the rise of the MERN stack, the fall of the AngularJS, or the rise of the Jamstack, the tech stack has always been a part of the web development process.

The LLM Stack is a new breed of tech stack that is designed to help developers build and run LLM applications.

## Why do we need a new stack?

LLM applications are really easy to get started, however once you begin to start scaling you quickly run into the limitations of the platform, and the need for tooling, observability, security, and more arise.

We wrote a complimentary blog that shows a [simple example of an LLM application](/blog/building-an-llm-stack) that goes from zero to hero and shows how the stack evolves.

<div>dfk;lk</div>

# The LLM Stack

| Service       | Products                                                                                                                                           |
| ------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- |
| Vector DB     | - [Helicone Vector](/products/helicone-vector)<br/>- [Pinecone](https://www.pinecone.io/)<br/>- [Weaviate](https://www.weaviate.io/)               |
| Observability | - [Helicone Observability](/products/helicone-observability)<br/>- [Pinecone](https://www.pinecone.io/)<br/>- [Weaviate](https://www.weaviate.io/) |

# Where Helicone sits in the stack

![](/static/pasted_images/llm-stack-helicone-in-picture.png)

## Observability

![](/static/pasted_images/llm-stack-helicone-observability.png)

## Gateway

![](/static/pasted_images/llm-stack-helicone-gateway.png)

## Experiments

![](/static/pasted_images/llm-stack-helicone-experiments.png)

### LLM Stack for - Hobbiest

- Side projects
- Internal tools
- Small classifier
- Sub-feature within a larger application

For these users we typically recommend just to call the model itself. You can get 99% of the way there for most cases just prompting the model directly.
You probably don't need to use a tooling layer, you can probably just use classic string formatting libraries to format your prompts, and simple logs to help you debug.

### LLM Stack for - Startups

2024-08-04-23-16-24.png

### LLM Stack for - Small to Medium Businesses

### LLM Stack for - Enterprise Scale Enterprises
