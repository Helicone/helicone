As seasoned developers, we are all familiar with the rise and fall of various tech stacks. Whether it's the rise of the MERN stack, the decline of AngularJS, or the emergence of the Jamstack, tech stacks have always been integral to the web development process.

The LLM Stack represents a new breed of tech stack designed to help developers build and run LLM applications effectively.

## Why Do We Need a New Stack?

While LLM applications are easy to start with, scaling them often reveals limitations in the platform. This leads to an increased need for tooling, observability, security, and more.

We wrote a complementary blog that illustrates a [simple example of an LLM application](/blog/building-an-llm-stack) that evolves from zero to hero, showcasing how the stack develops.

# Common LLM Architecture

The following architecture diagram depicts the most common components of an LLM stack. This simplified view is derived from the example [outlined here](/blog/building-an-llm-stack).

![](/static/pasted_images/llm-stack-the-modern-stack.png)

Let's break this down into its components.

### Observability Layer

<img
  src="/static/pasted_images/observability-layer.png"
  alt="Observability Layer"
  width="50%"
/>

| Service         | Products (Examples)                                                                                                                            |
| --------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |
| Observability   | - [Helicone](/products/helicone-observability)<br/>- LangSmith<br/>- LangFuse<br/>- Traceloop<br/>- PromptLayer<br/>- HoneyHive<br/>- AgentOps |
| Clients         | - LangChain<br/>- LiteLLM<br/>- LlamaIndex                                                                                                     |
| Agents          | - CrewAI<br/>- AutoGPT                                                                                                                         |
| Prompting Layer | - [Helicone Prompting](https://docs.helicone.ai/features/prompts)<br/>- PromptLayer<br/>- PromptFoo                                            |
| Integrations    | - Zapier<br/>- LlamaIndex                                                                                                                      |

### Inference Layer

<img
  src="/static/pasted_images/inference-layer.png"
  alt="Inference Layer"
  width="70%"
/>

| Service           | Products (Examples)                                                                                                                          |
| ----------------- | -------------------------------------------------------------------------------------------------------------------------------------------- |
| Gateway           | - [Helicone Gateway](https://docs.helicone.ai/getting-started/integration-method/gateway)<br/>- Cloudflare AI<br/>- PortKey<br/>- KeywordsAI |
| LLM Load Balancer | - Martian<br/>- LiteLLM                                                                                                                      |
| Model Providers   | - OpenAI<br/>- Anthropic<br/>- TogetherAI<br/>- Anyscale<br/>- Groq                                                                          |

### Testing / Experiments

![](/static/pasted_images/testing-layer.png)

| Service         | Products (Examples)                                                                                                                         |
| --------------- | ------------------------------------------------------------------------------------------------------------------------------------------- |
| Datastores      | - [Helicone API](https://docs.helicone.ai/rest/request/post-v1requestquery)<br/>- LangFuse<br/>- LangSmith                                  |
| Experimentation | - [Helicone Experiments](https://docs.helicone.ai/use-cases/experiments#how-to-run-llm-prompt-experiments)<br/>- Promptfoo<br/>- Braintrust |
| Evaluators      | - PromptFoo<br/>- Lastmile<br/>- BaseRun                                                                                                    |
| Fine Tuning     | - OpenPipe<br/>- Autonomi                                                                                                                   |

# Where Helicone Fits in the Stack

If you are a user of Helicone, you may want to explore the various areas where Helicone can integrate within your LLM application. Our primary focus is on Gateway and Observability.

![](/static/pasted_images/llm-stack-helicone-in-picture.png)
