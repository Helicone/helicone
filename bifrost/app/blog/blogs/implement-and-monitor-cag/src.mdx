Retrieval-Augmented Generation (RAG) has been the gold standard for extending LLM knowledge beyond training cutoffs. But as context windows grow exponentially, a new approach is gaining traction: **Context-Augmented Generation (CAG)**.

![Implement and Monitor CAG with Helicone](/static/blog/implement-and-monitor-cag/cag-cover.webp)

Unlike RAG, which retrieves relevant chunks from a knowledge base, **CAG loads entire documents directly into the LLM's context window.** This simpler approach is now viable thanks to dramatically expanded context lengths, from just 4K tokens two years ago to 1-2 million tokens today.

Let's take a look at why this matters and how you can implement it in your LLM applications.

<CallToAction
  title="ðŸ’¡ If you prefer a video explanation, check out this video!"
  description="In this video by AI Jason, he walks through a complete implementation of CAG using Gemini 2.0 Flash, and how to set up Helicone to monitor your LLM app cost."
  primaryButtonText="Watch Video"
  primaryButtonLink="https://www.youtube.com/watch?v=KHDMoQ2Sp2s"
  secondaryButtonText="Set up Helicone"
  secondaryButtonLink="https://docs.helicone.ai/getting-started/quick-start"
/>

## Table of Contents

## Key Differences Between RAG and CAG

| Aspect             | RAG <br/>(Retrieval-Augmented Generation)                                                              | CAG <br/>(Context-Augmented Generation)            |
| ------------------ | ------------------------------------------------------------------------------------------------------ | -------------------------------------------------- |
| Implementation     | Complex pipeline with vector DB, embeddings, and retrieval                                             | Load entire documents directly into context window |
| Typical Workflow   | 1. Create embeddings<br/>2. Store in vector DB<br/>3. Search relevant chunks<br/>4. Feed chunks to LLM | 1. Load document<br/>2. Send to LLM with prompt    |
| Setup Time         | ðŸŸ¥ðŸŸ¥ðŸŸ¥ <br/>Hours to days                                                                              | ðŸŸ¥ <br/>Minutes                                    |
| Code Complexity    | ðŸŸ¥ðŸŸ¥ðŸŸ¥ <br/>Moderate to high                                                                           | ðŸŸ¥ <br/>Very low (10-15 lines)                     |
| Retrieval Accuracy | Depends on chunking strategy, embedding quality                                                        | Near-perfect (if document fits in context)         |
| Best For           | Massive datasets that exceed context windows                                                           | Documents that fit within context limits           |

**The key advantage of CAG is simplicity.** No complex chunking strategies, no vector database setup, and no worrying about whether you've retrieved the right information, because everything is there.

<BottomLine
  title="ðŸ’¡ When does using CAG make sense?"
  description="CAG works best when your document set is under 1-2M tokens (which is roughly 750K-1.5M words) and you need high accuracy with minimal implementation complexity."
/>

## What Makes CAG Now Possible?

### 1. Context Windows Expanded Massively

Modern flagship models have context windows orders of magnitude larger than before:

- 2022: ~4K tokens typical
- 2024: 100K-200K tokens common, with Gemini supporting up to 2M tokens

To put this in perspective, a 2M token context can fit:

- Two complete novels the length of _War and Peace_
- Thousands of pages of documentation
- Entire codebases

### 2. Retrieval Accuracy Improved Within Large Contexts

LLMs have become remarkably better at finding information in vast contexts. Google's "needle in a haystack" tests have shown models like <a href="https://www.helicone.ai/blog/gemini-2.0-flash" target="_blank" rel="noopener">Gemini 2.0</a> achieving near-perfect recall of specific information in contexts up to 1M tokens.

### 3. Token Pricing is Now More Affordable

Input token costs have fallen dramatically, especially for models optimized for large contexts like <a href="https://www.helicone.ai/blog/gemini-2.5-full-developer-guide" target="_blank" rel="noopener">Gemini 2.5</a> and <a href="https://www.helicone.ai/blog/gpt-4.1-full-developer-guide" target="_blank" rel="noopener">GPT-4.1</a>.

## Building a CAG Application in Just 10 Lines of Code

The simplicity of CAG is its biggest selling point. Here's a complete implementation that lets you chat with any PDF document using Gemini 2.0 Flash:

```python
doc_url = "https://discovery.ucl.ac.uk/id/eprint/10089234/1/343019_3_art_0_py4t4l_convrt.pdf"

response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents=[
        types.Part.from_bytes(
            data=httpx.get(doc_url).content,
            mime_type='application/pdf',
        ),
        "Summarize this document"
    ]
)
```

That's it! Just 10 lines of code to:

1. Set a document URL
2. Download the PDF
3. Feed it directly to Gemini
4. Get a response

The key part is using `Part.from_bytes()` to convert the PDF directly into a format Gemini can process. No chunking, no vector database, no complex retrieval mechanisms, just direct document processing.

A similar implementation with RAG would require setting up embeddings, vector storage, chunking logic, and retrieval mechanisms, potentially hundreds of lines of code and multiple dependencies.

For a more in-depth tutorial on how to implement CAG, check out <a href="https://www.youtube.com/watch?v=KHDMoQ2Sp2s" target="_blank" rel="noopener">this video</a> by AI Jason.

## Monitoring Your CAG Implementation

When using CAG, especially with large documents, monitoring becomes essential to track costs and optimize performance.

Helicone provides an easy way to add observability to your implementation by adding just a few lines of code.

For example, here's how you can add Helicone to your Gemini CAG implementation:

```python
client = genai.Client(
    api_key=os.environ['GOOGLE_API_KEY'],
    http_options={
        "base_url": 'https://gateway.helicone.ai',
        "headers": {
            "helicone-auth": f'Bearer {os.environ.get("HELICONE_API_KEY")}',
            "helicone-target-url": 'https://generativelanguage.googleapis.com'
        }
    }
)

doc_url = "https://arxiv.org/pdf/2412.15605v1"  # Replace with the actual URL of your PDF

# Retrieve and encode the PDF bytes
filepath = pathlib.Path('file.pdf')
filepath.write_bytes(httpx.get(doc_url).content)
```

### Why should I integrate Helicone?

By adding the Helicone integration (which is just a few lines of code using the Proxy method), you get:

1. **Cost tracking**: See exactly how much each CAG query costs in real-time
2. **Response caching**: Cache identical CAG queries to avoid redundant costs
3. **Latency monitoring**: track how context size affects response time
4. **Custom properties**: segment by document type, size, or user queries

<CallToAction
  title="Deploy CAG with Confidence Using Helicone âš¡ï¸"
  description="One line of code gives you complete visibility into your large-context LLM applications. Monitor performance across models, track costs in real-time, and implement automatic response caching."
  primaryButtonText="Integrate in 30 Seconds"
  primaryButtonLink="https://helicone.ai/signup"
  secondaryButtonText="Read Docs"
  secondaryButtonLink="https://docs.helicone.ai/features/getting-started/quick-start"
/>

## When to Use CAG vs. RAG

While CAG is incredibly powerful, it's not a complete replacement for RAG. Here's when to use each approach:

| Choose CAG when...                                                              | Choose RAG when...                                            |
| :------------------------------------------------------------------------------ | :------------------------------------------------------------ |
| Your entire knowledge base fits within context limits (typically < 1â€“2M tokens) | Your knowledge base is massive (many GB or TB of text)        |
| You need a simple, quick implementation with minimal maintenance                | You need to search across millions of documents               |
| You want highest possible retrieval accuracy                                    | You require advanced filtering by metadata, dates, categories |
| Your document set changes infrequently                                          | You need real-time updates to your knowledge base             |

### Hybrid Approaches

For larger datasets, consider these hybrid approaches:

1. **Filter, then use CAG.** Use traditional search or metadata filtering to select a subset of documents, then load them all into context.

2. **Parallel LLM calls.** Split large datasets into manageable chunks, process each with a separate LLM call, then have another LLM synthesize the results.

## Bottom Line

As LLM context windows continue to expand, CAG will likely become the default approach for many applications. We're already seeing several interesting developments:

1. **Context Caching**: Gemini and other models are introducing native "context caching," where you can store large contexts on the provider side and reference them in subsequent calls.

2. **Hybrid RAG-CAG Systems**: For truly massive datasets, systems that combine traditional search with CAG are emerging as the best solution.

3. **Multi-Model Architectures**: Using specialized models for different parts of the pipeline, like search, content filtering, and final generation.

The combination of larger context windows, improved retrieval accuracy, and greater simplicity has made CAG not just viable but potentially preferable to traditional RAG approaches.

As you explore CAG for your own applications, be sure to monitor your LLMs with tools like Helicone to ensure you're getting the most out of this powerful approach.

## You Might Also Like

- <a
    href="https://www.helicone.ai/blog/chain-of-draft"
    target="_blank"
    rel="noopener"
  >
    Chain-of-Draft Prompting: A More Efficient Alternative to Chain of Thought
  </a>

- <a
    href="https://www.helicone.ai/blog/openai-o3"
    target="_blank"
    rel="noopener"
  >
    OpenAI o3 Released: Benchmarks and Comparison to o1
  </a>

- <a
    href="https://www.helicone.ai/blog/gpt-4.1-full-developer-guide"
    target="_blank"
    rel="noopener"
  >
    GPT-4.1 Released: Benchmarks, Performance, and Migration Guide
  </a>

- <a
    href="https://www.helicone.ai/blog/claude-3.7-benchmarks-and-examples"
    target="_blank"
    rel="noopener"
  >
    Claude 3.7 Sonnet & Claude Code: A Technical Review
  </a>

<FAQ
  items={[
    {
      question: "What's the maximum document size I can use with CAG?",
      answer:
        "This depends on your chosen model. Gemini 2.0 supports up to 2M tokens (approximately 1.5M words), GPT-4.1 and o3 support 1M tokens, and Claude 3.7 Sonnet supports 200K tokens. For perspective, a typical non-fiction book is around 70-100K tokens.",
    },
    {
      question: "How do I handle documents that exceed context limits?",
      answer:
        "For documents that exceed context limits, consider: Using models with larger context windows, implementing a filter-then-CAG approach where you pre-select the most relevant sections, or falling back to traditional RAG with careful chunking strategies.",
    },
    {
      question: "Is CAG always more accurate than RAG?",
      answer:
        "CAG tends to be more accurate when the entire relevant document fits in context, as it eliminates retrieval errors common in RAG. However, RAG can be more accurate for massive datasets where CAG isn't possible, especially with advanced techniques like HyDE, query expansion, and reranking.",
    },
    {
      question: "How can I track costs when implementing CAG?",
      answer:
        "Integrating with Helicone provides real-time cost tracking for all your LLM calls, including CAG implementations. This allows you to monitor costs per query, implement response caching to avoid redundant costs, and track usage patterns to optimize your implementation.",
    },
    {
      question: "Should I completely replace my RAG systems with CAG?",
      answer:
        "Not necessarily. RAG still excels for massive datasets, real-time information, and complex metadata filtering. Consider your specific use case requirementsâ€”document size, update frequency, and query complexityâ€”before deciding. Many production systems now use hybrid approaches.",
    },
  ]}
/>

<Questions />
