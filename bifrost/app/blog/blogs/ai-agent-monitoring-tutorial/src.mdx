**Time to complete: ~30 minutes**

Your AI agent worked perfectly in testing, but now in production it's making bizarre recommendations and you have no idea why. Sound familiar? As AI agents grow increasingly complex, the black box problem is becoming the number one obstacle to reliable deployment.

In this first part of our **two-part series** on AI agent observability, we'll build a financial research assistant that demonstrates the key components of a modern AI agent. In part two, we'll explore how to effectively monitor it with Helicone's agentic AI observability features.

Let's get started!

## Table of Contents

## Prerequisites

Before we dive in, you'll need:

- **<a href="https://nodejs.org/" target="_blank" rel="noopener">Node.js 16+</a>** installed on your machine
- **<a href="https://platform.openai.com/api-keys" target="_blank" rel="noopener">OpenAI API</a>** key
- **<a href="https://www.alphavantage.co/support/#api-key" target="_blank" rel="noopener">Alpha Vantage API key</a>** (free tier available)
- **<a href="https://helicone.ai/signup" target="_blank" rel="noopener">Helicone API key</a>** (free tier available)

## Quick Start

Want to skip ahead and try the code immediately? Clone the GitHub repository and run the code:

```bash
git clone https://github.com/Yusu-f/helicone-agent-tutorial.git
cd helicone-agent-tutorial
npm install
```

Create a `.env` file with your API keys

```bash
OPENAI_API_KEY=your_openai_key_here
ALPHA_VANTAGE_API_KEY=your_alpha_vantage_key_here
HELICONE_API_KEY=your_helicone_key_here
```

Run the assistant

```bash
npm start
```

This gives you the version of the financial assistant with basic Helicone monitoring.

In part 2, we'll show you how to add comprehensive monitoring to your AI agent with Helicone's Sessions feature.

## How We'll Build Our Financial Assistant

Our financial assistant does two things:

1. Fetches real-time price information and news for specific tickers
2. Uses RAG to answer questions about financial concepts

The agent intelligently determines which approach to take for each query—a pattern applicable to many domains beyond finance, including customer support, healthcare, and legal applications.

## Key Components of Our AI Agent

### 1. Tools

Our agent uses OpenAI's function calling tools system to determine how to handle different queries:

```javascript
// Define OpenAI tools for function calling
const tools = [
  {
    type: "function",
    function: {
      name: "getStockData",
      description: "Get current price and other information for a specific stock by ticker symbol",
      parameters: {
        type: "object",
        properties: {
          ticker: {
            type: "string",
            description: "The stock ticker symbol, e.g., AAPL for Apple Inc."
          }
        },
        required: ["ticker"]
      }
    }
  },
  ...
]
```

This approach allows the model to decide which functions to call based on the user's query.

### 2. Basic Helicone Monitoring

The financial assistant uses Helicone's basic monitoring to track the cost, latency, and error rate of our LLM calls.

```javascript
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
  baseURL: "https://oai.helicone.ai/v1",
  defaultHeaders: {
    "Helicone-Auth": `Bearer ${process.env.HELICONE_API_KEY}`,
  },
});
```

### 3. RAG for Financial Concepts

For popular financial term queries, we use a vector store to retrieve relevant information:

```javascript
// In the searchFinancialTerms function
const resultsWithScores = await vectorStore.similaritySearchWithScore(query, 2);

// Check if we have relevant results with good similarity scores
if (resultsWithScores.length === 0 || resultsWithScores[0][1] < 0.8) {
  console.log("No relevant financial terms found in the knowledge base.");
  return { 
    found: false,
    message: "No relevant financial terms found in the knowledge base."
  };
}
```

This RAG implementation provides domain-specific knowledge to the agent. However, as we'll see later, without proper monitoring, detecting when the system operates outside its knowledge boundaries can be difficult.

### 4. External API Integration

For stock queries, such as real-time price information or news, we connect to the Alpha Vantage API:

```javascript
async function getStockData(ticker) {
  try {
    const url = `https://www.alphavantage.co/query?function=GLOBAL_QUOTE&symbol=${ticker}&apikey=${ALPHA_VANTAGE_API_KEY}`;
    const response = await axios.get(url);
    
    if (response.data['Global Quote'] && Object.keys(response.data['Global Quote']).length > 0) {
      const quote = response.data['Global Quote'];
      return {
        symbol: ticker.toUpperCase(),
        price: parseFloat(quote['05. price']),
        change: parseFloat(quote['09. change']),
        changePercent: quote['10. change percent'],
        volume: parseInt(quote['06. volume']),
        latestTradingDay: quote['07. latest trading day']
      };
    } else {
      return { error: `No data found for ticker ${ticker}` };
    }
  } catch (error) {
    console.error('Error fetching stock data:', error);
    return { error: `Failed to get stock data for ${ticker}` };
  }
}
```

By connecting to external APIs, we can access real-time data that the underlying LLM doesn't have access to.

### 5. Multi-Step Processing

The main `processQuery` function orchestrates the entire workflow:

```javascript
// Get response from LLM on how to process query
const initialResponse = await openai.chat.completions.create({
  model: "gpt-3.5-turbo",
  messages,
  tools,
  temperature: 0.1,
});

const initialMessage = initialResponse.choices[0].message;

// Check if the model wants to call a function
if (initialMessage.tool_calls) {
  console.log("Model requested tool calls:", initialMessage.tool_calls);
  
  // Create an array to store messages for this interaction
  const messageHistory = [...messages, initialMessage];
  
  // Process each tool call
  for (const toolCall of initialMessage.tool_calls) {
    ...
  }
  
  // Get a final response from the model with the function results
  console.log("Getting final response with tool results...");
  const finalResponse = await openai.chat.completions.create({
    model: "gpt-3.5-turbo",
    messages: messageHistory,
    temperature: 0.1,
  });
  
  return finalResponse.choices[0].message.content;
}
```

This approach allows the model to decide which tools to use, call them, and then use their output to generate a final response.

## Testing our Financial Assistant

Now, let's take our financial assistant for a spin!

Run the following command to start the assistant:

```bash
npm start
```

We can view the results of our queries in the **Helicone dashboard**.

> Prompt: What is tesla's stock price?

Result:

![What is Tesla's stock price?](/static/blog/ai-agent-monitoring-tutorial/tesla-prompt.webp)

> Prompt: What is halal investing

Result:

![What is halal investing?](/static/blog/ai-agent-monitoring-tutorial/halal-investing-prompt.webp)

It looks like our financial assistant works fine...**or does it?**

There's an issue with the second query. 

The term **"halal investing"** is not in our knowledge base, yet the agent confidently responded with a detailed answer. 

This is a perfect example of <a href="https://www.helicone.ai/blog/how-to-reduce-llm-hallucination" target="_blank" rel="noopener">hallucination</a> that would be nearly impossible to detect in a production app without proper monitoring.

## Debugging our Financial Assistant 

Looking at our implementation, there are several blind spots that could potentially cause issues:

- **Hallucinations**: Our agent provided information about halal investing despite not having this in its knowledge base - how do we detect this?
- **Cost Visibility**: How many tokens is each component of our agent consuming? Which queries are most expensive?
- **Latency Issues**: If the agent becomes slow, which step is causing the bottleneck?
- **Error Patterns**: Are certain types of queries consistently failing? Where in the pipeline do these failures occur?

In <a href="https://www.helicone.ai/blog/ai-agent-monitoring-tutorial-part-2" target="_blank" rel="noopener">Part 2</a>, we'll add Helicone to our financial assistant to gain comprehensive visibility into every step of the process. Here's a preview of what you can see:

![Using Helicone Sessions to Debug AI Agents](/static/blog/ai-agent-monitoring-tutorial/sessions-ai-agent.webp)

We'll monitor each step of the agent's workflow, resolve bugs, and gain insights into useful metrics like cost, latency, and error rates.

Stay tuned!

<CallToAction
  title="Observe Your AI Agents with Helicone ⚡️"
  description="Stop building AI in the dark. Get complete visibility into every step of your AI workflows, track costs down to the penny, and debug complex issues in minutes instead of days."
  primaryButtonText="Start Monitoring for Free"
  primaryButtonLink="https://helicone.ai/signup"
  secondaryButtonText="How Debugging Works"
  secondaryButtonLink="https://docs.helicone.ai/features/sessions"
/>

### You might also like:

- **<a href="https://www.helicone.ai/blog/ai-agent-monitoring-tutorial-part-2" target="_blank" rel="noopener">Step-by-Step Guide to Building & Optimizing AI Agents: Part 2</a>**
- **<a href="https://www.helicone.ai/blog/debugging-chatbots-and-ai-agents-with-sessions" target="_blank" rel="noopener">Debugging RAG Chatbots and AI Agents with Sessions</a>**
- **<a href="https://www.helicone.ai/blog/full-guide-to-improving-ai-agents" target="_blank" rel="noopener">The Full Developer's Guide to Building Effective AI Agents</a>**
- **<a href="https://www.helicone.ai/blog/agentic-rag-full-developer-guide" target="_blank" rel="noopener">Building Agentic RAG Systems: A Developer's Guide to Smarter Information Retrieval</a>**

<FAQ 
  items={[
    {
      question: "Why do AI agents need specialized observability tools?",
      answer: "AI agents have unique monitoring challenges including non-deterministic execution paths, multi-step LLM calls, complex branching logic, and dependencies on external systems. Unlike traditional applications with fixed flows, agents' decision trees vary with each request. Standard monitoring tools can't track these dynamic workflows or evaluate response quality across interconnected steps, which is why specialized tools like Helicone's session-based tracing are essential for AI agent observability."
    },
    {
      question: "What are the biggest blind spots when deploying AI agents to production?",
      answer: "The most dangerous blind spots include: undetected hallucinations in responses, hidden cost escalations from inefficient prompts, silent failures in multi-step reasoning chains, data leakage in RAG implementations, inconsistent performance across different user segments, and degrading accuracy over time as data or usage patterns change. Without proper observability, these issues can persist for weeks before being discovered, potentially causing significant business impact."
    },
    {
      question: "What metrics should I monitor for any AI agent?",
      answer: "Critical metrics for all AI agents include: end-to-end latency of complete workflows, token usage per step and total cost per request, step completion rates showing where agents get stuck, retrieval quality for RAG implementations, routing accuracy between different processing pathways, error rates for external API calls, and user satisfaction with responses. Tracking these metrics helps identify bottlenecks, optimize costs, and ensure reliable agent performance."
    },
    {
      question: "How do I implement observability across different AI agent frameworks?",
      answer: "Helicone offers flexible integration options for all major AI frameworks. For LangChain, CrewAI, and LlamaIndex, direct integrations are available. For custom agents or other frameworks, you can typically use either Helicone's proxy approach (changing just the base URL) or the SDK integration. The Sessions feature works consistently across most major frameworks to trace multi-step agent workflows regardless of your technology choices, giving you a unified view of all AI operations."
    }
  ]}
/>

<Questions />