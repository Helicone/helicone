## Improved Streaming Support and Async Stream Parser

We've made significant improvements to our streaming functionality with two key updates:

### Stream Fixes

We've resolved several issues with stream handling across different LLM providers, ensuring more reliable and consistent streaming experiences. These fixes address edge cases and improve compatibility with various streaming implementations.

Key improvements include:

- Better handling of stream interruptions and reconnections
- Enhanced error handling for streaming responses
- Improved compatibility with different LLM provider streaming formats

### Asynchronous Stream Parser

We've introduced a new asynchronous stream parser that significantly improves performance when working with streamed responses. This new parser:

- Processes stream chunks asynchronously for better performance
- Reduces latency when handling large streamed responses
- Provides more reliable token counting for streamed content

### Example Usage with Together AI

Here's an example of how to use the improved streaming functionality with Together AI:

```typescript
import Together from "together-ai";
import { HeliconeManualLogger } from "@helicone/helpers";

export async function main() {
  const heliconeLogger = new HeliconeManualLogger({
    apiKey: process.env.HELICONE_API_KEY!,
    headers: {},
  });

  const together = new Together();
  const body = {
    model: "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
    messages: [{ role: "user", content: "Your question here" }],
    stream: true,
  } as Together.Chat.CompletionCreateParamsStreaming & { stream: true };

  const response = await together.chat.completions.create(body);
  const [stream1, stream2] = response.tee();

  // Use the new async stream parser
  heliconeLogger.logStream(body, async (resultRecorder) => {
    resultRecorder.attachStream(stream1.toReadableStream());
  });

  // Process the stream for your application
  const textDecoder = new TextDecoder();
  for await (const chunk of stream2.toReadableStream()) {
    console.log(textDecoder.decode(chunk));
  }
}
```

These improvements make working with streaming LLMs more reliable and efficient, especially for applications that require real-time responses.
