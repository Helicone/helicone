{
  "llama-3.3-nemotron-super-49b-v1": {
    "id": "llama-3.3-nemotron-super-49b-v1",
    "name": "NVIDIA: Llama 3.3 Nemotron Super 49B v1",
    "author": "nvidia",
    "description": "Llama-3.3-Nemotron-Super-49B-v1 is a large language model (LLM) optimized for advanced reasoning, conversational interactions, retrieval-augmented generation (RAG), and tool-calling tasks. Derived from Meta's Llama-3.3-70B-Instruct, it employs a Neural Architecture Search (NAS) approach, significantly enhancing efficiency and reducing memory requirements. This allows the model to support a context length of up to 128K tokens and fit efficiently on single high-performance GPUs, such as NVIDIA H200.\n\nNote: you must include `detailed thinking on` in the system prompt to enable reasoning. Please see [Usage Recommendations](https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1#quick-start-and-usage-recommendations) for more.",
    "contextLength": 131072,
    "maxOutputTokens": null,
    "created": "2025-04-08T13:38:14.000Z",
    "modality": "text->text",
    "tokenizer": "Other"
  },
  "llama-3.1-nemotron-ultra-253b-v1:free": {
    "id": "llama-3.1-nemotron-ultra-253b-v1:free",
    "name": "NVIDIA: Llama 3.1 Nemotron Ultra 253B v1 (free)",
    "author": "nvidia",
    "description": "Llama-3.1-Nemotron-Ultra-253B-v1 is a large language model (LLM) optimized for advanced reasoning, human-interactive chat, retrieval-augmented generation (RAG), and tool-calling tasks. Derived from Meta’s Llama-3.1-405B-Instruct, it has been significantly customized using Neural Architecture Search (NAS), resulting in enhanced efficiency, reduced memory usage, and improved inference latency. The model supports a context length of up to 128K tokens and can operate efficiently on an 8x NVIDIA H100 node.\n\nNote: you must include `detailed thinking on` in the system prompt to enable reasoning. Please see [Usage Recommendations](https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1#quick-start-and-usage-recommendations) for more.",
    "contextLength": 131072,
    "maxOutputTokens": null,
    "created": "2025-04-08T12:24:19.000Z",
    "modality": "text->text",
    "tokenizer": "Llama3"
  },
  "llama-3.1-nemotron-ultra-253b-v1": {
    "id": "llama-3.1-nemotron-ultra-253b-v1",
    "name": "NVIDIA: Llama 3.1 Nemotron Ultra 253B v1",
    "author": "nvidia",
    "description": "Llama-3.1-Nemotron-Ultra-253B-v1 is a large language model (LLM) optimized for advanced reasoning, human-interactive chat, retrieval-augmented generation (RAG), and tool-calling tasks. Derived from Meta’s Llama-3.1-405B-Instruct, it has been significantly customized using Neural Architecture Search (NAS), resulting in enhanced efficiency, reduced memory usage, and improved inference latency. The model supports a context length of up to 128K tokens and can operate efficiently on an 8x NVIDIA H100 node.\n\nNote: you must include `detailed thinking on` in the system prompt to enable reasoning. Please see [Usage Recommendations](https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1#quick-start-and-usage-recommendations) for more.",
    "contextLength": 131072,
    "maxOutputTokens": null,
    "created": "2025-04-08T12:24:19.000Z",
    "modality": "text->text",
    "tokenizer": "Llama3"
  },
  "llama-3.1-nemotron-70b-instruct": {
    "id": "llama-3.1-nemotron-70b-instruct",
    "name": "NVIDIA: Llama 3.1 Nemotron 70B Instruct",
    "author": "nvidia",
    "description": "NVIDIA's Llama 3.1 Nemotron 70B is a language model designed for generating precise and useful responses. Leveraging [Llama 3.1 70B](/models/meta-llama/llama-3.1-70b-instruct) architecture and Reinforcement Learning from Human Feedback (RLHF), it excels in automatic alignment benchmarks. This model is tailored for applications requiring high accuracy in helpfulness and response generation, suitable for diverse user queries across multiple domains.\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).",
    "contextLength": 131072,
    "maxOutputTokens": 131072,
    "created": "2024-10-15T00:00:00.000Z",
    "modality": "text->text",
    "tokenizer": "Llama3"
  }
}