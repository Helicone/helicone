{
  "dolphin-mistral-24b-venice-edition:free": {
    "id": "dolphin-mistral-24b-venice-edition:free",
    "name": "Venice: Uncensored (free)",
    "author": "cognitivecomputations",
    "description": "Venice Uncensored Dolphin Mistral 24B Venice Edition is a fine-tuned variant of Mistral-Small-24B-Instruct-2501, developed by dphn.ai in collaboration with Venice.ai. This model is designed as an “uncensored” instruct-tuned LLM, preserving user control over alignment, system prompts, and behavior. Intended for advanced and unrestricted use cases, Venice Uncensored emphasizes steerability and transparent behavior, removing default safety and alignment layers typically found in mainstream assistant models.",
    "contextLength": 32768,
    "maxOutputTokens": null,
    "created": "2025-07-09T21:02:46.000Z",
    "modality": "text->text",
    "tokenizer": "Other"
  },
  "dolphin3.0-r1-mistral-24b:free": {
    "id": "dolphin3.0-r1-mistral-24b:free",
    "name": "Dolphin3.0 R1 Mistral 24B (free)",
    "author": "cognitivecomputations",
    "description": "Dolphin 3.0 R1 is the next generation of the Dolphin series of instruct-tuned models.  Designed to be the ultimate general purpose local model, enabling coding, math, agentic, function calling, and general use cases.\n\nThe R1 version has been trained for 3 epochs to reason using 800k reasoning traces from the Dolphin-R1 dataset.\n\nDolphin aims to be a general purpose reasoning instruct model, similar to the models behind ChatGPT, Claude, Gemini.\n\nPart of the [Dolphin 3.0 Collection](https://huggingface.co/collections/cognitivecomputations/dolphin-30-677ab47f73d7ff66743979a3) Curated and trained by [Eric Hartford](https://huggingface.co/ehartford), [Ben Gitter](https://huggingface.co/bigstorm), [BlouseJury](https://huggingface.co/BlouseJury) and [Cognitive Computations](https://huggingface.co/cognitivecomputations)",
    "contextLength": 32768,
    "maxOutputTokens": null,
    "created": "2025-02-13T16:01:38.000Z",
    "modality": "text->text",
    "tokenizer": "Other"
  },
  "dolphin3.0-r1-mistral-24b": {
    "id": "dolphin3.0-r1-mistral-24b",
    "name": "Dolphin3.0 R1 Mistral 24B",
    "author": "cognitivecomputations",
    "description": "Dolphin 3.0 R1 is the next generation of the Dolphin series of instruct-tuned models.  Designed to be the ultimate general purpose local model, enabling coding, math, agentic, function calling, and general use cases.\n\nThe R1 version has been trained for 3 epochs to reason using 800k reasoning traces from the Dolphin-R1 dataset.\n\nDolphin aims to be a general purpose reasoning instruct model, similar to the models behind ChatGPT, Claude, Gemini.\n\nPart of the [Dolphin 3.0 Collection](https://huggingface.co/collections/cognitivecomputations/dolphin-30-677ab47f73d7ff66743979a3) Curated and trained by [Eric Hartford](https://huggingface.co/ehartford), [Ben Gitter](https://huggingface.co/bigstorm), [BlouseJury](https://huggingface.co/BlouseJury) and [Cognitive Computations](https://huggingface.co/cognitivecomputations)",
    "contextLength": 32768,
    "maxOutputTokens": null,
    "created": "2025-02-13T16:01:38.000Z",
    "modality": "text->text",
    "tokenizer": "Other"
  },
  "dolphin3.0-mistral-24b:free": {
    "id": "dolphin3.0-mistral-24b:free",
    "name": "Dolphin3.0 Mistral 24B (free)",
    "author": "cognitivecomputations",
    "description": "Dolphin 3.0 is the next generation of the Dolphin series of instruct-tuned models.  Designed to be the ultimate general purpose local model, enabling coding, math, agentic, function calling, and general use cases.\n\nDolphin aims to be a general purpose instruct model, similar to the models behind ChatGPT, Claude, Gemini. \n\nPart of the [Dolphin 3.0 Collection](https://huggingface.co/collections/cognitivecomputations/dolphin-30-677ab47f73d7ff66743979a3) Curated and trained by [Eric Hartford](https://huggingface.co/ehartford), [Ben Gitter](https://huggingface.co/bigstorm), [BlouseJury](https://huggingface.co/BlouseJury) and [Cognitive Computations](https://huggingface.co/cognitivecomputations)",
    "contextLength": 32768,
    "maxOutputTokens": null,
    "created": "2025-02-13T15:53:39.000Z",
    "modality": "text->text",
    "tokenizer": "Other"
  },
  "dolphin3.0-mistral-24b": {
    "id": "dolphin3.0-mistral-24b",
    "name": "Dolphin3.0 Mistral 24B",
    "author": "cognitivecomputations",
    "description": "Dolphin 3.0 is the next generation of the Dolphin series of instruct-tuned models.  Designed to be the ultimate general purpose local model, enabling coding, math, agentic, function calling, and general use cases.\n\nDolphin aims to be a general purpose instruct model, similar to the models behind ChatGPT, Claude, Gemini. \n\nPart of the [Dolphin 3.0 Collection](https://huggingface.co/collections/cognitivecomputations/dolphin-30-677ab47f73d7ff66743979a3) Curated and trained by [Eric Hartford](https://huggingface.co/ehartford), [Ben Gitter](https://huggingface.co/bigstorm), [BlouseJury](https://huggingface.co/BlouseJury) and [Cognitive Computations](https://huggingface.co/cognitivecomputations)",
    "contextLength": 32768,
    "maxOutputTokens": null,
    "created": "2025-02-13T15:53:39.000Z",
    "modality": "text->text",
    "tokenizer": "Other"
  },
  "dolphin-mixtral-8x22b": {
    "id": "dolphin-mixtral-8x22b",
    "name": "Dolphin 2.9.2 Mixtral 8x22B 🐬",
    "author": "cognitivecomputations",
    "description": "Dolphin 2.9 is designed for instruction following, conversational, and coding. This model is a finetune of [Mixtral 8x22B Instruct](/models/mistralai/mixtral-8x22b-instruct). It features a 64k context length and was fine-tuned with a 16k sequence length using ChatML templates.\n\nThis model is a successor to [Dolphin Mixtral 8x7B](/models/cognitivecomputations/dolphin-mixtral-8x7b).\n\nThe model is uncensored and is stripped of alignment and bias. It requires an external alignment layer for ethical use. Users are cautioned to use this highly compliant model responsibly, as detailed in a blog post about uncensored models at [erichartford.com/uncensored-models](https://erichartford.com/uncensored-models).\n\n#moe #uncensored",
    "contextLength": 16000,
    "maxOutputTokens": 8192,
    "created": "2024-06-08T00:00:00.000Z",
    "modality": "text->text",
    "tokenizer": "Mistral"
  }
}