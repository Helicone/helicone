{
  "glm-4.1v-9b-thinking": {
    "id": "glm-4.1v-9b-thinking",
    "name": "THUDM: GLM 4.1V 9B Thinking",
    "author": "thudm",
    "description": "GLM-4.1V-9B-Thinking is a 9B parameter vision-language model developed by THUDM, based on the GLM-4-9B foundation. It introduces a reasoning-centric \"thinking paradigm\" enhanced with reinforcement learning to improve multimodal reasoning, long-context understanding (up to 64K tokens), and complex problem solving. It achieves state-of-the-art performance among models in its class, outperforming even larger models like Qwen-2.5-VL-72B on a majority of benchmark tasks. ",
    "contextLength": 65536,
    "maxOutputTokens": 8000,
    "created": "2025-07-11T14:33:05.000Z",
    "modality": "text+image->text",
    "tokenizer": "Other"
  },
  "glm-z1-32b:free": {
    "id": "glm-z1-32b:free",
    "name": "THUDM: GLM Z1 32B (free)",
    "author": "thudm",
    "description": "GLM-Z1-32B-0414 is an enhanced reasoning variant of GLM-4-32B, built for deep mathematical, logical, and code-oriented problem solving. It applies extended reinforcement learning—both task-specific and general pairwise preference-based—to improve performance on complex multi-step tasks. Compared to the base GLM-4-32B model, Z1 significantly boosts capabilities in structured reasoning and formal domains.\n\nThe model supports enforced “thinking” steps via prompt engineering and offers improved coherence for long-form outputs. It’s optimized for use in agentic workflows, and includes support for long context (via YaRN), JSON tool calling, and fine-grained sampling configuration for stable inference. Ideal for use cases requiring deliberate, multi-step reasoning or formal derivations.",
    "contextLength": 32768,
    "maxOutputTokens": null,
    "created": "2025-04-17T21:09:08.000Z",
    "modality": "text->text",
    "tokenizer": "Other"
  },
  "glm-4-32b": {
    "id": "glm-4-32b",
    "name": "THUDM: GLM 4 32B",
    "author": "thudm",
    "description": "GLM-4-32B-0414 is a 32B bilingual (Chinese-English) open-weight language model optimized for code generation, function calling, and agent-style tasks. Pretrained on 15T of high-quality and reasoning-heavy data, it was further refined using human preference alignment, rejection sampling, and reinforcement learning. The model excels in complex reasoning, artifact generation, and structured output tasks, achieving performance comparable to GPT-4o and DeepSeek-V3-0324 across several benchmarks.",
    "contextLength": 32000,
    "maxOutputTokens": 32000,
    "created": "2025-04-17T20:15:15.000Z",
    "modality": "text->text",
    "tokenizer": "Other"
  }
}