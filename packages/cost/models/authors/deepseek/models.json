{
  "deepseek-r1-distill-qwen-7b": {
    "id": "deepseek-r1-distill-qwen-7b",
    "name": "DeepSeek: R1 Distill Qwen 7B",
    "author": "deepseek",
    "description": "DeepSeek-R1-Distill-Qwen-7B is a 7 billion parameter dense language model distilled from DeepSeek-R1, leveraging reinforcement learning-enhanced reasoning data generated by DeepSeek's larger models. The distillation process transfers advanced reasoning, math, and code capabilities into a smaller, more efficient model architecture based on Qwen2.5-Math-7B. This model demonstrates strong performance across mathematical benchmarks (92.8% pass@1 on MATH-500), coding tasks (Codeforces rating 1189), and general reasoning (49.1% pass@1 on GPQA Diamond), achieving competitive accuracy relative to larger models while maintaining smaller inference costs.",
    "contextLength": 131072,
    "maxOutputTokens": null,
    "created": "2025-05-30T18:03:57.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen"
  },
  "deepseek-r1-0528-qwen3-8b:free": {
    "id": "deepseek-r1-0528-qwen3-8b:free",
    "name": "DeepSeek: Deepseek R1 0528 Qwen3 8B (free)",
    "author": "deepseek",
    "description": "DeepSeek-R1-0528 is a lightly upgraded release of DeepSeek R1 that taps more compute and smarter post-training tricks, pushing its reasoning and inference to the brink of flagship models like O3 and Gemini 2.5 Pro.\nIt now tops math, programming, and logic leaderboards, showcasing a step-change in depth-of-thought.\nThe distilled variant, DeepSeek-R1-0528-Qwen3-8B, transfers this chain-of-thought into an 8 B-parameter form, beating standard Qwen3 8B by +10 pp and tying the 235 B “thinking” giant on AIME 2024.",
    "contextLength": 131072,
    "maxOutputTokens": null,
    "created": "2025-05-29T17:09:03.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen"
  },
  "deepseek-r1-0528-qwen3-8b": {
    "id": "deepseek-r1-0528-qwen3-8b",
    "name": "DeepSeek: Deepseek R1 0528 Qwen3 8B",
    "author": "deepseek",
    "description": "DeepSeek-R1-0528 is a lightly upgraded release of DeepSeek R1 that taps more compute and smarter post-training tricks, pushing its reasoning and inference to the brink of flagship models like O3 and Gemini 2.5 Pro.\nIt now tops math, programming, and logic leaderboards, showcasing a step-change in depth-of-thought.\nThe distilled variant, DeepSeek-R1-0528-Qwen3-8B, transfers this chain-of-thought into an 8 B-parameter form, beating standard Qwen3 8B by +10 pp and tying the 235 B “thinking” giant on AIME 2024.",
    "contextLength": 32000,
    "maxOutputTokens": null,
    "created": "2025-05-29T17:09:03.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen"
  },
  "deepseek-r1-0528:free": {
    "id": "deepseek-r1-0528:free",
    "name": "DeepSeek: R1 0528 (free)",
    "author": "deepseek",
    "description": "May 28th update to the [original DeepSeek R1](/deepseek/deepseek-r1) Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass.\n\nFully open-source model.",
    "contextLength": 163840,
    "maxOutputTokens": null,
    "created": "2025-05-28T17:59:30.000Z",
    "modality": "text->text",
    "tokenizer": "DeepSeek"
  },
  "deepseek-r1-0528": {
    "id": "deepseek-r1-0528",
    "name": "DeepSeek: R1 0528",
    "author": "deepseek",
    "description": "May 28th update to the [original DeepSeek R1](/deepseek/deepseek-r1) Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass.\n\nFully open-source model.",
    "contextLength": 163840,
    "maxOutputTokens": null,
    "created": "2025-05-28T17:59:30.000Z",
    "modality": "text->text",
    "tokenizer": "DeepSeek"
  },
  "deepseek-prover-v2": {
    "id": "deepseek-prover-v2",
    "name": "DeepSeek: DeepSeek Prover V2",
    "author": "deepseek",
    "description": "DeepSeek Prover V2 is a 671B parameter model, speculated to be geared towards logic and mathematics. Likely an upgrade from [DeepSeek-Prover-V1.5](https://huggingface.co/deepseek-ai/DeepSeek-Prover-V1.5-RL) Not much is known about the model yet, as DeepSeek released it on Hugging Face without an announcement or description.",
    "contextLength": 163840,
    "maxOutputTokens": null,
    "created": "2025-04-30T11:38:14.000Z",
    "modality": "text->text",
    "tokenizer": "DeepSeek"
  },
  "deepseek-v3-base": {
    "id": "deepseek-v3-base",
    "name": "DeepSeek: DeepSeek V3 Base",
    "author": "deepseek",
    "description": "Note that this is a base model mostly meant for testing, you need to provide detailed prompts for the model to return useful responses. \n\nDeepSeek-V3 Base is a 671B parameter open Mixture-of-Experts (MoE) language model with 37B active parameters per forward pass and a context length of 128K tokens. Trained on 14.8T tokens using FP8 mixed precision, it achieves high training efficiency and stability, with strong performance across language, reasoning, math, and coding tasks. \n\nDeepSeek-V3 Base is the pre-trained model behind [DeepSeek V3](/deepseek/deepseek-chat-v3)",
    "contextLength": 163840,
    "maxOutputTokens": null,
    "created": "2025-03-29T18:13:43.000Z",
    "modality": "text->text",
    "tokenizer": "DeepSeek"
  },
  "deepseek-chat-v3-0324:free": {
    "id": "deepseek-chat-v3-0324:free",
    "name": "DeepSeek: DeepSeek V3 0324 (free)",
    "author": "deepseek",
    "description": "DeepSeek V3, a 685B-parameter, mixture-of-experts model, is the latest iteration of the flagship chat model family from the DeepSeek team.\n\nIt succeeds the [DeepSeek V3](/deepseek/deepseek-chat-v3) model and performs really well on a variety of tasks.",
    "contextLength": 163840,
    "maxOutputTokens": 163840,
    "created": "2025-03-24T13:59:15.000Z",
    "modality": "text->text",
    "tokenizer": "DeepSeek"
  },
  "deepseek-chat-v3-0324": {
    "id": "deepseek-chat-v3-0324",
    "name": "DeepSeek: DeepSeek V3 0324",
    "author": "deepseek",
    "description": "DeepSeek V3, a 685B-parameter, mixture-of-experts model, is the latest iteration of the flagship chat model family from the DeepSeek team.\n\nIt succeeds the [DeepSeek V3](/deepseek/deepseek-chat-v3) model and performs really well on a variety of tasks.",
    "contextLength": 163840,
    "maxOutputTokens": 163840,
    "created": "2025-03-24T13:59:15.000Z",
    "modality": "text->text",
    "tokenizer": "DeepSeek"
  },
  "deepseek-r1-distill-llama-8b": {
    "id": "deepseek-r1-distill-llama-8b",
    "name": "DeepSeek: R1 Distill Llama 8B",
    "author": "deepseek",
    "description": "DeepSeek R1 Distill Llama 8B is a distilled large language model based on [Llama-3.1-8B-Instruct](/meta-llama/llama-3.1-8b-instruct), using outputs from [DeepSeek R1](/deepseek/deepseek-r1). The model combines advanced distillation techniques to achieve high performance across multiple benchmarks, including:\n\n- AIME 2024 pass@1: 50.4\n- MATH-500 pass@1: 89.1\n- CodeForces Rating: 1205\n\nThe model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.\n\nHugging Face: \n- [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) \n- [DeepSeek-R1-Distill-Llama-8B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |",
    "contextLength": 32000,
    "maxOutputTokens": 32000,
    "created": "2025-02-07T14:15:18.000Z",
    "modality": "text->text",
    "tokenizer": "Llama3"
  },
  "deepseek-r1-distill-qwen-1.5b": {
    "id": "deepseek-r1-distill-qwen-1.5b",
    "name": "DeepSeek: R1 Distill Qwen 1.5B",
    "author": "deepseek",
    "description": "DeepSeek R1 Distill Qwen 1.5B is a distilled large language model based on  [Qwen 2.5 Math 1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B), using outputs from [DeepSeek R1](/deepseek/deepseek-r1). It's a very small and efficient model which outperforms [GPT 4o 0513](/openai/gpt-4o-2024-05-13) on Math Benchmarks.\n\nOther benchmark results include:\n\n- AIME 2024 pass@1: 28.9\n- AIME 2024 cons@64: 52.7\n- MATH-500 pass@1: 83.9\n\nThe model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.",
    "contextLength": 131072,
    "maxOutputTokens": 32768,
    "created": "2025-01-31T12:54:27.000Z",
    "modality": "text->text",
    "tokenizer": "Other"
  },
  "deepseek-r1-distill-qwen-32b": {
    "id": "deepseek-r1-distill-qwen-32b",
    "name": "DeepSeek: R1 Distill Qwen 32B",
    "author": "deepseek",
    "description": "DeepSeek R1 Distill Qwen 32B is a distilled large language model based on [Qwen 2.5 32B](https://huggingface.co/Qwen/Qwen2.5-32B), using outputs from [DeepSeek R1](/deepseek/deepseek-r1). It outperforms OpenAI's o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\\n\\nOther benchmark results include:\\n\\n- AIME 2024 pass@1: 72.6\\n- MATH-500 pass@1: 94.3\\n- CodeForces Rating: 1691\\n\\nThe model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.",
    "contextLength": 131072,
    "maxOutputTokens": 16384,
    "created": "2025-01-29T23:53:50.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen"
  },
  "deepseek-r1-distill-qwen-14b:free": {
    "id": "deepseek-r1-distill-qwen-14b:free",
    "name": "DeepSeek: R1 Distill Qwen 14B (free)",
    "author": "deepseek",
    "description": "DeepSeek R1 Distill Qwen 14B is a distilled large language model based on [Qwen 2.5 14B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B), using outputs from [DeepSeek R1](/deepseek/deepseek-r1). It outperforms OpenAI's o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\nOther benchmark results include:\n\n- AIME 2024 pass@1: 69.7\n- MATH-500 pass@1: 93.9\n- CodeForces Rating: 1481\n\nThe model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.",
    "contextLength": 64000,
    "maxOutputTokens": null,
    "created": "2025-01-29T23:39:00.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen"
  },
  "deepseek-r1-distill-qwen-14b": {
    "id": "deepseek-r1-distill-qwen-14b",
    "name": "DeepSeek: R1 Distill Qwen 14B",
    "author": "deepseek",
    "description": "DeepSeek R1 Distill Qwen 14B is a distilled large language model based on [Qwen 2.5 14B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B), using outputs from [DeepSeek R1](/deepseek/deepseek-r1). It outperforms OpenAI's o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\nOther benchmark results include:\n\n- AIME 2024 pass@1: 69.7\n- MATH-500 pass@1: 93.9\n- CodeForces Rating: 1481\n\nThe model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.",
    "contextLength": 64000,
    "maxOutputTokens": 32000,
    "created": "2025-01-29T23:39:00.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen"
  },
  "deepseek-r1-distill-llama-70b:free": {
    "id": "deepseek-r1-distill-llama-70b:free",
    "name": "DeepSeek: R1 Distill Llama 70B (free)",
    "author": "deepseek",
    "description": "DeepSeek R1 Distill Llama 70B is a distilled large language model based on [Llama-3.3-70B-Instruct](/meta-llama/llama-3.3-70b-instruct), using outputs from [DeepSeek R1](/deepseek/deepseek-r1). The model combines advanced distillation techniques to achieve high performance across multiple benchmarks, including:\n\n- AIME 2024 pass@1: 70.0\n- MATH-500 pass@1: 94.5\n- CodeForces Rating: 1633\n\nThe model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.",
    "contextLength": 8192,
    "maxOutputTokens": 4096,
    "created": "2025-01-23T20:12:49.000Z",
    "modality": "text->text",
    "tokenizer": "Llama3"
  },
  "deepseek-r1-distill-llama-70b": {
    "id": "deepseek-r1-distill-llama-70b",
    "name": "DeepSeek: R1 Distill Llama 70B",
    "author": "deepseek",
    "description": "DeepSeek R1 Distill Llama 70B is a distilled large language model based on [Llama-3.3-70B-Instruct](/meta-llama/llama-3.3-70b-instruct), using outputs from [DeepSeek R1](/deepseek/deepseek-r1). The model combines advanced distillation techniques to achieve high performance across multiple benchmarks, including:\n\n- AIME 2024 pass@1: 70.0\n- MATH-500 pass@1: 94.5\n- CodeForces Rating: 1633\n\nThe model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.",
    "contextLength": 131072,
    "maxOutputTokens": null,
    "created": "2025-01-23T20:12:49.000Z",
    "modality": "text->text",
    "tokenizer": "Llama3"
  },
  "deepseek-r1:free": {
    "id": "deepseek-r1:free",
    "name": "DeepSeek: R1 (free)",
    "author": "deepseek",
    "description": "DeepSeek R1 is here: Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass.\n\nFully open-source model & [technical report](https://api-docs.deepseek.com/news/news250120).\n\nMIT licensed: Distill & commercialize freely!",
    "contextLength": 163840,
    "maxOutputTokens": null,
    "created": "2025-01-20T13:51:35.000Z",
    "modality": "text->text",
    "tokenizer": "DeepSeek"
  },
  "deepseek-r1": {
    "id": "deepseek-r1",
    "name": "DeepSeek: R1",
    "author": "deepseek",
    "description": "DeepSeek R1 is here: Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass.\n\nFully open-source model & [technical report](https://api-docs.deepseek.com/news/news250120).\n\nMIT licensed: Distill & commercialize freely!",
    "contextLength": 163840,
    "maxOutputTokens": 163840,
    "created": "2025-01-20T13:51:35.000Z",
    "modality": "text->text",
    "tokenizer": "DeepSeek"
  },
  "deepseek-chat": {
    "id": "deepseek-chat",
    "name": "DeepSeek: DeepSeek V3",
    "author": "deepseek",
    "description": "DeepSeek-V3 is the latest model from the DeepSeek team, building upon the instruction following and coding abilities of the previous versions. Pre-trained on nearly 15 trillion tokens, the reported evaluations reveal that the model outperforms other open-source models and rivals leading closed-source models.\n\nFor model details, please visit [the DeepSeek-V3 repo](https://github.com/deepseek-ai/DeepSeek-V3) for more information, or see the [launch announcement](https://api-docs.deepseek.com/news/news1226).",
    "contextLength": 163840,
    "maxOutputTokens": null,
    "created": "2024-12-26T19:28:40.000Z",
    "modality": "text->text",
    "tokenizer": "DeepSeek"
  }
}