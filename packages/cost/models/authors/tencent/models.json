{
  "hunyuan-a13b-instruct:free": {
    "id": "hunyuan-a13b-instruct:free",
    "name": "Tencent: Hunyuan A13B Instruct (free)",
    "author": "tencent",
    "description": "Hunyuan-A13B is a 13B active parameter Mixture-of-Experts (MoE) language model developed by Tencent, with a total parameter count of 80B and support for reasoning via Chain-of-Thought. It offers competitive benchmark performance across mathematics, science, coding, and multi-turn reasoning tasks, while maintaining high inference efficiency via Grouped Query Attention (GQA) and quantization support (FP8, GPTQ, etc.).",
    "contextLength": 32768,
    "maxOutputTokens": null,
    "created": "2025-07-08T15:14:24.000Z",
    "modality": "text->text",
    "tokenizer": "Other"
  },
  "hunyuan-a13b-instruct": {
    "id": "hunyuan-a13b-instruct",
    "name": "Tencent: Hunyuan A13B Instruct",
    "author": "tencent",
    "description": "Hunyuan-A13B is a 13B active parameter Mixture-of-Experts (MoE) language model developed by Tencent, with a total parameter count of 80B and support for reasoning via Chain-of-Thought. It offers competitive benchmark performance across mathematics, science, coding, and multi-turn reasoning tasks, while maintaining high inference efficiency via Grouped Query Attention (GQA) and quantization support (FP8, GPTQ, etc.).",
    "contextLength": 32768,
    "maxOutputTokens": null,
    "created": "2025-07-08T15:14:24.000Z",
    "modality": "text->text",
    "tokenizer": "Other"
  }
}