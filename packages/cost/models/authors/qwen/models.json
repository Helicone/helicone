{
  "qwen3-30b-a3b-instruct-2507": {
    "id": "qwen3-30b-a3b-instruct-2507",
    "name": "Qwen: Qwen3 30B A3B Instruct 2507",
    "author": "qwen",
    "description": "Qwen3-30B-A3B-Instruct-2507 is a 30.5B-parameter mixture-of-experts language model from Qwen, with 3.3B active parameters per inference. It operates in non-thinking mode and is designed for high-quality instruction following, multilingual understanding, and agentic tool use. Post-trained on instruction data, it demonstrates competitive performance across reasoning (AIME, ZebraLogic), coding (MultiPL-E, LiveCodeBench), and alignment (IFEval, WritingBench) benchmarks. It outperforms its non-instruct variant on subjective and open-ended tasks while retaining strong factual and coding performance.",
    "contextLength": 131072,
    "maxOutputTokens": 32768,
    "created": "2025-07-29T16:36:05.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen3"
  },
  "qwen3-235b-a22b-thinking-2507": {
    "id": "qwen3-235b-a22b-thinking-2507",
    "name": "Qwen: Qwen3 235B A22B Thinking 2507",
    "author": "qwen",
    "description": "Qwen3-235B-A22B-Thinking-2507 is a high-performance, open-weight Mixture-of-Experts (MoE) language model optimized for complex reasoning tasks. It activates 22B of its 235B parameters per forward pass and natively supports up to 262,144 tokens of context. This \"thinking-only\" variant enhances structured logical reasoning, mathematics, science, and long-form generation, showing strong benchmark performance across AIME, SuperGPQA, LiveCodeBench, and MMLU-Redux. It enforces a special reasoning mode (</think>) and is designed for high-token outputs (up to 81,920 tokens) in challenging domains.\n\nThe model is instruction-tuned and excels at step-by-step reasoning, tool use, agentic workflows, and multilingual tasks. This release represents the most capable open-source variant in the Qwen3-235B series, surpassing many closed models in structured reasoning use cases.",
    "contextLength": 262144,
    "maxOutputTokens": null,
    "created": "2025-07-25T13:19:17.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen3"
  },
  "qwen3-coder:free": {
    "id": "qwen3-coder:free",
    "name": "Qwen: Qwen3 Coder  (free)",
    "author": "qwen",
    "description": "Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation model developed by the Qwen team. It is optimized for agentic coding tasks such as function calling, tool use, and long-context reasoning over repositories. The model features 480 billion total parameters, with 35 billion active per forward pass (8 out of 160 experts).\n\nPricing for the Alibaba endpoints varies by context length. Once a request is greater than 128k input tokens, the higher pricing is used.",
    "contextLength": 262144,
    "maxOutputTokens": null,
    "created": "2025-07-23T00:29:06.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen3"
  },
  "qwen3-coder": {
    "id": "qwen3-coder",
    "name": "Qwen: Qwen3 Coder ",
    "author": "qwen",
    "description": "Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation model developed by the Qwen team. It is optimized for agentic coding tasks such as function calling, tool use, and long-context reasoning over repositories. The model features 480 billion total parameters, with 35 billion active per forward pass (8 out of 160 experts).\n\nPricing for the Alibaba endpoints varies by context length. Once a request is greater than 128k input tokens, the higher pricing is used.",
    "contextLength": 262144,
    "maxOutputTokens": null,
    "created": "2025-07-23T00:29:06.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen3"
  },
  "qwen3-235b-a22b-2507": {
    "id": "qwen3-235b-a22b-2507",
    "name": "Qwen: Qwen3 235B A22B Instruct 2507",
    "author": "qwen",
    "description": "Qwen3-235B-A22B-Instruct-2507 is a multilingual, instruction-tuned mixture-of-experts language model based on the Qwen3-235B architecture, with 22B active parameters per forward pass. It is optimized for general-purpose text generation, including instruction following, logical reasoning, math, code, and tool usage. The model supports a native 262K context length and does not implement \"thinking mode\" (<think> blocks).\n\nCompared to its base variant, this version delivers significant gains in knowledge coverage, long-context reasoning, coding benchmarks, and alignment with open-ended tasks. It is particularly strong on multilingual understanding, math reasoning (e.g., AIME, HMMT), and alignment evaluations like Arena-Hard and WritingBench.",
    "contextLength": 262144,
    "maxOutputTokens": null,
    "created": "2025-07-21T17:39:15.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen3"
  },
  "qwen3-4b:free": {
    "id": "qwen3-4b:free",
    "name": "Qwen: Qwen3 4B (free)",
    "author": "qwen",
    "description": "Qwen3-4B is a 4 billion parameter dense language model from the Qwen3 series, designed to support both general-purpose and reasoning-intensive tasks. It introduces a dual-mode architecture—thinking and non-thinking—allowing dynamic switching between high-precision logical reasoning and efficient dialogue generation. This makes it well-suited for multi-turn chat, instruction following, and complex agent workflows.",
    "contextLength": 40960,
    "maxOutputTokens": null,
    "created": "2025-04-30T16:38:24.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen3"
  },
  "qwen3-30b-a3b:free": {
    "id": "qwen3-30b-a3b:free",
    "name": "Qwen: Qwen3 30B A3B (free)",
    "author": "qwen",
    "description": "Qwen3, the latest generation in the Qwen large language model series, features both dense and mixture-of-experts (MoE) architectures to excel in reasoning, multilingual support, and advanced agent tasks. Its unique ability to switch seamlessly between a thinking mode for complex reasoning and a non-thinking mode for efficient dialogue ensures versatile, high-quality performance.\n\nSignificantly outperforming prior models like QwQ and Qwen2.5, Qwen3 delivers superior mathematics, coding, commonsense reasoning, creative writing, and interactive dialogue capabilities. The Qwen3-30B-A3B variant includes 30.5 billion parameters (3.3 billion activated), 48 layers, 128 experts (8 activated per task), and supports up to 131K token contexts with YaRN, setting a new standard among open-source models.",
    "contextLength": 40960,
    "maxOutputTokens": null,
    "created": "2025-04-28T22:16:44.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen3"
  },
  "qwen3-30b-a3b": {
    "id": "qwen3-30b-a3b",
    "name": "Qwen: Qwen3 30B A3B",
    "author": "qwen",
    "description": "Qwen3, the latest generation in the Qwen large language model series, features both dense and mixture-of-experts (MoE) architectures to excel in reasoning, multilingual support, and advanced agent tasks. Its unique ability to switch seamlessly between a thinking mode for complex reasoning and a non-thinking mode for efficient dialogue ensures versatile, high-quality performance.\n\nSignificantly outperforming prior models like QwQ and Qwen2.5, Qwen3 delivers superior mathematics, coding, commonsense reasoning, creative writing, and interactive dialogue capabilities. The Qwen3-30B-A3B variant includes 30.5 billion parameters (3.3 billion activated), 48 layers, 128 experts (8 activated per task), and supports up to 131K token contexts with YaRN, setting a new standard among open-source models.",
    "contextLength": 40960,
    "maxOutputTokens": null,
    "created": "2025-04-28T22:16:44.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen3"
  },
  "qwen3-8b:free": {
    "id": "qwen3-8b:free",
    "name": "Qwen: Qwen3 8B (free)",
    "author": "qwen",
    "description": "Qwen3-8B is a dense 8.2B parameter causal language model from the Qwen3 series, designed for both reasoning-heavy tasks and efficient dialogue. It supports seamless switching between \"thinking\" mode for math, coding, and logical inference, and \"non-thinking\" mode for general conversation. The model is fine-tuned for instruction-following, agent integration, creative writing, and multilingual use across 100+ languages and dialects. It natively supports a 32K token context window and can extend to 131K tokens with YaRN scaling.",
    "contextLength": 40960,
    "maxOutputTokens": 40960,
    "created": "2025-04-28T21:43:52.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen3"
  },
  "qwen3-8b": {
    "id": "qwen3-8b",
    "name": "Qwen: Qwen3 8B",
    "author": "qwen",
    "description": "Qwen3-8B is a dense 8.2B parameter causal language model from the Qwen3 series, designed for both reasoning-heavy tasks and efficient dialogue. It supports seamless switching between \"thinking\" mode for math, coding, and logical inference, and \"non-thinking\" mode for general conversation. The model is fine-tuned for instruction-following, agent integration, creative writing, and multilingual use across 100+ languages and dialects. It natively supports a 32K token context window and can extend to 131K tokens with YaRN scaling.",
    "contextLength": 128000,
    "maxOutputTokens": 20000,
    "created": "2025-04-28T21:43:52.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen3"
  },
  "qwen3-14b:free": {
    "id": "qwen3-14b:free",
    "name": "Qwen: Qwen3 14B (free)",
    "author": "qwen",
    "description": "Qwen3-14B is a dense 14.8B parameter causal language model from the Qwen3 series, designed for both complex reasoning and efficient dialogue. It supports seamless switching between a \"thinking\" mode for tasks like math, programming, and logical inference, and a \"non-thinking\" mode for general-purpose conversation. The model is fine-tuned for instruction-following, agent tool use, creative writing, and multilingual tasks across 100+ languages and dialects. It natively handles 32K token contexts and can extend to 131K tokens using YaRN-based scaling.",
    "contextLength": 40960,
    "maxOutputTokens": null,
    "created": "2025-04-28T21:41:18.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen3"
  },
  "qwen3-14b": {
    "id": "qwen3-14b",
    "name": "Qwen: Qwen3 14B",
    "author": "qwen",
    "description": "Qwen3-14B is a dense 14.8B parameter causal language model from the Qwen3 series, designed for both complex reasoning and efficient dialogue. It supports seamless switching between a \"thinking\" mode for tasks like math, programming, and logical inference, and a \"non-thinking\" mode for general-purpose conversation. The model is fine-tuned for instruction-following, agent tool use, creative writing, and multilingual tasks across 100+ languages and dialects. It natively handles 32K token contexts and can extend to 131K tokens using YaRN-based scaling.",
    "contextLength": 40960,
    "maxOutputTokens": null,
    "created": "2025-04-28T21:41:18.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen3"
  },
  "qwen3-32b": {
    "id": "qwen3-32b",
    "name": "Qwen: Qwen3 32B",
    "author": "qwen",
    "description": "Qwen3-32B is a dense 32.8B parameter causal language model from the Qwen3 series, optimized for both complex reasoning and efficient dialogue. It supports seamless switching between a \"thinking\" mode for tasks like math, coding, and logical inference, and a \"non-thinking\" mode for faster, general-purpose conversation. The model demonstrates strong performance in instruction-following, agent tool use, creative writing, and multilingual tasks across 100+ languages and dialects. It natively handles 32K token contexts and can extend to 131K tokens using YaRN-based scaling. ",
    "contextLength": 40960,
    "maxOutputTokens": null,
    "created": "2025-04-28T21:32:25.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen3"
  },
  "qwen3-235b-a22b:free": {
    "id": "qwen3-235b-a22b:free",
    "name": "Qwen: Qwen3 235B A22B (free)",
    "author": "qwen",
    "description": "Qwen3-235B-A22B is a 235B parameter mixture-of-experts (MoE) model developed by Qwen, activating 22B parameters per forward pass. It supports seamless switching between a \"thinking\" mode for complex reasoning, math, and code tasks, and a \"non-thinking\" mode for general conversational efficiency. The model demonstrates strong reasoning ability, multilingual support (100+ languages and dialects), advanced instruction-following, and agent tool-calling capabilities. It natively handles a 32K token context window and extends up to 131K tokens using YaRN-based scaling.",
    "contextLength": 131072,
    "maxOutputTokens": null,
    "created": "2025-04-28T21:29:17.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen3"
  },
  "qwen3-235b-a22b": {
    "id": "qwen3-235b-a22b",
    "name": "Qwen: Qwen3 235B A22B",
    "author": "qwen",
    "description": "Qwen3-235B-A22B is a 235B parameter mixture-of-experts (MoE) model developed by Qwen, activating 22B parameters per forward pass. It supports seamless switching between a \"thinking\" mode for complex reasoning, math, and code tasks, and a \"non-thinking\" mode for general conversational efficiency. The model demonstrates strong reasoning ability, multilingual support (100+ languages and dialects), advanced instruction-following, and agent tool-calling capabilities. It natively handles a 32K token context window and extends up to 131K tokens using YaRN-based scaling.",
    "contextLength": 40960,
    "maxOutputTokens": 40960,
    "created": "2025-04-28T21:29:17.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen3"
  },
  "qwen2.5-vl-32b-instruct:free": {
    "id": "qwen2.5-vl-32b-instruct:free",
    "name": "Qwen: Qwen2.5 VL 32B Instruct (free)",
    "author": "qwen",
    "description": "Qwen2.5-VL-32B is a multimodal vision-language model fine-tuned through reinforcement learning for enhanced mathematical reasoning, structured outputs, and visual problem-solving capabilities. It excels at visual analysis tasks, including object recognition, textual interpretation within images, and precise event localization in extended videos. Qwen2.5-VL-32B demonstrates state-of-the-art performance across multimodal benchmarks such as MMMU, MathVista, and VideoMME, while maintaining strong reasoning and clarity in text-based tasks like MMLU, mathematical problem-solving, and code generation.",
    "contextLength": 8192,
    "maxOutputTokens": null,
    "created": "2025-03-24T18:10:38.000Z",
    "modality": "text+image->text",
    "tokenizer": "Qwen"
  },
  "qwen2.5-vl-32b-instruct": {
    "id": "qwen2.5-vl-32b-instruct",
    "name": "Qwen: Qwen2.5 VL 32B Instruct",
    "author": "qwen",
    "description": "Qwen2.5-VL-32B is a multimodal vision-language model fine-tuned through reinforcement learning for enhanced mathematical reasoning, structured outputs, and visual problem-solving capabilities. It excels at visual analysis tasks, including object recognition, textual interpretation within images, and precise event localization in extended videos. Qwen2.5-VL-32B demonstrates state-of-the-art performance across multimodal benchmarks such as MMMU, MathVista, and VideoMME, while maintaining strong reasoning and clarity in text-based tasks like MMLU, mathematical problem-solving, and code generation.",
    "contextLength": 16384,
    "maxOutputTokens": null,
    "created": "2025-03-24T18:10:38.000Z",
    "modality": "text+image->text",
    "tokenizer": "Qwen"
  },
  "qwq-32b:free": {
    "id": "qwq-32b:free",
    "name": "Qwen: QwQ 32B (free)",
    "author": "qwen",
    "description": "QwQ is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems. QwQ-32B is the medium-sized reasoning model, which is capable of achieving competitive performance against state-of-the-art reasoning models, e.g., DeepSeek-R1, o1-mini.",
    "contextLength": 32768,
    "maxOutputTokens": null,
    "created": "2025-03-05T21:06:54.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen"
  },
  "qwq-32b": {
    "id": "qwq-32b",
    "name": "Qwen: QwQ 32B",
    "author": "qwen",
    "description": "QwQ is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems. QwQ-32B is the medium-sized reasoning model, which is capable of achieving competitive performance against state-of-the-art reasoning models, e.g., DeepSeek-R1, o1-mini.",
    "contextLength": 131072,
    "maxOutputTokens": null,
    "created": "2025-03-05T21:06:54.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen"
  },
  "qwen-vl-plus": {
    "id": "qwen-vl-plus",
    "name": "Qwen: Qwen VL Plus",
    "author": "qwen",
    "description": "Qwen's Enhanced Large Visual Language Model. Significantly upgraded for detailed recognition capabilities and text recognition abilities, supporting ultra-high pixel resolutions up to millions of pixels and extreme aspect ratios for image input. It delivers significant performance across a broad range of visual tasks.\n",
    "contextLength": 7500,
    "maxOutputTokens": 1500,
    "created": "2025-02-05T04:54:15.000Z",
    "modality": "text+image->text",
    "tokenizer": "Qwen"
  },
  "qwen-vl-max": {
    "id": "qwen-vl-max",
    "name": "Qwen: Qwen VL Max",
    "author": "qwen",
    "description": "Qwen VL Max is a visual understanding model with 7500 tokens context length. It excels in delivering optimal performance for a broader spectrum of complex tasks.\n",
    "contextLength": 7500,
    "maxOutputTokens": 1500,
    "created": "2025-02-01T18:25:04.000Z",
    "modality": "text+image->text",
    "tokenizer": "Qwen"
  },
  "qwen-turbo": {
    "id": "qwen-turbo",
    "name": "Qwen: Qwen-Turbo",
    "author": "qwen",
    "description": "Qwen-Turbo, based on Qwen2.5, is a 1M context model that provides fast speed and low cost, suitable for simple tasks.",
    "contextLength": 1000000,
    "maxOutputTokens": 8192,
    "created": "2025-02-01T11:56:14.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen"
  },
  "qwen2.5-vl-72b-instruct:free": {
    "id": "qwen2.5-vl-72b-instruct:free",
    "name": "Qwen: Qwen2.5 VL 72B Instruct (free)",
    "author": "qwen",
    "description": "Qwen2.5-VL is proficient in recognizing common objects such as flowers, birds, fish, and insects. It is also highly capable of analyzing texts, charts, icons, graphics, and layouts within images.",
    "contextLength": 32768,
    "maxOutputTokens": null,
    "created": "2025-02-01T11:45:11.000Z",
    "modality": "text+image->text",
    "tokenizer": "Qwen"
  },
  "qwen2.5-vl-72b-instruct": {
    "id": "qwen2.5-vl-72b-instruct",
    "name": "Qwen: Qwen2.5 VL 72B Instruct",
    "author": "qwen",
    "description": "Qwen2.5-VL is proficient in recognizing common objects such as flowers, birds, fish, and insects. It is also highly capable of analyzing texts, charts, icons, graphics, and layouts within images.",
    "contextLength": 32000,
    "maxOutputTokens": null,
    "created": "2025-02-01T11:45:11.000Z",
    "modality": "text+image->text",
    "tokenizer": "Qwen"
  },
  "qwen-plus": {
    "id": "qwen-plus",
    "name": "Qwen: Qwen-Plus",
    "author": "qwen",
    "description": "Qwen-Plus, based on the Qwen2.5 foundation model, is a 131K context model with a balanced performance, speed, and cost combination.",
    "contextLength": 131072,
    "maxOutputTokens": 8192,
    "created": "2025-02-01T11:37:20.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen"
  },
  "qwen-max": {
    "id": "qwen-max",
    "name": "Qwen: Qwen-Max ",
    "author": "qwen",
    "description": "Qwen-Max, based on Qwen2.5, provides the best inference performance among [Qwen models](/qwen), especially for complex multi-step tasks. It's a large-scale MoE model that has been pretrained on over 20 trillion tokens and further post-trained with curated Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) methodologies. The parameter count is unknown.",
    "contextLength": 32768,
    "maxOutputTokens": 8192,
    "created": "2025-02-01T09:31:29.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen"
  },
  "qwq-32b-preview": {
    "id": "qwq-32b-preview",
    "name": "Qwen: QwQ 32B Preview",
    "author": "qwen",
    "description": "QwQ-32B-Preview is an experimental research model focused on AI reasoning capabilities developed by the Qwen Team. As a preview release, it demonstrates promising analytical abilities while having several important limitations:\n\n1. **Language Mixing and Code-Switching**: The model may mix languages or switch between them unexpectedly, affecting response clarity.\n2. **Recursive Reasoning Loops**: The model may enter circular reasoning patterns, leading to lengthy responses without a conclusive answer.\n3. **Safety and Ethical Considerations**: The model requires enhanced safety measures to ensure reliable and secure performance, and users should exercise caution when deploying it.\n4. **Performance and Benchmark Limitations**: The model excels in math and coding but has room for improvement in other areas, such as common sense reasoning and nuanced language understanding.\n\n",
    "contextLength": 32768,
    "maxOutputTokens": null,
    "created": "2024-11-28T00:42:21.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen"
  },
  "qwen-2.5-coder-32b-instruct:free": {
    "id": "qwen-2.5-coder-32b-instruct:free",
    "name": "Qwen2.5 Coder 32B Instruct (free)",
    "author": "qwen",
    "description": "Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). Qwen2.5-Coder brings the following improvements upon CodeQwen1.5:\n\n- Significantly improvements in **code generation**, **code reasoning** and **code fixing**. \n- A more comprehensive foundation for real-world applications such as **Code Agents**. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies.\n\nTo read more about its evaluation results, check out [Qwen 2.5 Coder's blog](https://qwenlm.github.io/blog/qwen2.5-coder-family/).",
    "contextLength": 32768,
    "maxOutputTokens": null,
    "created": "2024-11-11T23:40:00.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen"
  },
  "qwen-2.5-coder-32b-instruct": {
    "id": "qwen-2.5-coder-32b-instruct",
    "name": "Qwen2.5 Coder 32B Instruct",
    "author": "qwen",
    "description": "Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). Qwen2.5-Coder brings the following improvements upon CodeQwen1.5:\n\n- Significantly improvements in **code generation**, **code reasoning** and **code fixing**. \n- A more comprehensive foundation for real-world applications such as **Code Agents**. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies.\n\nTo read more about its evaluation results, check out [Qwen 2.5 Coder's blog](https://qwenlm.github.io/blog/qwen2.5-coder-family/).",
    "contextLength": 32768,
    "maxOutputTokens": null,
    "created": "2024-11-11T23:40:00.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen"
  },
  "qwen-2.5-7b-instruct": {
    "id": "qwen-2.5-7b-instruct",
    "name": "Qwen2.5 7B Instruct",
    "author": "qwen",
    "description": "Qwen2.5 7B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\n- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n\n- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).",
    "contextLength": 65536,
    "maxOutputTokens": null,
    "created": "2024-10-16T00:00:00.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen"
  },
  "qwen-2.5-72b-instruct:free": {
    "id": "qwen-2.5-72b-instruct:free",
    "name": "Qwen2.5 72B Instruct (free)",
    "author": "qwen",
    "description": "Qwen2.5 72B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\n- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n\n- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).",
    "contextLength": 32768,
    "maxOutputTokens": null,
    "created": "2024-09-19T00:00:00.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen"
  },
  "qwen-2.5-72b-instruct": {
    "id": "qwen-2.5-72b-instruct",
    "name": "Qwen2.5 72B Instruct",
    "author": "qwen",
    "description": "Qwen2.5 72B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\n- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n\n- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).",
    "contextLength": 32768,
    "maxOutputTokens": null,
    "created": "2024-09-19T00:00:00.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen"
  },
  "qwen-2.5-vl-7b-instruct": {
    "id": "qwen-2.5-vl-7b-instruct",
    "name": "Qwen: Qwen2.5-VL 7B Instruct",
    "author": "qwen",
    "description": "Qwen2.5 VL 7B is a multimodal LLM from the Qwen Team with the following key enhancements:\n\n- SoTA understanding of images of various resolution & ratio: Qwen2.5-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.\n\n- Understanding videos of 20min+: Qwen2.5-VL can understand videos over 20 minutes for high-quality video-based question answering, dialog, content creation, etc.\n\n- Agent that can operate your mobiles, robots, etc.: with the abilities of complex reasoning and decision making, Qwen2.5-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions.\n\n- Multilingual Support: to serve global users, besides English and Chinese, Qwen2.5-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc.\n\nFor more details, see this [blog post](https://qwenlm.github.io/blog/qwen2-vl/) and [GitHub repo](https://github.com/QwenLM/Qwen2-VL).\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).",
    "contextLength": 32768,
    "maxOutputTokens": null,
    "created": "2024-08-28T00:00:00.000Z",
    "modality": "text+image->text",
    "tokenizer": "Qwen"
  },
  "qwen-2-72b-instruct": {
    "id": "qwen-2-72b-instruct",
    "name": "Qwen 2 72B Instruct",
    "author": "qwen",
    "description": "Qwen2 72B is a transformer-based model that excels in language understanding, multilingual capabilities, coding, mathematics, and reasoning.\n\nIt features SwiGLU activation, attention QKV bias, and group query attention. It is pretrained on extensive data with supervised finetuning and direct preference optimization.\n\nFor more details, see this [blog post](https://qwenlm.github.io/blog/qwen2/) and [GitHub repo](https://github.com/QwenLM/Qwen2).\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).",
    "contextLength": 32768,
    "maxOutputTokens": 4096,
    "created": "2024-06-07T00:00:00.000Z",
    "modality": "text->text",
    "tokenizer": "Qwen"
  }
}