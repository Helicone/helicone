{
  "phi-4-reasoning-plus": {
    "id": "phi-4-reasoning-plus",
    "name": "Microsoft: Phi 4 Reasoning Plus",
    "author": "microsoft",
    "description": "Phi-4-reasoning-plus is an enhanced 14B parameter model from Microsoft, fine-tuned from Phi-4 with additional reinforcement learning to boost accuracy on math, science, and code reasoning tasks. It uses the same dense decoder-only transformer architecture as Phi-4, but generates longer, more comprehensive outputs structured into a step-by-step reasoning trace and final answer.\n\nWhile it offers improved benchmark scores over Phi-4-reasoning across tasks like AIME, OmniMath, and HumanEvalPlus, its responses are typically ~50% longer, resulting in higher latency. Designed for English-only applications, it is well-suited for structured reasoning workflows where output quality takes priority over response speed.",
    "contextLength": 32768,
    "maxOutputTokens": null,
    "created": "2025-05-01T20:22:41.000Z",
    "modality": "text->text",
    "tokenizer": "Other"
  },
  "mai-ds-r1:free": {
    "id": "mai-ds-r1:free",
    "name": "Microsoft: MAI DS R1 (free)",
    "author": "microsoft",
    "description": "MAI-DS-R1 is a post-trained variant of DeepSeek-R1 developed by the Microsoft AI team to improve the model’s responsiveness on previously blocked topics while enhancing its safety profile. Built on top of DeepSeek-R1’s reasoning foundation, it integrates 110k examples from the Tulu-3 SFT dataset and 350k internally curated multilingual safety-alignment samples. The model retains strong reasoning, coding, and problem-solving capabilities, while unblocking a wide range of prompts previously restricted in R1.\n\nMAI-DS-R1 demonstrates improved performance on harm mitigation benchmarks and maintains competitive results across general reasoning tasks. It surpasses R1-1776 in satisfaction metrics for blocked queries and reduces leakage in harmful content categories. The model is based on a transformer MoE architecture and is suitable for general-purpose use cases, excluding high-stakes domains such as legal, medical, or autonomous systems.",
    "contextLength": 163840,
    "maxOutputTokens": null,
    "created": "2025-04-21T00:08:20.000Z",
    "modality": "text->text",
    "tokenizer": "DeepSeek"
  },
  "mai-ds-r1": {
    "id": "mai-ds-r1",
    "name": "Microsoft: MAI DS R1",
    "author": "microsoft",
    "description": "MAI-DS-R1 is a post-trained variant of DeepSeek-R1 developed by the Microsoft AI team to improve the model’s responsiveness on previously blocked topics while enhancing its safety profile. Built on top of DeepSeek-R1’s reasoning foundation, it integrates 110k examples from the Tulu-3 SFT dataset and 350k internally curated multilingual safety-alignment samples. The model retains strong reasoning, coding, and problem-solving capabilities, while unblocking a wide range of prompts previously restricted in R1.\n\nMAI-DS-R1 demonstrates improved performance on harm mitigation benchmarks and maintains competitive results across general reasoning tasks. It surpasses R1-1776 in satisfaction metrics for blocked queries and reduces leakage in harmful content categories. The model is based on a transformer MoE architecture and is suitable for general-purpose use cases, excluding high-stakes domains such as legal, medical, or autonomous systems.",
    "contextLength": 163840,
    "maxOutputTokens": null,
    "created": "2025-04-21T00:08:20.000Z",
    "modality": "text->text",
    "tokenizer": "DeepSeek"
  },
  "phi-4-multimodal-instruct": {
    "id": "phi-4-multimodal-instruct",
    "name": "Microsoft: Phi 4 Multimodal Instruct",
    "author": "microsoft",
    "description": "Phi-4 Multimodal Instruct is a versatile 5.6B parameter foundation model that combines advanced reasoning and instruction-following capabilities across both text and visual inputs, providing accurate text outputs. The unified architecture enables efficient, low-latency inference, suitable for edge and mobile deployments. Phi-4 Multimodal Instruct supports text inputs in multiple languages including Arabic, Chinese, English, French, German, Japanese, Spanish, and more, with visual input optimized primarily for English. It delivers impressive performance on multimodal tasks involving mathematical, scientific, and document reasoning, providing developers and enterprises a powerful yet compact model for sophisticated interactive applications. For more information, see the [Phi-4 Multimodal blog post](https://azure.microsoft.com/en-us/blog/empowering-innovation-the-next-generation-of-the-phi-family/).\n",
    "contextLength": 131072,
    "maxOutputTokens": null,
    "created": "2025-03-08T01:11:24.000Z",
    "modality": "text+image->text",
    "tokenizer": "Other"
  },
  "phi-4": {
    "id": "phi-4",
    "name": "Microsoft: Phi 4",
    "author": "microsoft",
    "description": "[Microsoft Research](/microsoft) Phi-4 is designed to perform well in complex reasoning tasks and can operate efficiently in situations with limited memory or where quick responses are needed. \n\nAt 14 billion parameters, it was trained on a mix of high-quality synthetic datasets, data from curated websites, and academic materials. It has undergone careful improvement to follow instructions accurately and maintain strong safety standards. It works best with English language inputs.\n\nFor more information, please see [Phi-4 Technical Report](https://arxiv.org/pdf/2412.08905)\n",
    "contextLength": 16384,
    "maxOutputTokens": null,
    "created": "2025-01-10T06:17:52.000Z",
    "modality": "text->text",
    "tokenizer": "Other"
  },
  "phi-3.5-mini-128k-instruct": {
    "id": "phi-3.5-mini-128k-instruct",
    "name": "Microsoft: Phi-3.5 Mini 128K Instruct",
    "author": "microsoft",
    "description": "Phi-3.5 models are lightweight, state-of-the-art open models. These models were trained with Phi-3 datasets that include both synthetic data and the filtered, publicly available websites data, with a focus on high quality and reasoning-dense properties. Phi-3.5 Mini uses 3.8B parameters, and is a dense decoder-only transformer model using the same tokenizer as [Phi-3 Mini](/models/microsoft/phi-3-mini-128k-instruct).\n\nThe models underwent a rigorous enhancement process, incorporating both supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures. When assessed against benchmarks that test common sense, language understanding, math, code, long context and logical reasoning, Phi-3.5 models showcased robust and state-of-the-art performance among models with less than 13 billion parameters.",
    "contextLength": 128000,
    "maxOutputTokens": null,
    "created": "2024-08-21T00:00:00.000Z",
    "modality": "text->text",
    "tokenizer": "Other"
  },
  "phi-3-mini-128k-instruct": {
    "id": "phi-3-mini-128k-instruct",
    "name": "Microsoft: Phi-3 Mini 128K Instruct",
    "author": "microsoft",
    "description": "Phi-3 Mini is a powerful 3.8B parameter model designed for advanced language understanding, reasoning, and instruction following. Optimized through supervised fine-tuning and preference adjustments, it excels in tasks involving common sense, mathematics, logical reasoning, and code processing.\n\nAt time of release, Phi-3 Medium demonstrated state-of-the-art performance among lightweight models. This model is static, trained on an offline dataset with an October 2023 cutoff date.",
    "contextLength": 128000,
    "maxOutputTokens": null,
    "created": "2024-05-26T00:00:00.000Z",
    "modality": "text->text",
    "tokenizer": "Other"
  },
  "phi-3-medium-128k-instruct": {
    "id": "phi-3-medium-128k-instruct",
    "name": "Microsoft: Phi-3 Medium 128K Instruct",
    "author": "microsoft",
    "description": "Phi-3 128K Medium is a powerful 14-billion parameter model designed for advanced language understanding, reasoning, and instruction following. Optimized through supervised fine-tuning and preference adjustments, it excels in tasks involving common sense, mathematics, logical reasoning, and code processing.\n\nAt time of release, Phi-3 Medium demonstrated state-of-the-art performance among lightweight models. In the MMLU-Pro eval, the model even comes close to a Llama3 70B level of performance.\n\nFor 4k context length, try [Phi-3 Medium 4K](/models/microsoft/phi-3-medium-4k-instruct).",
    "contextLength": 128000,
    "maxOutputTokens": null,
    "created": "2024-05-24T00:00:00.000Z",
    "modality": "text->text",
    "tokenizer": "Other"
  },
  "wizardlm-2-8x22b": {
    "id": "wizardlm-2-8x22b",
    "name": "WizardLM-2 8x22B",
    "author": "microsoft",
    "description": "WizardLM-2 8x22B is Microsoft AI's most advanced Wizard model. It demonstrates highly competitive performance compared to leading proprietary models, and it consistently outperforms all existing state-of-the-art opensource models.\n\nIt is an instruct finetune of [Mixtral 8x22B](/models/mistralai/mixtral-8x22b).\n\nTo read more about the model release, [click here](https://wizardlm.github.io/WizardLM2/).\n\n#moe",
    "contextLength": 65536,
    "maxOutputTokens": 65536,
    "created": "2024-04-16T00:00:00.000Z",
    "modality": "text->text",
    "tokenizer": "Mistral"
  }
}