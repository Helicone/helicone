---
title: "AI Gateway Overview"
sidebarTitle: "Overview"
description: "Use any LLM provider through a single OpenAI-compatible API with intelligent routing, fallbacks, and unified observability"
---

Helicone AI Gateway provides a unified API for 100+ LLM providers through the OpenAI SDK format. Instead of learning different SDKs and APIs for each provider, use one familiar interface to access any model with intelligent routing, automatic fallbacks, and complete observability.

## Why Use AI Gateway?

<CardGroup cols={2}>
<Card title="One SDK for All Models" icon="code">
  Use OpenAI SDK to access GPT, Claude, Gemini, and 100+ other models
</Card>
<Card title="No Rate Limits" icon="gauge">
  Skip provider tier restrictions - use credits with 0% markup
</Card>
<Card title="Always Online" icon="shield-check">
  Automatic failover across providers keeps your app running
</Card>
<Card title="Unified Observability" icon="chart-line">
  Track usage, costs, and performance across all providers in one dashboard
</Card>
</CardGroup>

## How It Works

The AI Gateway sits between your application and LLM providers, acting as a unified translation layer:

1. **You make one request** - Use the OpenAI SDK format, regardless of which provider you want
2. **We translate & route** - Helicone converts your request to the correct provider format (Anthropic, Google, etc.)
3. **Provider responds** - The LLM provider processes your request
4. **We log & return** - You get the response back while we capture metrics, costs, and errors

All through a single endpoint: `https://ai-gateway.helicone.ai`

<Note>
  With credits, we manage provider API keys for you. Your requests automatically work with OpenAI, Anthropic, Google, and 100+ other providers without signing up for each one.
</Note>

## Quick Example

**❌ Old way - manage your own OpenAI account**

```typescript
import { OpenAI } from "openai";

const client = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
});

const response = await client.chat.completions.create({
  model: "gpt-4o",
  messages: [{ role: "user", content: "Hello!" }]
});
```

**✅ With Helicone - access any provider, no rate limits**

```typescript
import { OpenAI } from "openai";

const client = new OpenAI({
  baseURL: "https://ai-gateway.helicone.ai",
  apiKey: process.env.HELICONE_API_KEY,
});

const response = await client.chat.completions.create({
  model: "gpt-4o",  // Or: claude-sonnet-4, gemini-2.5-flash, etc.
  messages: [{ role: "user", content: "Hello!" }]
});
```

## Helicone vs OpenRouter

| Feature | Helicone | OpenRouter |
|---------|----------|------------|
| **Observability Dashboard** | ✅ Full-featured with sessions, users, custom properties | ❌ Extremely basic (requests/costs per model only) |
| **Pricing** | ✅ 0% markup on credits | ❌ 5.5% markup |
| **Session Tracking** | ✅ Debug multi-step AI workflows | ❌ Not available |
| **Prompt Management** | ✅ Version control and deploy without code changes | ❌ Not available |
| **Caching** | ✅ Built-in request caching | ❌ Not available |
| **Rate Limiting** | ✅ Custom rate limits per user/key | ❌ Not available |
| **Self-Hostable** | ✅ Open source | ❌ Closed source |
| **BYOK** | ✅ Bring your own provider keys | ✅ Bring your own keys |
| **Automatic Fallbacks** | ✅ Configure fallback chains | ✅ Available |
| **Best for** | Production apps needing observability & debugging | Simple model access |

## Next Steps

<CardGroup cols={2}>
<Card title="Get Started in 5 Minutes" icon="rocket" href="/getting-started/quick-start">
  Set up AI Gateway and make your first request
</Card>
<Card title="Browse Model Registry" icon="list" href="https://helicone.ai/models">
  See all supported models and provider formats
</Card>
<Card title="Provider Routing" icon="route" href="/gateway/provider-routing">
  Configure automatic routing and fallbacks for reliability
</Card>
<Card title="Prompt Integration" icon="wand-magic-sparkles" href="/gateway/prompt-integration">
  Deploy and manage prompts through the gateway
</Card>
</CardGroup>

<Note>
Want to integrate a new model provider to the AI Gateway? Check out our [tutorial](/references/provider-integration) for detailed instructions.
</Note>
