---
title: "Provider Routing"
sidebarTitle: "Provider Routing"
description: "Automatic model routing across 100+ providers for reliability and performance"
---

Never worry about provider outages again. The AI Gateway automatically routes your requests to the best available provider, with instant failover when things go wrong.

## The Problem

Using LLMs in production means dealing with:
- **Provider outages** that break your app
- **Rate limits** that block your users
- **Regional restrictions** that limit availability
- **Vendor lock-in** that prevents optimization

## The Solution

Provider routing gives you access to the same model across multiple providers. When OpenAI goes down, your app automatically switches to Azure or AWS Bedrock using Helicone's managed keys. When you hit rate limits, traffic flows to another provider. All without setup or code changes.


## How It Works

<Steps>
<Step title="You request a model">
  Your app asks for `gpt-4o-mini` just like normal
</Step>
<Step title="Gateway finds providers">
  Consults the Model Registry to find all providers offering this model
</Step>
<Step title="Smart routing">
  Applies sorting algorithm (cheapest first) then attempts providers
</Step>
<Step title="Automatic failover">
  If a provider fails, instantly tries the next one
</Step>
</Steps>

The result? Your request succeeds even when providers fail.

## You request a model

The simplest approach lets the gateway handle everything:

```typescript
// Just specify the model - gateway handles the rest
const response = await client.chat.completions.create({
  model: "gpt-4o-mini",
  messages: [{ role: "user", content: "Hello!" }]
});

// Behind the scenes, the gateway tries:
// OpenAI → Azure OpenAI → AWS Bedrock → Others
// Until one succeeds
```

### Routing Options

**Default (Recommended):** Just specify the model name

```typescript
model: "gpt-4o-mini"
```

The gateway automatically tries all providers offering this model, sorted by cost. Maximum uptime with zero configuration.

---

### Advanced Options

Only needed for specific requirements:

**Lock to specific provider:** `model: "gpt-4o-mini/openai"`
Use when: Compliance requires specific provider, or testing provider-exclusive features

**Custom deployment:** `model: "gpt-4o-mini/azure/clm1a2b3c"`
Use when: Regional compliance (EU data residency) or using provider credits

**Fallback chain:** `model: "model1,model2,model3"`
Use when: Need full control over exact fallback sequence

## How the Gateway Finds Models

The [Model Registry](https://helicone.ai/models) is our source of truth for which providers support which models. This powers intelligent routing.

### How to Use Provider Routing

#### Default: Use Credits (Recommended)

Zero configuration required. Helicone manages provider keys for you at 0% markup.

```typescript
model: "gpt-4o-mini"  // Automatically uses Helicone's keys
```

**Benefits:**
- No provider signups needed
- No rate limits from provider tiers
- Automatic access to all providers
- 0% markup pricing

#### Advanced: Bring Your Own Keys (BYOK)

For specific use cases like regional compliance or using provider credits, add your own keys in [Provider Settings](https://us.helicone.ai/providers).

<Note>
When you add a provider deployment, **ALL models and regions** that provider supports become available through your deployment.
</Note>

**Example:**

You add an Azure deployment in Brazil.

When you request any Azure-supported model:
```typescript
model: "gpt-4o-mini"  // Uses your Brazil deployment
```

The gateway uses your configured deployment for all requests.

### Passthrough Routing (Unknown Models)

The gateway forwards **ANY model/provider combination**, even if not in our registry:

```typescript
// Brand new model
model: "o3-preview/openai"

// Custom fine-tuned model
model: "ft:gpt-3.5-turbo:my-org::abc123/openai"
```

**Important:** Unknown models ONLY route through YOUR deployments (BYOK). The provider's API determines if the model is valid.

Cost tracking is best-effort for unknown models.

## Smart Routing Algorithm

When multiple deployments are available, the gateway intelligently selects which to use:

### Routing Priority
1. **Your deployments (BYOK)** - If configured, always tried first
2. **Helicone's managed keys (Credits)** - Automatic fallback for reliability

### Selection Logic
Within each priority level, we:
1. **Sort by cost** - Cheapest deployments first
2. **Load balance** - If costs are equal or unknown, randomly distribute requests

**Example with credits (default):**

You request `gpt-4o-mini` using credits.

**Routing order:**
1. Helicone tries all available providers (sorted by cheapest first)
2. Automatic failover if any provider fails

**Example with BYOK + credits:**

You have Azure Brazil + Azure US deployments configured.

**Routing order:**
1. Your cheapest deployment (e.g., Brazil if cheaper)
2. Your other deployments (e.g., US)
3. Helicone's managed keys as fallback

This ensures optimal cost while maintaining reliability.

## Failover Triggers

The gateway automatically tries the next provider when encountering these errors:

| Error | Description |
|-------|-------------|
| 429 | Rate limit errors |
| 401 | Authentication errors |
| 400 | Context length errors |
| 408 | Timeout errors |
| 500+ | Server errors |

<Note>
With credits, the gateway automatically tries all available providers. If you've configured your own keys (BYOK), those are tried first before falling back to Helicone's managed keys.
</Note>

## Real World Examples

### Scenario: OpenAI Outage

Your production app uses GPT-4. OpenAI goes down at 3am.

```typescript
// Your code doesn't change
const response = await client.chat.completions.create({
  model: "gpt-4o-mini",
  messages: [{ role: "user", content: "Process this customer request" }]
});
```

**What happens:** Gateway automatically fails over to Azure OpenAI, then AWS Bedrock if needed. Your app stays online, customers never notice.

### Scenario: Using Azure Credits

Your company has $100k in Azure credits to burn before year-end.

```typescript
// Prioritize Azure but keep fallback for reliability
const response = await client.chat.completions.create({
  model: "gpt-4o-mini/azure,gpt-4o-mini",  
  messages: messages
});
```

**What happens:** Tries your Azure deployment first (using credits), but falls back to other providers if Azure fails. Balances credit usage with reliability.

### Scenario: EU Compliance Requirements

GDPR requires EU customer data to stay in EU regions.

```typescript
// Use your custom EU deployment
await client.chat.completions.create({
  model: "gpt-4o/azure/eu-frankfurt-deployment",  // Your CUID
  messages: messages
});
```

**What happens:** Requests ONLY go through your Frankfurt deployment. No data leaves the EU.

## Next Steps

<CardGroup cols={2}>
<Card title="Browse Models" icon="grid" href="https://helicone.ai/models">
  Explore all available models and providers
</Card>
<Card title="Add Provider Keys" icon="key" href="https://us.helicone.ai/providers">
  Connect your provider accounts
</Card>
<Card title="Prompt Management" icon="wand-magic-sparkles" href="/gateway/prompt-integration">
  Combine routing with managed prompts
</Card>
</CardGroup>