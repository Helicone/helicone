---
title: "Provider Configuration"
sidebarTitle: "Providers"
description: "Configure LLM providers, their endpoints, and available models"
---

# Provider Configuration

Provider configuration defines which LLM providers your gateway can connect to and how to reach them. The gateway ships with comprehensive defaults for all major providers including OpenAI, Anthropic, Google Gemini, and Ollama.

## Why Configure Providers?

You'll typically override provider settings to:
- **Point to local instances** during development (e.g., local Ollama)
- **Use different API endpoints** for testing
- **Add custom models** to existing providers
- **Configure provider-specific settings** like API versions

## How It Works

Provider configurations merge with embedded defaults. You only need to specify the fields you want to override - the gateway automatically combines your overrides with the comprehensive defaults. This allows minimal configuration while maintaining full flexibility.

## Default Provider Support

The gateway includes [exhaustive default configurations](https://github.com/Helicone/helicone-router/blob/main/llm-proxy/config/embedded/providers.yaml) for all supported providers with their complete model lists and standard endpoints.

### Supported Providers

- **OpenAI** - GPT-4, GPT-3.5, and all other OpenAI models
- **Anthropic** - Claude 3 models (Opus, Sonnet, Haiku)
- **Google Gemini** - Gemini 1.5 Pro and Flash
- **AWS Bedrock** - All Bedrock-hosted models
- **Ollama** - Local model hosting
- **Groq** - High-speed inference
- **And many more...**

## Configuration Examples

### Basic Provider Override

Override just the fields you need to change:

```yaml
providers:
  openai:
    base-url: "https://api.openai.com/v1"
    models:
      - gpt-4
      - gpt-4o
      - gpt-4o-mini
  
  anthropic:
    base-url: "https://api.anthropic.com"
    version: "2023-06-01"
    models:
      - claude-3-opus
      - claude-3-sonnet
      - claude-3-haiku
```

### Local Development Setup

Point to local instances for development:

```yaml
providers:
  ollama:
    base-url: "http://localhost:11434"
    models:
      - llama3
      - deepseek-r1
      - custom-model
  
  # Still use cloud providers for other models
  openai:
    models:
      - gpt-4o-mini  # Restrict to cheaper models for dev
```

### Custom API Endpoints

Use different endpoints for testing or custom deployments:

```yaml
providers:
  openai:
    base-url: "https://custom-openai-proxy.example.com"
    # All other settings inherited from defaults
  
  anthropic:
    base-url: "https://staging-api.anthropic.com"
    version: "2024-01-01"  # Different API version
```

### Model Restrictions

Limit which models are available to your routers:

```yaml
providers:
  openai:
    models:
      - gpt-4o-mini  # Cost-optimized deployment
  
  anthropic:
    models:
      - claude-3-haiku  # Only fastest model
```

## Configuration Fields

### Required Fields

| Field | Type | Description | Example |
|-------|------|-------------|---------|
| `base-url` | string | API endpoint URL | `"https://api.openai.com"` |
| `models` | array | Available models | `["gpt-4", "gpt-4o"]` |

### Optional Fields

| Field | Type | Description | Example |
|-------|------|-------------|---------|
| `version` | string | API version | `"2023-06-01"` |
| `timeout` | duration | Request timeout | `"30s"` |
| `max-retries` | integer | Provider-specific retries | `3` |

## Best Practices

1. **Start with defaults** - Only override what you need to change
2. **Use local instances** for development to reduce costs
3. **Restrict models** in production to control costs and capabilities
4. **Test custom endpoints** thoroughly before production deployment
5. **Monitor provider health** when using custom configurations

## Provider-Specific Notes

### OpenAI
- Supports multiple regions automatically
- Rate limiting handled by load balancer
- All models supported by default

### Anthropic  
- Requires `version` field for API compatibility
- Different model naming conventions
- Built-in safety features

### Ollama
- Perfect for local development
- Custom model support
- No API key required

### AWS Bedrock
- Regional deployment support
- IAM-based authentication
- Multiple model providers in one endpoint

## Troubleshooting

### Common Issues

**Provider not responding:**
- Check base-url is correct and accessible
- Verify API keys are properly configured
- Test connectivity outside the gateway

**Models not available:**
- Ensure model names match provider specifications
- Check if models are properly enabled in provider account
- Verify model access permissions

**Authentication errors:**
- Confirm API keys are valid and not expired
- Check if provider-specific headers are required
- Verify account has access to requested models 