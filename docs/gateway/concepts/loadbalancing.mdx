---
title: "Load Balancing Strategies"
sidebarTitle: "Load Balancing"
description: "Intelligent request routing across providers with latency-based P2C and weighted algorithms"
---

# Load Balancing Strategies

The AI Gateway automatically distributes requests across multiple providers using sophisticated algorithms that consider latency, provider health, and your custom preferences. All strategies are **rate-limit aware** and **health-monitored**—unhealthy providers are automatically removed and re-added when they recover.

## Why Use Load Balancing?

Load balancing helps you:
- **Optimize latency** by routing to the fastest available providers
- **Improve reliability** with automatic failover when providers fail
- **Handle rate limits** by temporarily removing rate-limited providers
- **Control traffic distribution** with custom weights for cost optimization
- **Enable gradual rollouts** and A/B testing across providers

## Available Strategies

<AccordionGroup>
  <Accordion title="Latency-based (P2C + PeakEWMA) - Default" icon="bolt">
    **Power-of-Two-Choices with Peak Exponentially Weighted Moving Average** *(Available in v0)*
    
    Maintains a moving average of each provider's RTT latency, weighted by the number of outstanding requests, to distribute traffic to providers with the least load and optimize for latency.
    
    **Best for:** Production workloads where latency matters most
    
    **How it works:**
    1. Randomly selects 2 providers from the healthy pool
    2. Calculates load using RTT weighted by outstanding requests  
    3. Routes to the provider with lower load score
    4. Updates moving averages with actual response times
    
    **Configuration:**
    ```yaml
    routers:
      default:
        load-balance:
          chat:
            strategy: latency
            targets:
              - openai
              - anthropic
              - google-gemini
    ```
  </Accordion>

  <Accordion title="Weighted Strategy" icon="chart-pie">
    **Custom traffic percentages across providers** *(Available in v0)*
    
    Routes traffic based on arbitrary weights you specify. For example, if you have providers [A, B, C] with weights [0.80, 0.15, 0.05], then A gets 80% of traffic, B gets 15%, and C gets 5%.
    
    **Best for:** Cost optimization, gradual provider migrations, or compliance requirements
    
    **Configuration:**
    ```yaml
    routers:
      default:
        load-balance:
          chat:
            strategy: weighted
            targets:
              - provider: anthropic
                weight: 0.75
              - provider: openai
                weight: 0.25
    ```
    
    <Callout type="warning">
      Weights must sum to exactly 1.0, or the AI Gateway will reject the configuration.
    </Callout>
  </Accordion>

  <Accordion title="Cost-Optimized Strategy" icon="dollar-sign">
    **Route to the cheapest equivalent model** *(Coming in v2)*
    
    For a given model, picks the provider that offers that same model or any allowed configured equivalent models for the lowest price.
    
    **Best for:** Cost-sensitive workloads where minor latency differences are acceptable
  </Accordion>

  <Accordion title="Model-Level Weighted Strategy" icon="bullseye">
    **Provider + model specific weighting** *(Coming in v2)*
    
    Same as weighted strategy over providers, except configurable for provider+model pairs. E.g., [openai/o1, bedrock/claude-3-5-sonnet] with weights [0.90, 0.10].
    
    **Best for:** Fine-grained control over specific model routing
  </Accordion>

  <Accordion title="Tag-based Routing" icon="tag">
    **Header-driven routing decisions** *(Coming in v3)*
    
    Route requests to specific providers and models based on tags passed via request headers.
    
    **Best for:** A/B testing, user-specific routing, compliance requirements
  </Accordion>
</AccordionGroup>

## Load Balancing Levels

Conduit supports load balancing at multiple levels of granularity:

| Level                   | Availability | Description                                                                                                                 | Example                                          |
| ----------------------- | ------------ | --------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------ |
| **Providers**           | v0           | Balance across different AI companies                                                                                       | OpenAI vs Anthropic vs Google Gemini             |
| **Models**              | v2           | Balance over (provider, model) pairs. Enables rate limit aware, cost usage load balancing, and specific weighted strategies | `openai/gpt-4o` vs `anthropic/claude-3-5-sonnet` |
| **Deployments/Regions** | v3           | Balance across regions for providers that support them                                                                     | `us-east-1` vs `us-west-2` vs `eu-west-1`        |

<Callout type="info">
  Currently, the AI Gateway load balances at the **provider level**. Model-level and regional load balancing are planned for future releases.
</Callout>

## Health Monitoring

All load balancing strategies automatically handle provider failures through intelligent health monitoring:

- **Error rate monitoring** → Providers with high error rates (default: >10%) are automatically removed
- **Rate limit detection** → Rate-limited providers are temporarily removed and re-added when limits reset
- **Grace period handling** → Providers need minimum requests (default: 20) before being considered for removal
- **Automatic recovery** → Unhealthy providers are periodically retested and re-added when healthy

<Callout type="tip">
  The AI Gateway monitors provider health every 5 seconds by default. The health check uses a rolling 60-second window with configurable error thresholds.
</Callout>

### Health Monitoring Configuration

You can customize health monitoring behavior:

```yaml
monitor:
  health:
    type: error-ratio
    ratio: 0.15           # 15% error threshold (default: 0.10)
    window: 60s           # Rolling window (default: 60s)  
    buckets: 10           # Window buckets (default: 10)
    interval: 5s          # Check interval (default: 5s)
    grace-period:
      min-requests: 30    # Minimum requests before removal (default: 20)
```

## Configuration Examples

<Tabs>
  <Tab title="Latency-based (Default)">
    ```yaml
    routers:
      default:
        load-balance:
          chat:
            strategy: latency
            targets:
              - openai
              - anthropic
              - google-gemini
    ```
    
    <Callout type="info">
      Latency-based load balancing requires at least 2 providers. With more providers, it becomes more effective at finding the optimal choice.
    </Callout>
  </Tab>

  <Tab title="Weighted Distribution">
    ```yaml
    routers:
      default:
        load-balance:
          chat:
            strategy: weighted
            targets:
              - provider: openai
                weight: 0.60
              - provider: anthropic  
                weight: 0.30
              - provider: google-gemini
                weight: 0.10
    ```
    
         <Callout type="warning">
       Weights must sum to exactly 1.0, or the AI Gateway will reject the configuration during startup.
     </Callout>
  </Tab>

  <Tab title="Provider Migration Example">
    ```yaml
    routers:
      default:
        load-balance:
          chat:
            strategy: weighted
            targets:
              - provider: anthropic    # New provider
                weight: 0.80
              - provider: openai       # Old provider  
                weight: 0.20
    ```
    
    <Callout type="note">
      Gradually adjust weights over time to safely migrate traffic between providers.
    </Callout>
  </Tab>
</Tabs>

## Choosing the Right Strategy

| Use Case                 | Recommended Strategy       | Availability |
| ------------------------ | -------------------------- | ------------ |
| **Production APIs**      | Latency-based (P2C)        | v0           |
| **Cost optimization**    | Weighted → Cost-optimized  | v0 → v2      |
| **Provider migration**   | Weighted                   | v0           |
| **A/B testing**          | Weighted → Tag-based       | v0 → v3      |
| **Fine-grained control** | Model-level weighted       | v2           |
| **Compliance routing**   | Tag-based                  | v3           |

## Best Practices

1. **Start with latency-based load balancing** for most production workloads—it's self-optimizing and battle-tested
2. **Use weighted routing** when you need predictable traffic distribution or cost control
3. **Monitor provider performance** in your logs to understand which providers work best for your workload
4. **Test with low traffic** when adding new providers to your load balancer pool
5. **Gradually adjust weights** when migrating between providers to avoid sudden traffic shifts
6. **Plan for future features** like model-level and regional load balancing in v2/v3

<Callout type="warning">
  Load balancing works best with at least 2 providers. Single-provider configurations will route all traffic to that provider (no balancing occurs).
</Callout>

## Advanced Configuration

### Multiple Endpoint Types

You can configure different load balancing strategies for different endpoint types:

```yaml
routers:
  default:
    load-balance:
      chat:
        strategy: latency
        targets:
          - openai
          - anthropic
      completions:
        strategy: weighted
        targets:
          - provider: openai
            weight: 1.0
```

### Router-Specific Load Balancing

Define custom load balancing per router:

```yaml
routers:
  production:
    load-balance:
      chat:
        strategy: latency
        targets: [openai, anthropic]
  
  development:
    load-balance:
      chat:
        strategy: weighted
        targets:
          - provider: openai
            weight: 1.0
```

---

*For more information on configuring providers and routers, see the [Configuration Guide](/docs/configuration).*
