---
title: "Rate Limiting & Spend Controls"
sidebarTitle: "Rate Limiting"
description: "GCRA-based rate limiting with burst capacity and smooth request throttling"
---

The AI Gateway provides flexible rate limiting using GCRA (Generic Cell Rate Algorithm) to help you manage request frequency and prevent abuse. Configure limits globally or per-router with burst capacity and smooth rate limiting.

Rate limiting uses **[configurable storage backends](#storage-backend-options)** and is applied with configurable burst capacity and sustained rates.

## Why Use Rate Limiting?

Rate limiting helps you:
- **Prevent abuse** by limiting request rates per API key
- **Manage costs** by controlling request frequency
- **Ensure stability** by preventing traffic spikes from overwhelming your system
- **Fair usage** by distributing capacity across different API keys
- **Control your own traffic** based on your business requirements

<Info>
  **Provider rate limits are handled automatically** by the load balancing system. This rate limiting feature is for controlling your own API traffic based on your business requirements.
</Info>

<Note>
  For complete configuration options and syntax, see the [Configuration Reference](/gateway/config#rate-limiting).
</Note>

## Available Strategies

<AccordionGroup>
  <Accordion title="Requests Per Time Period - Default" icon="gauge">
    **Burst-aware rate limiting with smooth sustained rates**
    
    Control request frequency using capacity (burst allowance) and refill frequency (sustained rate). Uses GCRA (Generic Cell Rate Algorithm) for smooth token bucket behavior that's more sophisticated than simple sliding window approaches.
    
    **Best for:** Production workloads requiring smooth rate limiting with burst tolerance
    
    **How it works:**
    1. Each API key gets a virtual token bucket with specified capacity
    2. Requests consume tokens from the bucket
    3. Tokens refill at the specified rate (refill-frequency / capacity)
    4. Requests are allowed if tokens are available, rejected otherwise
    
    **Example:**
    ```yaml
    rate-limit:
      global:
        per-api-key:
          capacity: 500
          refill-frequency: 1s  # 500 requests per second sustained
    ```
  </Accordion>

  <Accordion title="Fixed Window Rate Limiting" icon="window">
    **Simple time-based request counting** *(Coming in v2)*
    
    Traditional rate limiting based on fixed time windows (e.g., 1000 requests per hour, reset at the top of each hour).
    
    **Best for:** Simple use cases where burst behavior is less important than predictable reset times
  </Accordion>

  <Accordion title="Sliding Window Rate Limiting" icon="chart-line">
    **Rolling time-based request counting** *(Coming in v2)*
    
    Rate limiting based on a rolling time window that provides smoother behavior than fixed windows.
    
    **Best for:** Applications requiring more predictable rate limiting without the complexity of token buckets
  </Accordion>
</AccordionGroup>

## Rate Limiting Levels

The AI Gateway supports rate limiting at multiple levels based on **who** gets rate limited:

| Level                   | Availability | Description                                                                 | Example Use Case       |
| ----------------------- | ------------ | --------------------------------------------------------------------------- | ---------------------- |
| **Per-API-Key**         | v0 (live!)   | Limits applied to each API key individually                                | Prevent API key abuse  |
| **Per-End-User**        | v1 (coming)  | Limits applied to end users via `Helicone-User-Id` header                 | SaaS user quotas       |
| **Per-Team**            | v2 (planned) | Limits applied to teams for budget and governance controls                 | Department budgets     |
| **Per-Team-Member**     | v2 (planned) | Limits applied to individual team members for governance                   | Developer quotas       |

### Configuration Scope

Rate limits are checked in precedence order when a request comes in:

<Steps>
  <Step title="Global Rate Limits">
    **Application-wide limits** *(Available in v0)*
    
    Applied to all requests across all routers. These limits are checked first and act as a safety net for your entire system.
  </Step>
  
  <Step title="Router-Specific Rate Limits">
    **Individual router limits** *(Available in v0)*
    
    Applied after global limits pass. Each router can have custom limits or opt out of rate limiting entirely.
  </Step>
</Steps>

<Note>
  Global and router-specific settings control the **configuration scope**, not who gets limited.
</Note>

## Storage Backend Options

Rate limiting counters can be stored in different backends depending on your deployment needs:

<AccordionGroup>
  <Accordion title="In-Memory Storage" icon="memory">
    **Local memory storage** *(Available in v0)*
    
    Rate limiting state is stored locally in each router instance. Fast and simple, but limits are not shared across multiple instances.
    
    ```yaml
    rate-limit:
      global:
        store: in-memory
        cleanup-interval: 5m
    ```
    
    **Best for:**
    - Single instance deployments
    - Development environments
    - High-performance scenarios where cross-instance coordination isn't needed
  </Accordion>

  <Accordion title="Redis Storage" icon="database">
    **Distributed storage** *(Coming in v1)*
    
    Rate limiting state is stored in Redis, allowing coordination across multiple router instances for consistent limits.
    
    ```yaml
    rate-limit:
      global:
        store: redis
        redis:
          url: "redis://localhost:6379"
        cleanup-interval: 5m
    ```
    
    **Best for:**
    - Multi-instance deployments
    - Load-balanced router setups
    - Consistent rate limiting across a distributed system
  </Accordion>

  <Accordion title="Database Storage" icon="server">
    **Persistent storage** *(Planned for future releases)*
    
    Rate limiting state stored in a database for persistence and advanced querying capabilities.
    
    **Best for:**
    - Long-term rate limiting analytics
    - Compliance requirements
    - Complex rate limiting policies
  </Accordion>
</AccordionGroup>

## Configuration Examples

<Tabs>
  <Tab title="Production API - Abuse Prevention">
    **Use case:** Production API that needs to prevent abuse while allowing reasonable burst traffic for legitimate users.

    ```yaml
    rate-limit:
      global:
        store: in-memory
        per-api-key:
          capacity: 1000
          refill-frequency: 1m  # 1000 requests per minute
        cleanup-interval: 5m
    ```
  </Tab>

  <Tab title="Development Environment - Cost Safety">
    **Use case:** Development environment where you want to prevent accidental high costs while allowing reasonable experimentation.

    ```yaml
    rate-limit:
      global:
        store: in-memory
        per-api-key:
          capacity: 100
          refill-frequency: 1h  # 100 requests per hour
        cleanup-interval: 5m
    ```
  </Tab>

  <Tab title="Multi-Tier Service - Different Router Limits">
    **Use case:** Different service tiers with varying rate limits. Premium router gets higher limits than basic router.

    ```yaml
    rate-limit:
      global:
        store: in-memory
        cleanup-interval: 5m
    
    routers:
      premium:
        rate-limit:
          per-api-key:
            capacity: 5000
            refill-frequency: 1m  # 5000 requests per minute
      
      basic:
        rate-limit:
          per-api-key:
            capacity: 100
            refill-frequency: 1m  # 100 requests per minute
    ```
  </Tab>

  <Tab title="High-Volume Production - Burst Tolerance">
    **Use case:** High-traffic production system that needs to handle traffic spikes while maintaining overall rate control.

    ```yaml
    rate-limit:
      global:
        store: in-memory
        per-api-key:
          capacity: 10000       # Allow large bursts
          refill-frequency: 10s # 1000 requests per second sustained
        cleanup-interval: 2m
    ```
  </Tab>

  <Tab title="Router-Specific Override">
    **Use case:** Most routers use global settings, but one router needs stricter limits for sensitive operations.

    ```yaml
    rate-limit:
      global:
        store: in-memory
        per-api-key:
          capacity: 1000
          refill-frequency: 1m
        cleanup-interval: 5m
    
    routers:
      sensitive:
        rate-limit:
          per-api-key:
            capacity: 10
            refill-frequency: 1m  # Much stricter: 10 requests per minute
    ```
  </Tab>
</Tabs>

## Choosing the Right Configuration

| Use Case                    | Recommended Approach          | Availability |
| --------------------------- | ----------------------------- | ------------ |
| **Production APIs**         | Global GCRA with burst tolerance | v0       |
| **Development/Testing**     | Conservative limits with long refill periods | v0 |
| **Multi-tier services**     | Router-specific limit overrides | v0        |
| **High-volume systems**     | High capacity with short refill periods | v0 |
| **Single instance**         | In-memory storage             | v0           |
| **Distributed deployments** | Redis storage                 | v1           |
| **SaaS applications**       | Per-end-user limits           | v1           |
| **Enterprise governance**   | Per-team and per-team-member limits | v2    |

## GCRA Rate Limiting

All rate limiting strategies use the Generic Cell Rate Algorithm to provide smooth, burst-aware request throttling:

<CardGroup>
  <Card title="Burst tolerance">
    Allows legitimate traffic spikes while maintaining sustained rate controls
  </Card>
  <Card title="Smooth rate limiting">
    More sophisticated than simple sliding windows - avoids thundering herd problems
  </Card>
  <Card title="Per-API-key tracking">
    Individual limits for each API key with consistent behavior across routers
  </Card>
  <Card title="Automatic cleanup">
    Expired rate limiting state is automatically cleaned up to manage memory usage
  </Card>
</CardGroup>

<Note>
  **Provider rate limits are handled automatically** by the load balancing system. This rate limiting feature is for controlling your own API traffic based on your business requirements.
</Note>

To customize rate limiting parameters and behavior, see the [Configuration Reference](/gateway/config#rate-limiting).