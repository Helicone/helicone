---
title: "Configuration Reference"
sidebarTitle: "Configuration"
description: "Complete reference for configuring your LLM Gateway"
---

The AI Gateway is configured through a `config.yaml` file that defines how requests are routed, load balanced, and processed across different LLM providers.

## Routers

Each Helicone AI Gateway deployment can configure multiple independent routing policies for different use cases. Each router operates with its own load balancing strategy, provider set, and configuration.

<ParamField path="routers" type="object">
  Define one or more routers. Each router name becomes part of the URL path when making requests.

  ```yaml
  routers:
    production:
      load-balance:
        chat:
          strategy: latency
          providers:
            - openai
            - anthropic
    
    experimental:
      balance:
        chat:
          strategy: weighted
          providers:
            - provider: anthropic
              weight: '0.9'
            - provider: openai
              weight: '0.1'
  ```

  **Usage:** Set your OpenAI SDK baseURL to `http://localhost:8080/production` or `http://localhost:8080/experimental`
</ParamField>

## Load Balancing

Distribute requests across multiple providers to optimize performance, costs, and reliability. The gateway currently supports latency-based and weighted strategies with cost strategies coming soon.

[Learn more](/gateway/features/loadbalancing)

### Latency Strategy

<ParamField path="routers.[name].load-balance.chat.strategy" type="string">
  Use `latency` for automatic load balancing that routes to the provider with the lowest latency.

  ```yaml
  routers:
    production:
      load-balance:
        chat:
          strategy: latency
          providers:
            - openai
            - anthropic
            - gemini
            - ollama
  ```

  **How it works:** Routes requests to the provider with the lowest measured latency based on the PeakEWMA
  for the time to first byte/token.
</ParamField>

### Weighted Strategy

<ParamField path="routers.[name].load-balance.chat.strategy" type="string">
  Use `weighted` to distribute requests based on specific percentages.

  ```yaml
  routers:
    production:
      load-balance:
        chat:
          strategy: weighted
          providers:
            - provider: anthropic
              weight: '0.95'
            - provider: openai
              weight: '0.05'
  ```

  **Important:** Weights must sum to exactly `1.0`
</ParamField>

### Balance providers

<ParamField path="routers.[name].load-balance.chat.providers" type="array">
  List of target providers for load balancing.

  **For Latency Strategy:**
  ```yaml
  providers:
    - openai
    - anthropic
    - gemini
  ```

  **For Weighted Strategy:**
  ```yaml
  providers:
    - provider: anthropic
      weight: '0.7'
    - provider: openai
      weight: '0.3'
  ```
</ParamField>

<Note>
  Support is coming soon to configure load balance strategies with specific provider and model combinations (e.g., `openai/gpt-4o`, `anthropic/claude-3-5-sonnet`) for more granular routing control.
</Note>

## Caching

Store and reuse LLM responses for identical requests to dramatically reduce costs and improve response times. Cache directives control response freshness and staleness tolerance.

[Learn more](/gateway/features/cache)

<ParamField path="routers.[name].cache" type="object" optional>
  Configure response caching for a router.

  ```yaml
  routers:
    production:
      cache:
        directive: "max-age=3600, max-stale=1800"
        buckets: 10
        seed: "unique-cache-seed"
  ```
</ParamField>

<ParamField path="routers.[name].cache.store" type="string" default="in-memory">
  Storage backend for cache data.

  ```yaml
  cache:
    store: in-memory
  ```

  **Options:**
  - `in-memory` - Local in-memory storage (default)

  <Note>
    `redis` support is coming soon for distributed caching across multiple gateway instances.
  </Note>
</ParamField>

<ParamField path="routers.[name].cache.directive" type="string">
  HTTP cache-control directive string.

  ```yaml
  cache:
    directive: "max-age=3600, max-stale=1800"
  ```

  **How it works:** Defines default `cache-control` request headers applied to all requests in the router.
  This can be disabled on a per-request basis with a `Helicone-cache-enabled: false` request header.

  `cache-control` headers sent by the request will take precedence over what's set in the Gateway config.
</ParamField>

<ParamField path="routers.[name].cache.buckets" type="number">
  Number of responses stored per cache key before random selection begins.

  ```yaml
  cache:
    buckets: 10
  ```

  **How it works:** Stores n number of different responses for identical requests, then randomly selects from the stored responses to add variability.
</ParamField>

<ParamField path="routers.[name].cache.seed" type="string">
  Unique seed for cache key generation.

  ```yaml
  cache:
    seed: "unique-cache-seed"
  ```

  **How it works:** Creates isolated cache namespaces - different seeds maintain separate cache spaces for the same requests.
</ParamField>

## Rate Limiting

Control request frequency using GCRA (Generic Cell Rate Algorithm) with burst capacity and smooth rate limiting. Global limits are checked first, then router-specific limits are applied.

[Learn more](/gateway/features/rate-limiting)

### Global Rate Limiting

<ParamField path="global.rate-limit" type="object">
  Configure application-wide rate limits that apply to all requests.

  ```yaml
  global:
    rate-limit:
      store: in-memory
      per-api-key:
        capacity: 500
        refill-frequency: 1s
  ```

  **How it works:** These limits are checked first for every request across all routers.
</ParamField>

### Router-Level Rate Limiting

<ParamField path="routers.[name].rate-limit" type="object">
  Configure additional rate limiting specific to this router (applied after global limits).

  ```yaml
  routers:
    production:
      rate-limit:
        per-api-key:
          capacity: 100
          refill-frequency: 1m
  ```

  **How it works:** If global limits are configured, they're checked first. Then these router-specific limits are applied as an additional layer.
</ParamField>

### Rate Limit Configuration Fields

The following fields are available for both global and router-level rate limiting:

<ParamField path="[context].rate-limit.store" type="string" default="in-memory">
  Storage backend for rate limit counters.

  ```yaml
  store: in-memory
  ```

  **Options:**
  - `in-memory` - Local memory storage (default)
</ParamField>

<Note>
  Support is coming soon to use Redids and S3 as stores.
</Note>

<ParamField path="[context].rate-limit.per-api-key" type="object">
  Rate limits applied per API key.

  ```yaml
  per-api-key:
    capacity: 500
    refill-frequency: 1s
  ```
</ParamField>

<ParamField path="[context].rate-limit.per-api-key.capacity" type="integer" default="500">
  Maximum number of requests in the bucket (burst capacity).

  ```yaml
  per-api-key:
    capacity: 1000
  ```

  **How it works:** This is the maximum number of requests that can be made instantly before rate limiting kicks in.
</ParamField>

<ParamField path="[context].rate-limit.per-api-key.refill-frequency" type="duration" default="1s">
  Time to completely refill the capacity bucket.

  ```yaml
  per-api-key:
    refill-frequency: 1s
  ```

  **How it works:** With capacity=500 and refill-frequency=1s, you get 500 requests per second sustained rate.
</ParamField>

<ParamField path="[context].rate-limit.cleanup-interval" type="duration" default="5m">
  How often to clean up expired rate limit entries.

  ```yaml
  cleanup-interval: 5m
  ```

  **Note:** Only available for global rate limiting configuration.
</ParamField>

## Retries

<Note>
Support for automatic retries is shipping soon!
</Note>

Automatic retry logic for failed requests with configurable backoff strategies to improve reliability without overwhelming providers.

[Learn more](/gateway/features/retries)

<ParamField path="routers.[name].retries" type="object" optional>
  Configure retry behavior for this router.

  ```yaml
  routers:
    production:
      retries:
        max-retries: 3
        strategy: exponential
        base: 500ms
        max: 10s
  ```
</ParamField>

<ParamField path="routers.[name].retries.max-retries" type="integer">
  Maximum number of retry attempts before giving up.

  ```yaml
  retries:
    max-retries: 5
  ```

  **Range:** 0-255. Setting to 0 effectively disables retries.
</ParamField>

<ParamField path="routers.[name].retries.strategy" type="string">
  Backoff strategy for spacing retry attempts.

  ```yaml
  retries:
    strategy: exponential
  ```

  **Options:**
  - `exponential` - Doubles wait time between retries (recommended)
</ParamField>

<ParamField path="routers.[name].retries.base" type="duration">
  Initial wait time before the first retry attempt.

  ```yaml
  retries:
    base: 500ms
  ```

  **Format:** Human-readable durations like `1s`, `500ms`, `2m`
</ParamField>

<ParamField path="routers.[name].retries.max" type="duration">
  Maximum wait time between retry attempts.

  ```yaml
  retries:
    max: 10s
  ```

  **Exponential cap:** Prevents exponential backoff from growing indefinitely.
</ParamField>


## Helicone Observability

Configure integration with the Helicone platform for authentication and observability. When enabled, requests must include a valid Helicone API key
for your organization.

[Learn more](/gateway/features/helicone-integration)

### Authentication

If you plan on enabling authentication, you'll need to register your router with Helicone first.

<ParamField path="helicone-observability.enable" type="boolean" default="false">
  Enable Helicone platform integration and authentication.

  ```yaml
  helicone-observability:
    enable: true
  ```

  **When enabled:** Requests must include a valid Helicone API key for your organization, and observability data is automatically sent to Helicone.
</ParamField>

<ParamField path="helicone-observability.base-url" type="string" default="https://api.helicone.ai">
  Helicone API endpoint URL.

  ```yaml
  helicone-observability:
    base-url: "https://api.helicone.ai"
  ```
</ParamField>


## Provider Configuration

Configure LLM providers, their endpoints, and available models. 

<Note>
The gateway ships with [comprehensive defaults](https://github.com/Helicone/helicone-router/blob/main/llm-proxy/config/embedded/providers.yaml) for all major providers. Most users will not need to configure this section, [this guide](/gateway/features/providers) will walk you through when you might need to.
</Note>

<ParamField path="providers" type="object">
  Configure provider settings to override defaults.

  ```yaml
  providers:
    anthropic:
      enabled: true
      base-url: "https://api.anthropic.com"
      version: "2023-06-01"
      models:
        - claude-3-5-haiku
    
    ollama:
      enabled: true
      base-url: "http://192.168.1.100:11434"
      models:
        - llama3.2
        - deepseek-r1
        - custom-fine-tuned-model
    
    bedrock:
      enabled: true
      base-url: "https://bedrock-runtime.us-west-2.amazonaws.com"
      models:
        - anthropic.claude-3-5-sonnet-20241022-v2:0
        - anthropic.claude-3-haiku-20240307-v1:0
  ```
</ParamField>

<ParamField path="providers.[name].base-url" type="string" required>
  API endpoint URL for the provider.

  ```yaml
  providers:
    openai:
      base-url: "https://api.openai.com"
  ```
</ParamField>

<ParamField path="providers.[name].models" type="array" required>
  List of supported models for this provider.

  ```yaml
  providers:
    openai:
      models:
        - gpt-4
        - gpt-4o
        - gpt-4o-mini
  ```
</ParamField>

<ParamField path="providers.[name].version" type="string" optional>
  API version (required for some providers like Anthropic).

  ```yaml
  providers:
    anthropic:
      version: "2023-06-01"
  ```
</ParamField>

## Model Mapping

Define equivalencies between models from different providers for seamless switching and load balancing. 

<Note>
The Gateway ships with [comprehensive defaults](https://github.com/Helicone/helicone-router/blob/main/llm-proxy/config/embedded/model-mapping.yaml) for all major providers. Most users will not need to configure this section, [this guide](/gateway/features/model-mapping) will walk you through when you might need to.
</Note>

<ParamField path="routers.[name].model-mappings" type="object" optional>
  Router-specific model mappings for fallback when requested model isn't available.

  ```yaml
  routers:
    custom-router:
      model-mappings:
        gpt-4o: claude-3-opus
        claude-3-5-sonnet: gemini-1.5-pro
        gpt-4o-mini: claude-3-5-sonnet
  ```
</ParamField>

<ParamField path="default-model-mapping" type="object">
  Global fallback mappings used when router-specific mappings aren't defined.

  ```yaml
  default-model-mapping:
    gpt-4o: claude-3-opus
    gpt-4o-mini: claude-3-5-sonnet
    claude-3-5-sonnet: gemini-1.5-pro
  ```
</ParamField>

## Telemetry

Configure Gateway application monitoring and logging via OpenTelemetry for observability into the AI Gateway's health and performance.

<Note>
  Monitor the AI Gateway's health and performance with OpenTelemetry. We provide Docker Compose for local testing and Grafana dashboard configs for production. 
  
  [Learn more](/gateway/features/observability)
</Note>

<ParamField path="telemetry.level" type="string" default="info">
  Logging level in env logger format.

  ```yaml
  telemetry:
    level: "info"
  ```

  **Common patterns:**
  - `"info"` - General information for all modules
  - `"debug,llm_proxy=info"` - Debug for dependencies, info for gateway
  - `"warn,hyper=off"` - Warnings only, disable specific modules
</ParamField>

<ParamField path="telemetry.otlp-endpoint" type="string" default="http://localhost:4317/v1/metrics">
  OTLP collector endpoint URL.

  ```yaml
  telemetry:
    otlp-endpoint: "http://localhost:4317"
  ```
</ParamField>

<ParamField path="telemetry.propagate" type="boolean" default="true">
  Enables tracing propagation via OpenTelemetry [context propagation](https://opentelemetry.io/docs/concepts/context-propagation/).

  ```yaml
  telemetry:
    propagate: true
  ```
</ParamField>

## Response Headers

Control which headers are returned to provide visibility into the gateway's routing decisions and processing.

<ParamField path="response-headers.provider" type="boolean" default="true">
  Add `helicone-provider` header showing which provider handled the request.

  ```yaml
  response-headers:
    provider: true
  ```

  **When enabled:** Responses include header like `helicone-provider: openai` or `helicone-provider: anthropic`.
</ParamField>

<ParamField path="response-headers.provider-request-id" type="boolean" default="true">
  Add `helicone-provider-req-id` header showing the provider's request ID.

  ```yaml
  response-headers:
    provider-request-id: true
  ```

  **When enabled:** Responses include header like `helicone-provider-req-id: req-12345` for request tracing.
</ParamField>

## Deployment Configuration

Configure the AI Gateway's runtime behavior for different deployment patterns and environments.

[Learn more about deployment modes](/gateway/deployment)

<ParamField path="deployment-target" type="string" default="sidecar">
  Deployment environment type.

  ```yaml
  deployment-target: "self-hosted"  # sidecar | self-hosted
  ```
</ParamField>

<ParamField path="is-production" type="boolean" default="false">
  Production environment flag for optimized logging and performance settings.

  ```yaml
  is-production: true
  ```

  **When enabled:** Optimizes telemetry output and logging levels for production workloads.
</ParamField>

## Health Monitoring

Configure how the AI Gateway monitors provider health and automatically removes failing providers from load balancing rotation.

<ParamField path="discover.monitor.health.type" type="string">
  Health monitoring strategy.

  ```yaml
  discover:
    monitor:
      health:
        ratio: 0.1
        window: 60s
        grace-period:
          min-requests: 20
  ```

  **Options:**
  - `error-ratio` - Monitor based on error rate thresholds (only option currently)
</ParamField>

<ParamField path="discover.monitor.health.ratio" type="number" default="0.1">
  Error ratio threshold (0.0-1.0) that triggers provider removal.

  ```yaml
  discover:
    monitor:
      health:
        ratio: 0.15
  ```

  **How it works:** If errors/requests exceeds this ratio, provider is marked unhealthy and removed from load balancing.
</ParamField>

<ParamField path="discover.monitor.health.window" type="duration" default="60s">
  Time window for measuring error ratios.

  ```yaml
  discover:
    monitor:
      health:
        window: 60s
  ```

  **How it works:** Rolling window size for calculating error rates.
</ParamField>

<ParamField path="discover.monitor.health.grace-period.min-requests" type="integer" default="20">
  Minimum requests required before health monitoring takes effect.

  ```yaml
  discover:
    monitor:
      health:
        grace-period:
          min-requests: 20
  ```

  **How it works:** Providers won't be marked unhealthy until they've handled at least this many requests.
</ParamField>

## Rate Limit Aware Load Balancing

All routers are automatically rate limit aware. This means an LLM provider that returns a rate limit
will automatically take them out of the potential providers to serve requests until the rate limit 
will resolve according to the `retry-after` response header or a default value of 1 minute.
