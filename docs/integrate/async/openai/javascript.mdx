---
title: "OpenAI Async with JavaScript/TypeScript"
sidebarTitle: "JavaScript/TypeScript"
description: "Integrate OpenAI with Helicone async logging using JavaScript/TypeScript"
---

# OpenAI Async Integration with JavaScript/TypeScript

This guide shows you how to log your OpenAI API calls asynchronously to Helicone using JavaScript or TypeScript.

## Prerequisites

- An OpenAI API key
- A Helicone API key (get one at [dashboard.helicone.ai](https://dashboard.helicone.ai))
- Node.js installed on your machine

## Installation

```bash
npm install openai helicone
# or
yarn add openai helicone
```

## Basic Setup

```javascript
import OpenAI from "openai";
import { helicone } from "helicone";

// Initialize Helicone
const heliconeClient = helicone({
  apiKey: process.env.HELICONE_API_KEY,
});

// Create OpenAI client
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Make your regular API call
const response = await openai.chat.completions.create({
  messages: [{ role: "user", content: "Hello, how are you?" }],
  model: "gpt-3.5-turbo",
});

console.log(response.choices[0].message);

// Log to Helicone asynchronously
heliconeClient.log({
  provider: "openai",
  request: {
    model: "gpt-3.5-turbo",
    messages: [{ role: "user", content: "Hello, how are you?" }],
  },
  response: response,
});
```

## Adding Custom Properties

```javascript
heliconeClient.log({
  provider: "openai",
  request: {
    model: "gpt-3.5-turbo",
    messages: [{ role: "user", content: "Hello, how are you?" }],
  },
  response: response,
  properties: {
    userId: "user-123",
    sessionId: "session-456",
    conversationId: "conv-789",
    feature: "chat-support",
    priority: "high",
  },
});
```

## Error Handling

```javascript
try {
  const response = await openai.chat.completions.create({
    messages: [{ role: "user", content: "Hello, how are you?" }],
    model: "gpt-3.5-turbo",
  });

  // Log successful response
  heliconeClient.log({
    provider: "openai",
    request: {
      model: "gpt-3.5-turbo",
      messages: [{ role: "user", content: "Hello, how are you?" }],
    },
    response: response,
  });

  return response;
} catch (error) {
  // Log error response
  heliconeClient.log({
    provider: "openai",
    request: {
      model: "gpt-3.5-turbo",
      messages: [{ role: "user", content: "Hello, how are you?" }],
    },
    error: error,
  });

  throw error;
}
```

## Handling Complex Scenarios

### Streaming Responses

```javascript
import OpenAI from "openai";
import { helicone } from "helicone";

const heliconeClient = helicone({
  apiKey: process.env.HELICONE_API_KEY,
});

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Create a streaming response
const stream = await openai.chat.completions.create({
  model: "gpt-3.5-turbo",
  messages: [{ role: "user", content: "Write a poem about AI" }],
  stream: true,
});

let fullContent = "";
let chunks = [];

// Collect all chunks
for await (const chunk of stream) {
  const content = chunk.choices[0]?.delta?.content || "";
  fullContent += content;
  chunks.push(chunk);
}

// Log completed stream
heliconeClient.log({
  provider: "openai",
  request: {
    model: "gpt-3.5-turbo",
    messages: [{ role: "user", content: "Write a poem about AI" }],
    stream: true,
  },
  response: {
    choices: [
      {
        message: {
          content: fullContent,
          role: "assistant",
        },
      },
    ],
    model: "gpt-3.5-turbo",
    // Add any other fields that are important for your analysis
  },
  properties: {
    streaming: "true",
  },
});
```

## Using with Frameworks

### Next.js Example

```javascript
// pages/api/chat.js
import OpenAI from "openai";
import { helicone } from "helicone";

// Initialize Helicone outside the handler for reuse
const heliconeClient = helicone({
  apiKey: process.env.HELICONE_API_KEY,
});

export default async function handler(req, res) {
  try {
    const openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
    });

    const response = await openai.chat.completions.create({
      messages: req.body.messages,
      model: "gpt-3.5-turbo",
    });

    // Log to Helicone asynchronously
    heliconeClient.log({
      provider: "openai",
      request: {
        model: "gpt-3.5-turbo",
        messages: req.body.messages,
      },
      response: response,
      properties: {
        userId: req.session?.user?.id || "anonymous",
        endpoint: "chat-api",
      },
    });

    res.status(200).json({ result: response.choices[0].message });
  } catch (error) {
    // Log error to Helicone
    heliconeClient.log({
      provider: "openai",
      request: {
        model: "gpt-3.5-turbo",
        messages: req.body.messages,
      },
      error: error,
      properties: {
        userId: req.session?.user?.id || "anonymous",
        endpoint: "chat-api",
      },
    });

    res.status(500).json({ error: error.message });
  }
}
```

## Next Steps

- [Add custom properties](/docs/features/properties) for better filtering and analytics
- [Set up cost tracking](/docs/dashboard/cost-tracking) to monitor your OpenAI spending
- [Explore the dashboard](/docs/dashboard/overview) to analyze your OpenAI usage

<div
  style={{
    display: "flex",
    justifyContent: "center",
    marginTop: "2rem",
    marginBottom: "2rem",
  }}
>
  <a
    href="/docs/integrate/async/openai"
    style={{
      display: "flex",
      alignItems: "center",
      gap: "0.5rem",
      padding: "0.625rem 1rem",
      color: "#94a3b8",
      fontSize: "0.9375rem",
      textDecoration: "none",
      transition: "color 0.2s ease",
    }}
  >
    <svg
      width="16"
      height="16"
      viewBox="0 0 24 24"
      fill="none"
      xmlns="http://www.w3.org/2000/svg"
    >
      <path
        d="M19 12H5M5 12L12 19M5 12L12 5"
        stroke="currentColor"
        strokeWidth="2"
        strokeLinecap="round"
        strokeLinejoin="round"
      />
    </svg>
    Back to language selection
  </a>
</div>{" "}
