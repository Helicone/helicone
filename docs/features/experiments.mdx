---
title: "Experiments"
sidebarTitle: "Experiments (New)"
description: "Experiments is a spreadsheet-like experience designed to tune your LLM prompts for production. "
"twitter:title": "Experiments | Helicone OSS LLM Observability"
---

import QuestionsSection from "/snippets/questions-section.mdx";

## What are Experiments?

Helicone's Experiments is designed to help you tune your LLM prompt, test it on production data, and verify your iterations with quantifiable data. In Helicone, you can define and run LLM-as-a-judge or custom evaluators, then compare the results between prompts side-by-side to determine the best one for production.

<Frame>
  <img
    src="/images/experiments/experiments-cover.webp"
    alt="Experiments: spreadsheet-like experience to tune your LLM prompts for production"
  />
</Frame>

## Why experiment with prompts?

- **Prevent regression:** Prompt engineering is iterative. Engineers want to prevent regression with each prompt change.
- **Model update:** LLMs are sensitive to prompt changes. As new models with advanced capability come out, prompts that worked previously can become less effective.
- **Quick feedback loop:** Instead of waiting for user feedback, get feedback immediately and adjust before pushing to production.

## What is the prompt iteration workflow?

1. Create a new experiment
2. Create new prompt variations
3. Add input rows
4. Create and run custom evaluators
5. Push changes to production

## Quick Start

### 1. Create a new experiment

Head to the `Experiment` tab. If you have a prompt with Helicone, click on `start from a prompt`, then choose the desired prompt version. Otherwise, click on `start from scratch`.

<Frame>
  <img
    src="/images/experiments/create-new-experiment.webp"
    alt="Start an experiment with a prompt or from scratch. "
  />
</Frame>

### 2. Create new prompt variations

To create a new prompt, click `Add column` and select a prompt that you want to fork from.

<Note>
  Keep in mind that you can only fork from an existing prompt in the Experiment.
</Note>

<Frame>
  <img
    src="/images/experiments/fork-prompt.webp"
    alt="Fork existing prompts to create new variations in Helicone's Experiments. "
  />
</Frame>

#### Add prompt variables

Type `{{ input_name }}` to add input variables in your prompt. These input variables will appear in the `Inputs` column.

<Frame>
  <img
    src="/images/experiments/input-variable.webp"
    alt="Add input variables by styling variables in this format {{input_variable}}"
  />
</Frame>

### 3. Add input rows

The next step is to import golden datasets or your request data in Helicone. There are four ways:

<Frame>
  <img
    src="/images/experiments/add-input.webp"
    alt="Add input rows by manually entering, selecting from an input set or a dataset, or a randomly sampled selection"
  />
</Frame>

- `Add manual inputs`: Manually enter values for each input variable you defined.
- `Select an input set`: Select production request data that matches the same prompt ID.
- `Random prod`: Randomly select any number of production request data. We wrote about [why this approach is recommended](https://www.helicone.ai/blog/prompt-evaluation-for-llms).
- `Add from a dataset (coming soon)`: Soon, you can use datasets created in Helicone to test your prompt.

### 4. Create and run custom evaluators

<Steps>
    <Step title="Toggle on `Show scores`.">
      <Frame>
        <img
          src="/images/experiments/select-eval.webp"
          alt="Create LLM-as-a-judge or custom evaluators to test your prompt. "
        />
      </Frame>
    </Step>
    <Step title="Under the dropdown, select 'Create new custom evaluators'">
    </Step>
    <Step title="Create the evaluator">
        On the side panel, you will be able to create a new evaluator. We currently support LLM-as-a-judge. Python and TypeScript support is coming soon!
        <Tip>You can add as many evaluators as you like, and run them all at the same time. </Tip>
    </Step>
    <Step title="Run Evaluator">
        Once you add an evaluator, you will notice a warning that prompts you to re-run evaluation. Click on `Run Evaluators` to see the scores in graph.
        <Frame>
        <img
            src="/images/experiments/run-eval.webp"
            alt="View evaluator results in detail. "
        />
        </Frame>
    </Step>
    <Step title="(Optional) View score breakdown">
        To see the scores breakdown by input, click on an evaluator (humor in this example). You will see individual scores appear in each cell. 
        Notice that cells that perform above average will have a green indicator. Cells below average will have a red indicator.

        <Frame>
        <img
            src="/images/experiments/run-custom-eval.webp"
            alt="View evaluator results in detail. "
        />
        </Frame>

    </Step>

</Steps>

### 5. Push changes to production

The more prompts you create, the more data points on the scores graph you will see. Keep in mind that prompt engineering is an iterative process. The more input you test, the more edge cases you can cover with your new prompts.
Once you are happy with a prompt, copy and paste it in your code for production.

<QuestionsSection />
