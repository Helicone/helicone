---
title: "Streaming"
---

Helicone smoothly integrates streaming functionality and offers benefits that you can't find with the standard OpenAI package!

Currently, **OpenAI doesn't provide usage statistics such as prompt and completion tokens**. However, Helicone overcomes this limitation by estimating these statistics with the help of the gpt3-tokenizer package, which is designed to work with all tokenized OpenAI GPT models.

All you have to do is import helicone or import openai through the Helicone package and the rest of your code works as it does.

```python
from helicone.openai_proxy import openai
```

The following examples work with or without Helicone!

### Streaming mode with synchronous requests

In this mode, the request is made synchronously, but the response is streamed.

```python
for chunk in openai.ChatCompletion.create(
    model = 'gpt-3.5-turbo',
    messages = [{
        'role': 'user',
        'content': "Hello World!"
    }],
    stream=True
):
    content = chunk["choices"][0].get("delta", {}).get("content")
    if content is not None:
        print(content, end='')
```

### Streaming mode with asynchronous requests

In this mode, both the request is made asynchronously and the response is streamed. You'll need to use the `await` keyword when calling `openai.ChatCompletion.acreate`, and use an `async` for loop to iterate over the response.

```python
for chunk in await openai.ChatCompletion.acreate(
    model = 'gpt-3.5-turbo',
    messages = [{
        'role': 'user',
        'content': "Hello World!"
    }],
    stream=True
):
    content = chunk["choices"][0].get("delta", {}).get("content")
    if content is not None:
        print(content, end='')
```

### Asynchronous Stream Parser

Helicone now provides an improved asynchronous stream parser that enhances performance and reliability when working with streamed responses. This new parser processes stream chunks asynchronously, reducing latency and providing more reliable token counting.

#### Using the Async Stream Parser with Manual Logging

If you're using manual logging with providers like Together AI, you can take advantage of the new async stream parser:

```typescript
import Together from "together-ai";
import { HeliconeManualLogger } from "@helicone/helpers";

export async function main() {
  const heliconeLogger = new HeliconeManualLogger({
    apiKey: process.env.HELICONE_API_KEY!,
    headers: {},
  });

  const together = new Together();
  const body = {
    model: "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
    messages: [{ role: "user", content: "Your question here" }],
    stream: true,
  } as Together.Chat.CompletionCreateParamsStreaming & { stream: true };

  const response = await together.chat.completions.create(body);
  const [stream1, stream2] = response.tee();

  // Use the new async stream parser
  heliconeLogger.logStream(body, async (resultRecorder) => {
    resultRecorder.attachStream(stream1.toReadableStream());
  });

  // Process the stream for your application
  const textDecoder = new TextDecoder();
  for await (const chunk of stream2.toReadableStream()) {
    console.log(textDecoder.decode(chunk));
  }
}
```

### Stream Fixes and Improvements

We've made several improvements to our stream handling across different LLM providers:

- Better handling of stream interruptions and reconnections
- Enhanced error handling for streaming responses
- Improved compatibility with different LLM provider streaming formats
- More reliable token counting for streamed content

These improvements ensure a more consistent and reliable streaming experience, especially when working with different LLM providers that implement streaming in various ways.

For information on how to accurately calculate costs when using streaming features, please refer to our [streaming usage guide](/faq/enable-stream-usage).
