---
title: "Prompts"
sidebarTitle: "Prompts"
description: "Automatically version and manage your prompts from the codebase or the UI."
"twitter:title": "Prompts | Helicone OSS LLM Observability"
---

import QuestionsSection from "/snippets/questions-section.mdx";

<iframe
  width="100%"
  height="420"
  src="https://www.youtube.com/embed/pJAn6hoQJDc?autoplay=1&mute=1"
></iframe>

## What is Prompt Management?

Helicone's prompt management provides a seamless way for users to version, track and optimize their prompts used in their AI applications.

<Frame caption="Example: A prompt template designed for a course generation application. ">
  <img
    src="/images/prompts/prompt-cover.webp"
    alt="Example: A prompt template designed for a course generation application. "
  />
</Frame>

## Why manage prompts in Helicone?

Once you set up prompts in Helicone, your incoming requests will be matched to a `helicone-prompt-id`, allowing you to:

- version and track iterations of your prompt over time.
- maintain a dataset of inputs and outputs for each prompt.
- generate content using predefined prompts through our [Generate API](/features/prompts/generate).

## Quick Start

### Prerequisites

Please set up Helicone in **proxy mode** using one of the methods in the [Starter Guide](https://docs.helicone.ai/getting-started/quick-start#quick-start).

<Note>
  Not sure if proxy is for you? We've created a guide to explain the difference
  between [Helicone Proxy vs Helicone Async
  integration](https://docs.helicone.ai/getting-started/proxy-vs-async).
</Note>

### Create prompt templates

As you modify your prompt in code, Helicone automatically tracks the new version and maintains a record of the old prompt. Additionally, a dataset of input/output keys is preserved for each version.

<Tabs>
  <Tab title="Typescript / Javascript">
    <Steps> 
      <Step title="Install the package">
        ```bash
        npm install @helicone/prompts
        ```
      </Step>
      <Step title="Import hpf">
        ```tsx
        import { hpf } from "@helicone/prompts";
        ```
      </Step>
      <Step title="Add `hpf` and identify input variables">
      By prefixing your prompt with `hpf` and enclosing your input variables in an additional `{}`, it allows Helicone to easily detect your prompt and inputs. We've designed for minimum code change to keep it as easy as possible to use Prompts.

      ```tsx
      const location = "space";
      const character = "two brothers";
      const promptInput = hpf`
      Compose a movie scene involving ${{ character }}, set in ${{ location }}
      `;
      ```

      #### Static Prompts with hpstatic

      In addition to `hpf`, Helicone provides `hpstatic` for creating static prompts that don't change between requests. This is useful for system prompts or other constant text that you don't want to be treated as variable input.

      To use `hpstatic`, import it along with `hpf`:

      ```typescript
      import { hpf, hpstatic } from "@helicone/prompts";
      ```

      Then, you can use it like this:

      ```typescript
      const systemPrompt = hpstatic`You are a helpful assistant.`;
      const userPrompt = hpf`Write a story about ${{ character }}`;
      ```

      The `hpstatic` function wraps the entire text in `<helicone-prompt-static>` tags, indicating to Helicone that this part of the prompt should not be treated as variable input.


      #### Change input name
      To rename your input or have a custom input, change the key-value pair in the passed dictionary to the string formatter function:

      ```tsx
      content: hpf`Write a story about ${{ "my_magical_input": character  }}`,
      ```

      </Step>

      <Step title="Assign an id to your prompt">
        Assign a `Helicone-Prompt-Id` header to your LLM request.

        Assigning an id allows us to associate your prompt with future versions of your prompt, and **automatically manage versions on your behalf.**

        ```tsx
        headers: {
          "Helicone-Prompt-Id": "prompt_story",
        },
        ```
      </Step>
    </Steps>

  </Tab> 
  <Tab title="Python">
    <Steps>
      <Step title="Install the package">
        ```bash
        pip install helicone_prompts
        ```
      </Step>
      <Step title="Import hpf">
        ```python
        from helicone_prompts import hpf, hpstatic
        ```
      </Step>
      <Step title="Add `hpf` and identify input variables">
      By prefixing your prompt with `hpf` and providing your input variables as keyword arguments, it allows Helicone to easily detect your prompt and inputs. We've designed for minimum code change to keep it as easy as possible to use Prompts.

      ```python
      location = "space"
      character = "two brothers"
      prompt_input = hpf(
          "Compose a movie scene involving {character}, set in {location}",
          character=character,
          location=location
      )
      ```

      #### Static Prompts with hpstatic

      In addition to `hpf`, Helicone provides `hpstatic` for creating static prompts that don't change between requests. This is useful for system prompts or other constant text that you don't want to be treated as variable input.

      To use `hpstatic`, import it along with `hpf`:

      ```python
      from helicone_prompts import hpf, hpstatic
      ```

      Then, you can use it like this:

      ```python
      system_prompt = hpstatic("You are a helpful assistant.")
      user_prompt = hpf("Write a story about {character}", character=character)
      ```

      The `hpstatic` function wraps the entire text in `<helicone-prompt-static>` tags, indicating to Helicone that this part of the prompt should not be treated as variable input.

      #### Change input name
      To rename your input or have a custom input, use a different keyword argument name:

      ```python
      content = hpf(
          "Write a story about {my_magical_input}",
          my_magical_input=character
      )
      ```
      </Step>

      <Step title="Assign an id to your prompt">
        Assign a `Helicone-Prompt-Id` header to your LLM request.

        Assigning an id allows us to associate your prompt with future versions of your prompt, and **automatically manage versions on your behalf.**

        ```python
        headers = {
            "Helicone-Prompt-Id": "prompt_story"
        }
        ```
      </Step>
    </Steps>

  </Tab>
  <Tab title="Packageless (cURL)">
    Currently, we only support packages for TypeScript and JavaScript for easy integration. For other users, we recommend manually implementing input variables as follows:
    <Steps>
      <Step title="Identify the input variable, and create a key name.">
      Given the below string, let's say `a secret agent` is our inputted variable. Let's name our key `character`.

      ```tsx
      Write a story about a secret agent
      ```
      </Step>

      <Step title="Add helicone-prompt-input tags">
      Add `<helicone-prompt-input key="<INPUT_ID>">` before your input variable, and `</helicone-prompt-input>` after, where `INPUT_ID` is your key name. In this example, `key="character"`:

      ```tsx
      Write a story about <helicone-prompt-input key="character" >a secret agent</helicone-prompt-input>
      ```
      <Check> Don't worry, Helicone will remove these tags for you before your prompt is sent to your LLM.</Check>
      <Note>It is crucial that each key has a **unique identifier**; otherwise, the same variable will be replaced in multiple places. </Note>


      #### Static Prompts

      For static prompts, you can manually wrap static parts of your prompt in `<helicone-prompt-static>` tags:

      ```tsx
      <helicone-prompt-static>You are a helpful assistant.</helicone-prompt-static>
      Write a story about <helicone-prompt-input key="character">a secret agent</helicone-prompt-input>
      ```
    This tells Helicone that the first part of the prompt is static and should not be treated as variable input.


      </Step>

      <Step title="Tag your prompt">

        In order to securely ensure that we know when and how to parse and categorize your prompt, please append the following header to your LLM request.

        ```tsx
        Helicone-Prompt-Id: you_prompt_id
        ```
        <Note>
        The id must not include any spaces or special characters. Underscores and dashes are acceptable.
        </Note>

      </Step>

      <Step title="Make a request">
        You just created a prompt template! Helicone will now keep track of all of your inputs for you. Rest assured that all of the helicone-prompt-input tags will be removed before your prompt is sent to your LLM.
      </Step>

    </Steps>

  </Tab> 
</Tabs>

### Put it together

Let's say we have an app that generates a short story, where users are able to input their own `character`. For example, the prompt is "Write a story about a secret agent", where the `character` is "a secret agent".

<Tabs>
  <Tab title="Typescript / Javascript example">
    ``` tsx
    // 1. Add imports
    import { hpf, hpstatic } from "@helicone/prompts";

    const chatCompletion = await openai.chat.completions.create(
      {
        messages: [
          {
            role: "system",
            // 2. Use hpstatic for static prompts
            content: hpstatic`You are a creative storyteller.`,
          },
          {
            role: "user",
            // 3: Add hpf to any string, and nest any variable in additional brackets `{}`
            content: hpf`Write a story about ${{ character }}`,
          },
        ],
        model: "gpt-3.5-turbo",
      },
      {
        // 4. Add Prompt Id Header
        headers: {
          "Helicone-Prompt-Id": "prompt_story",
        },
      }
    );
    ```

  </Tab>

  <Tab title="Python example">
    ```python
    # 1. Add imports
    from helicone_prompts import hpf, hpstatic

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {
                "role": "system",
                # 2. Use hpstatic for static prompts
                "content": hpstatic("You are a creative storyteller.")
            },
            {
                "role": "user",
                # 3. Call hpf with a string and provide variables as keyword arguments
                "content": hpf("Write a story about {character}", character="a secret agent")
            }
        ],
        # 4. Add Prompt Id Header
        headers={
          "Helicone-Prompt-Id": "prompt_story",
        }
    )
    ```

  </Tab>

  <Tab title="Packageless (cURL) example">

    ```python
    openai.ChatCompletion.create(
      model="[DEPLOYMENT]",
      # Manually add starting and ending tags with before and after variables
      messages=[{"role": "User", "content": f'Write a story about <helicone-prompt-input key="character" >{character} </helicone-prompt-input>'}],
      headers={
        # Add Prompt Id header
        "Helicone-Prompt-Id": "[my_prompt_id]",
      }
    )
    ```
    If you want more package support, please reach out to [engineering@helicone.ai](mailto:engineering@helicone.ai).

  </Tab>
</Tabs>

### Create prompts from the UI

In Helicone, you can create a prompt without touching the codebase for better collaboration between technical and non-technical teams.

  <Steps> 
    <Step title="Click on `Create Prompt`">
      <Frame>
        <img
          src="/images/prompts/create-prompt.webp"
          alt="Helicone's Prompts interface with a `Create Prompt` button to get started."
        />
      </Frame>
    </Step>
    <Step title="Toggle on `I'm not technical`">
      <Frame caption="Create prompts from Helicone's UI">
        <img
          src="/images/prompts/create-new-prompt-from-ui.webp"
          alt="An interface that shows two prompt creation methods: code and UI."
        />
      </Frame>
    </Step>
    <Step title="Click on `Save prompt`"></Step>

  </Steps>

### Use prompts created in the UI in code

If you've created a prompt on the UI, you can either:

1. Use the [Generate API](/features/prompts/generate) for a simplified integration that works with multiple LLM providers, or
2. Pull the prompt into your codebase using the following API endpoint:

<Tabs>
  <Tab title="Typescript / Javascript">
    ```javascript
    export async function getPrompt(
      id: string,
      variables: Record<string, any>
    ): Promise<any> {
      const getHeliconePrompt = async (id: string) => {
        const res = await fetch(
          `https://api.helicone.ai/v1/prompt/${id}/template`,
          {
            headers: {
              Authorization: `Bearer ${YOUR_HELICONE_API_KEY}`,
              "Content-Type": "application/json",
            },
            method: "POST",
            body: JSON.stringify({
              inputs: variables,
            }),
          }
        );

        return (await res.json()) as Result<PromptVersionCompiled, any>;
      };

      const heliconePrompt = await getHeliconePrompt(id);
      if (heliconePrompt.error) {
        throw new Error(heliconePrompt.error);
      }
      return heliconePrompt.data?.filled_helicone_template;
    }

    async function pullPromptAndRunCompletion() {
      const prompt = await getPrompt("my-prompt-id", {
        color: "red",
      });
      console.log(prompt);

      const openai = new OpenAI({
        apiKey: "YOUR_OPENAI_API_KEY",
        baseURL: `https://oai.helicone.ai/v1/${YOUR_HELICONE_API_KEY}`,
      });
      const response = await openai.chat.completions.create(
        prompt satisfies OpenAI.Chat.Completions.ChatCompletionCreateParamsStreaming
      );
      console.log(response);
    }
    ```

  </Tab>
  <Tab title="Python">
    ```python
    async def get_prompt(id: str, variables: dict) -> Any:
        async def get_helicone_prompt(id: str):
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"https://api.helicone.ai/v1/prompt/{id}/template",
                    headers={
                        "Authorization": f"Bearer {YOUR_HELICONE_API_KEY}",
                        "Content-Type": "application/json",
                    },
                    json={"inputs": variables},
                ) as response:
                    return await response.json()

        helicone_prompt = await get_helicone_prompt(id)
        if "error" in helicone_prompt:
            raise Exception(helicone_prompt["error"])
        return helicone_prompt.get("data", {}).get("filled_helicone_template")

    async def pull_prompt_and_run_completion():
        prompt = await get_prompt("my-prompt-id", {
            "color": "red"
        })
        print(prompt)

        client = OpenAI(
            api_key="YOUR_OPENAI_API_KEY",
            base_url=f"https://oai.helicone.ai/v1/{YOUR_HELICONE_API_KEY}"
        )
        response = await client.chat.completions.create(**prompt)
        print(response)
    ```

  </Tab>
</Tabs>

## Run experiments

Once you've set up prompt management, you can use Helicone's [Experiments feature](https://docs.helicone.ai/features/experiments) to test and improve your prompts.

## Local testing

Many times in development, you may want to test your prompt locally before deploying it to production and you don't want Helicone to track new prompt versions.

<Tabs>
  <Tab title="Typescript / Javascript">
    To do this, you can set the `Helicone-Prompt-Mode` header to `testing` in your LLM request. This will prevent Helicone from tracking new prompt versions.

    ```tsx
    headers: {
      "Helicone-Prompt-Mode": "testing",
    },
    ```

  </Tab>
  <Tab title="Python">
    To do this, you can set the `Helicone-Prompt-Mode` header to `testing` in your LLM request. This will prevent Helicone from tracking new prompt versions.

    ```python
    headers={
            "Helicone-Prompt-Mode": "testing"
        },
    ```

  </Tab>
</Tabs>

## FAQ

<AccordionGroup>
  <Accordion title="How can I improve my LLM app's performance?">
     Improving your LLM app primarily revolves around sophisticated prompt engineering. Here are some key techniques to optimize your prompt:

      1.  Be specific and clear.
      2.  Use structured formats.
      3.  Leverage role-playing.
      4.  Implement few-shot learning.

      For more best practices, see [best practices of prompt engineering](https://docs.helicone.ai/guides/prompt-engineering).

   </Accordion>
   <Accordion title="Does Helicone own my prompts?">
    Helicone does not own your prompts. We simply provide a logging and observability platform that captures and securely stores your LLM interaction data. When you use our UI, prompts are temporarily stored in our system, but when using code-based integration, prompts remain exclusively in your codebase. Our commitment is to offer transparent insights into your LLM interactions while ensuring your intellectual property remains completely under your control.
  </Accordion>

   <Accordion title="How does the version indexing work?">
    In Helicone, you may notice that some of your prompt versions are labeled as `V2.0`, while others are labeled `V2.1`. Major versions (i.e., `V1.0`, `V2.0`, `V3.0`) are your prompts in production. Minor versions (`V1.1`, `V1.2`, `V1.3`) are experiment versions that are created in Helicone, which were forked off of your production version `V1.0` in this example.
  </Accordion>
</AccordionGroup>

<QuestionsSection />
