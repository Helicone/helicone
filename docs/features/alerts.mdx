---
title: "Alerts"
sidebarTitle: "Alerts"
description: "Get notified when your LLM applications hit error thresholds or cost limits"
---

Monitor error rates and costs to catch issues before they impact users. Helicone's alert system provides real-time notifications when your LLM applications experience problems, helping you maintain reliability and control costs.

## Alert Types

### Error Rate Alerts
Track the percentage of failed requests over a time window. Essential for maintaining application reliability.

**Use cases:**
- Detect provider outages or rate limiting issues
- Catch breaking changes in prompts or model behavior  
- Monitor deployment health after updates
- Identify patterns in user inputs causing failures

**Example:** Alert when error rate exceeds 5% over 30 minutes with at least 20 requests.

### Cost Alerts
Monitor spending to prevent budget overruns and detect unusual usage patterns.

**Use cases:**
- Prevent unexpected bills from runaway processes
- Track per-environment spending (dev/staging/prod)
- Detect potential abuse or misconfiguration
- Monitor cost trends for specific features or users

**Example:** Alert when daily spending exceeds \$1000 or when hourly spending exceeds \$100.

## Creating Alerts

Navigate to **Settings â†’ Alerts** in your Helicone dashboard to create new alerts.

<Frame caption="Helicone Alerts Dashboard showing configured alerts and their status">
  <img
    src="/images/alerts/alert-page.webp"
    alt="Helicone alerts dashboard with list of configured alerts"
  />
</Frame>

<Steps>
<Step title="Select alert type and threshold">
  <Frame caption="Creating a new alert in Helicone">
    <img
      src="/images/alerts/alert-create.webp"
      alt="Alert creation interface showing configuration options"
    />
  </Frame>
  
  **Error Rate Alerts:**
  - Percentage threshold: 1-100% (5-10% recommended for production)
  - Tracks ratio of failed requests to total requests
  - Failed requests include 4xx/5xx errors and timeouts
  
  **Cost Alerts:**
  - Dollar amount threshold (e.g., $50, $500, $1000)
  - Tracks cumulative spend within time window
  - Includes all model costs across providers
  - Optional filters let you scope the alert to a subset of traffic (e.g provider-specific requests, or certain properties)
</Step>

<Step title="Configure time window">  
  Choose how long to evaluate the metric:
  
  - **5-15 minutes**: Immediate detection, higher false positive rate
  - **30-60 minutes**: Balanced approach (recommended for most apps)
  - **2-4 hours**: Sustained issues only, fewer false positives
  - **Daily/Weekly**: Budget tracking and long-term trends
  
  <Note>
  Shorter windows detect issues faster but may trigger during brief spikes. Longer windows reduce noise but delay detection.
  </Note>
</Step>

<Step title="Set minimum request threshold">
  Prevent false positives during low traffic periods:
  
  - **Development**: 5-10 requests minimum
  - **Staging**: 10-20 requests minimum  
  - **Production**: 20-50 requests minimum
  
  Alerts only trigger when both the threshold AND minimum requests are met.
  
  <Warning>
  Always set a minimum request count to avoid alert fatigue. A single failed request during low traffic shouldn't trigger a 100% error rate alert.
  </Warning>
</Step>

<Step title="Configure notifications">
  Choose where alerts are sent:
  
  - **Email**: Add any email address (immediate delivery)
  - **Slack**: Select connected channels (#alerts, #engineering, etc.)
  - **Multiple recipients**: Add several emails or channels per alert
</Step>
</Steps>

<Tip>
Start with conservative thresholds (higher error %, longer windows) and tighten based on actual patterns. This prevents alert fatigue while you learn your app's normal behavior.
</Tip>

<Frame caption="Example of a configured cost alert">
  <img
    src="/images/alerts/alert-cost-configured.webp"
    alt="Cost alert configuration showing threshold and time window settings"
  />
</Frame>

## Targeted Monitoring with Filters

Each alert can watch a specific slice of your traffic. Use the filter builder when creating or editing an alert to narrow the scope by provider, model, environment, or any custom properties you've set. For example, create a cost alert that only tracks Anthropic requests, or isolate prompts tagged with a certain property value to validate new functionality before rolling it out broadly.

You can also filter the Alerts list itself to quickly review production-critical rules versus experimental ones.

## Notification Channels

### Dashboard
All alerts appear in your Helicone dashboard with real-time status updates. When an alert triggers, you can immediately see affected requests and investigate the issue.

<Frame caption="Alert triggered view in the dashboard">
  <img
    src="/images/alerts/alert-triggered.webp"
    alt="Dashboard view when an alert has been triggered showing affected requests"
  />
</Frame>

### Email Notifications
Add any email address to receive alerts. Emails include:
- Alert type and threshold that triggered
- Current metric value and trend
- Direct link to affected requests in dashboard
- Time window and request count

<Frame caption="Example alert notification email">
  <img
    src="/images/alerts/alert-triggered-email.webp"
    alt="Email notification showing alert details and link to dashboard"
  />
</Frame>

### Slack Integration
When creating or editing an alert:
1. Select **Slack** as the notification method
2. Click **Connect Slack** button that appears
3. Authorize Helicone in your Slack workspace
4. Select a channel from the dropdown (#alerts, #engineering, etc.)

After connecting, you can simply select any channel from your workspace. Slack messages include the same details as emails with rich formatting and direct links to view affected requests.

## Configuration Examples

### Production Monitoring

```yaml
# Critical error detection
metric: error_rate
threshold: 10%
time_window: 10min
min_requests: 20
notify: [#incidents, oncall@company.com]

# Sustained error monitoring  
metric: error_rate
threshold: 5%
time_window: 30min
min_requests: 50
notify: #engineering

# Daily cost tracking
metric: cost
threshold: $1000
time_window: 24h
notify: [#finance, cto@company.com]
```

### Development Environment

```yaml
# Loose thresholds for dev
metric: error_rate
threshold: 25%
time_window: 60min
min_requests: 5
notify: dev-team@company.com

# Weekly budget check
metric: cost
threshold: $100
time_window: 7d
notify: #dev-alerts
```


## Advanced Features (Coming Soon)

Soon you'll be able to create massively customizable alerts:

- **Custom aggregations** - Alert on any metric (P95 latency, token usage, specific error codes)
- **Advanced filters** - Combine multiple [custom properties](/observability/custom-properties) with AND/OR logic
- **Complex thresholds** - Percentage changes, rolling averages, anomaly detection
- **Custom webhooks** - Send alerts to any endpoint

These features will enable precise monitoring for specific user segments, features, or any custom criteria you define.

## Related Features

<CardGroup cols={2}>
<Card title="Custom Properties" icon="tag" href="/observability/custom-properties">
  Filter alerts by environment, feature, or user segment
</Card>

<Card title="User Metrics" icon="users" href="/observability/user-metrics">  
  Track costs and errors per user to set appropriate thresholds
</Card>

<Card title="Sessions" icon="link" href="/features/sessions">
  Monitor multi-step workflows that might trigger alerts
</Card>

<Card title="Datasets" icon="database" href="/features/datasets">
  Collect examples of requests that triggered alerts for analysis
</Card>
</CardGroup>
