---
title: "Omit Logs"
sidebarTitle: "Omit Logs"
description: "Configure Helicone to selectively exclude sensitive request and response data from your LLM application logs for privacy and compliance needs."
"twitter:title": "Omit Logs - Helicone OSS LLM Observability"
---

import QuestionsSection from "/snippets/questions-section.mdx";

## Introduction

The "Omit Logs" feature is designed for users who may not want to have Helicone store or log their request body or response body. This can be particularly useful for those concerned with privacy, data sensitivity, or simply wanting to keep their data footprint minimal.

<Frame caption="Omit requests, responses or both in your logs. ">
  <img
    src="/images/omit-logs/example-omit-logs.png"
    alt="Manage logging preferences with Helicone's Omit Logs feature."
  />
</Frame>

### Why Omit Logs

- **Privacy Concerns**: Your logs may contain sensitive information such as personally identifiable information (PII) or proprietary data. Omitting certain logs can help maintain privacy and security.
- **Focus on Key Metrics**: Omitting requests and responses allows you to focus on capturing key metrics and making it easier to discern the most relevant information related to your LLM application.
- **Regulatory Compliance**: In certain industries or regions, there may be regulations or compliance requirements regarding the logging and storage of data. Omitting certain logs may be necessary to ensure compliance with these regulations.

<Check>
  All responses and requests are only stored in memory and never in long-term
  storage.{" "}
</Check>

There are two ways to omit logs:

1. **Headers**: Use this if you're not using Helicone's [Python Asynchronous Logging](/getting-started/integration-method/openllmetry).
2. **Helicone Python Async Logging**: Use this if you're using Helicone's [Python Asynchronous Logging](/getting-started/integration-method/openllmetry) and don't want to send the request or response to our backend.

## Headers

<Note>
  **Note:** This method does not store the request or response but it still
  sends it to our backend.
</Note>

To omit logging requests, set `Helicone-Omit-Request` to `true`.

To omit logging responses, set `Helicone-Omit-Response` to `true`.

<CodeGroup>

```bash Curl
curl https://oai.helicone.ai/v1/completions \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer YOUR_API_KEY' \
  -H 'Helicone-Omit-Request: true' \ # Add this header and set to true
  -H 'Helicone-Omit-Response: true' \ # Add this header and set to true
 -d '{
    "model": "text-davinci-003",
    "prompt": "How do I enable custom rate limit policies?",
}'
```

```python Python
client = OpenAI(
    base_url="https://oai.helicone.ai/v1",
    default_headers={
        "Helicone-Auth": f"Bearer {HELICONE_API_KEY}",
    }
)
client.chat.completions.create(
    model="text-davinci-003",
    prompt="How do I enable retries?",
    headers={
        "Helicone-Omit-Request": "true", # Add this header and set to true
         "Helicone-Omit-Response": "true", # Add this header and set to true
    }
)
```

```js Node
import { Configuration, OpenAIApi } from "openai";
const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,
  basePath: "https://oai.helicone.ai/v1",
  defaultHeaders: {
    "Helicone-Omit-Request": "true", // Add this header and set to true
    "Helicone-Omit-Response": "true", // Add this header and set to true
  },
});
const openai = new OpenAIApi(configuration);
```

</CodeGroup>

## Python Async Logging

If you are using Helicone's [Asynchronous Logging](/getting-started/integration-method/openllmetry), you have the option to not even send the request or response to our backend.

```python
from helicone_async import HeliconeAsyncLogger
from openai import OpenAI

logger = HeliconeAsyncLogger(
  api_key=HELICONE_API_KEY,
)

logger.init()

client = OpenAI(api_key=OPENAI_API_KEY)

logger.disable_content_tracing() # This will omit the request and response from being sent to our backend till the time you call logger.enable_content_tracing()

# Make the OpenAI call
response = client.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[{"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Who won the world series in 2020?"},
            {"role": "assistant",
             "content": "The Los Angeles Dodgers won the World Series in 2020."},
            {"role": "user", "content": "Where was it played?"}]
)

print(response.choices[0])
logger.enable_content_tracing() # All future requests will be sent with the request and response
```

### Request View

You can still see the key metrics for each request, without the request or response contents.

<Frame caption="Focus on capturing key metrics and ignore the noise of the request and response. ">
  <img
    src="/images/omit-logs/example-omit-logs-key-metrics.png"
    alt="View key metrics without logging request or response details with Helicone's Omit Logs feature."
  />
</Frame>

## Completely Disabling Logging

If you are using Helicone's Asynchronous Logging, you can also completely disable all logging to Helicone:

```python
from helicone_async import HeliconeAsyncLogger
from openai import OpenAI

logger = HeliconeAsyncLogger(
  api_key=HELICONE_API_KEY,
)

logger.init()

# Completely disable all logging
logger.disable_logging()

# Your OpenAI calls here
# No data will be sent to Helicone

# Later, re-enable logging if needed
logger.enable_logging()
```

When logging is disabled, no traces will be sent to Helicone at all. This is different from `disable_content_tracing()` which only omits request and response content but still sends other metrics. Note that this feature is only available when using Helicone's async integration mode and not with the proxy integration.

### Request View

You can still see the key metrics for each request, without the request or response contents.

<QuestionsSection />
