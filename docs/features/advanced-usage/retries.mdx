---
title: "Retries"
sidebarTitle: "Retries"
description: "Configure Helicone to automatically retry failed LLM requests, overcoming rate limits and server issues using intelligent exponential backoff."
"twitter:title": "Retries - Helicone OSS LLM Observability"
---

import QuestionsSection from "/snippets/questions-section.mdx";

## Introduction

Retrying requests is a common best practice when dealing with overloaded servers or hitting rate limits. These issues typically manifest as HTTP status codes `429` (Too Many Requests) and `500` (Internal Server Error).

For more information on error codes, see the [OpenAI API error codes documentation](https://platform.openai.com/docs/guides/error-codes/api-errors).

<Accordion title="Learn About Exponential Backoff">
  To effectively deal with retries, we use a strategy called **exponential
  backoff**. Exponential backoff involves increasing the wait time between
  retries exponentially, which helps to spread out the request load and gives
  the server a chance to recover. This is done by multiplying the wait time by a
  factor (default is 2) for each subsequent retry.
</Accordion>

### Why Retries

- Overcoming rate limits and server overload.
- Reducing the load on the server, increasing the likelihood of request success on subsequent attempts.

## Quick Start

To get started, set `Helicone-Retry-Enabled` to `true`.

<CodeGroup>
```bash Curl
curl --request POST \
     --url https://oai.helicone.ai/v1/chat/completions \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer $OPENAI_API_KEY" \
     -H "Helicone-Auth: Bearer $HELICONE_API_KEY" \
     -H "Helicone-Retry-Enabled: true" \
     --data '{
        "model": "gpt-4o-mini",
        "messages": [
          {
            "role": "system",
            "content": "Say Hello!"
            }
        ],
        "temperature": 1,
        "max_tokens": 30
      }'
```

```python Python
client = OpenAI(
    base_url="https://oai.helicone.ai/v1",
    default_headers={
        "Helicone-Auth": f"Bearer {HELICONE_API_KEY}",
    }
)

client.chat.completions.create(
    model="gpt-4o-mini",
    prompt="How do I enable retries?",
    extra_headers={
      "Helicone-Retry-Enabled": "true", # Add this header and set to true
    }
)
```

```js Node
import { Configuration, OpenAIApi } from "openai";
const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,
  basePath: "https://oai.helicone.ai/v1",
  defaultHeaders: {
    "Helicone-Auth": `Bearer ${HELICONE_API_KEY}`,
    "Helicone-Retry-Enabled": "true", // Add this header and set to true
  },
});
const openai = new OpenAIApi(configuration);
```

</CodeGroup>

<Check>When a retry happens, the request will be logged in Helicone. </Check>

### Retries Parameters

You can customize the behavior of the retries feature by setting additional headers in your request.

| Parameter                    | Description                                                                                                            | Default Value |
| ---------------------------- | ---------------------------------------------------------------------------------------------------------------------- | ------------- |
| `helicone-retry-num`         | Number of retries                                                                                                      | `5`           |
| `helicone-retry-factor`      | The exponential backoff factor used to increaase the wait time between subsequent retries. The default is usually `2`. | `2`           |
| `helicone-retry-min-timeout` | Minimum timeout (in milliseconds) between retries                                                                      | `1000`        |
| `helicone-retry-max-timeout` | Maximum timeout (in milliseconds) between retries                                                                      | `10000`       |

<Info>
  Header values have to be strings. For example, `"helicone-retry-num":
  "3"`.{" "}
</Info>

<QuestionsSection />
