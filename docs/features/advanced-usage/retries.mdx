---
title: "Retries"
sidebarTitle: "Retries"
description: "Configure Helicone to automatically retry failed LLM requests, overcoming rate limits and server issues using intelligent exponential backoff."
"twitter:title": "Retries - Helicone OSS LLM Observability"
---

import QuestionsSection from "/snippets/questions-section.mdx";

<Info>
  **Who can use this feature**: Anyone on any
  [plan](https://www.helicone.ai/pricing).{" "}
</Info>

## Introduction

Retrying requests is a common best practice when dealing with overloaded servers or hitting rate limits. These issues typically manifest as HTTP status codes `429` (Too Many Requests) and `500` (Internal Server Error).

For more information on error codes, see the [OpenAI API error codes documentation](https://platform.openai.com/docs/guides/error-codes/api-errors).

<Accordion title="Learn About Exponential Backoff">
  To effectively deal with retries, we use a strategy called **exponential
  backoff**. Exponential backoff involves increasing the wait time between
  retries exponentially, which helps to spread out the request load and gives
  the server a chance to recover. This is done by multiplying the wait time by a
  factor (default is 2) for each subsequent retry.
</Accordion>

### Why Retries

- Overcoming rate limits and server overload.
- Reducing the load on the server, increasing the likelihood of request success on subsequent attempts.

## Quick Start

To get started, set `Helicone-Retry-Enabled` to `true`.

<CodeGroup>
```bash Curl
curl https://oai.helicone.ai/v1/completions \
  -H 'Content-Type: application/json' \
  -H 'Helicone-Auth: Bearer YOUR_API_KEY' \
  -H 'Helicone-Retry-Enabled: true' \ # Add this header and set to true
  -d '{
    "model": "text-davinci-003",
    "prompt": "How do I enable retries?",
}'
```

```python Python
openai.api_base = "https://oai.helicone.ai/v1"

openai.Completion.create(
    model="text-davinci-003",
    prompt="How do I enable retries?",
    headers={
      "Helicone-Retry-Enabled": "true", # Add this header and set to true
    }
)
```

```js Node
import { Configuration, OpenAIApi } from "openai";
const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,
  basePath: "https://oai.helicone.ai/v1",
  defaultHeaders: {
    "Helicone-Auth": `Bearer ${HELICONE_API_KEY}`,
    "Helicone-Retry-Enabled": "true", // Add this header and set to true
  },
});
const openai = new OpenAIApi(configuration);
```

</CodeGroup>

<Check>When a retry happens, the request will be logged in Helicone. </Check>

### Retries Parameters

You can customize the behavior of the retries feature by setting additional headers in your request.

| Parameter                    | Description                                                                                                            |
| ---------------------------- | ---------------------------------------------------------------------------------------------------------------------- |
| `helicone-retry-num`         | Number of retries                                                                                                      |
| `helicone-retry-factor`      | The exponential backoff factor used to increaase the wait time between subsequent retries. The default is usually `2`. |
| `helicone-retry-min-timeout` | Minimum timeout (in milliseconds) between retries                                                                      |
| `helicone-retry-max-timeout` | Maximum timeout (in milliseconds) between retries                                                                      |

<Info>
  Header values have to be strings. For example, `"helicone-retry-num":
  "3"`.{" "}
</Info>

<QuestionsSection />
