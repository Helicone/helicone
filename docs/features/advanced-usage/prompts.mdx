---
title: "Prompt Management"
sidebarTitle: "Prompt Management"
description: "Compose and iterate prompts, then easily deploy them in any LLM call with the AI Gateway."
"twitter:title": "Prompt Management | Helicone OSS LLM Observability"
---

## Why Prompt Management?

Traditional prompt development involves hardcoded prompts in application code, messy string substitution, and frustrating and rebuilding deployments for every iteration. This creates friction that slows down experimentation and your team's ability to ship.

Our Prompts offers a better approach!
- **Powerful composability**: Variables of all types in system prompts, messages, and tool/response schemas
- **Clear version control**: Track, compare, and rollback prompt versions without code changes
- **Easy deployment**: Reference prompts by ID and let our AI Gateway handle the rest
- **Real-time testing**: Test prompts instantly with different models and parameters

## Quickstart

<Steps>
  <Step title="Create a Prompt">
    Build a prompt in the Playground. Save any prompt with clear commit histories and tags.

    <Frame>
      <video width="100%" autoPlay loop muted playsInline>
        <source
          src="https://marketing-assets-helicone.s3.us-west-2.amazonaws.com/Prompts2025-A.mp4"
          type="video/mp4"
        />
        <img
          src="/static/features/prompts/create_prompt_placeholder.png"
          alt="Creating a prompt in Helicone dashboard"
        />
      </video>
    </Frame>
  </Step>

  <Step title="Test and Iterate">
    Experiment with different variables, inputs, and models until you reach desired output. Variables can be used anywhere, even in tool schemas.

    <Frame>
      <video width="100%" autoPlay loop muted playsInline>
        <source
          src="https://marketing-assets-helicone.s3.us-west-2.amazonaws.com/Prompts2025-B.mp4"
          type="video/mp4"
        />
        <img
          src="/static/features/prompts/test_prompt_placeholder.png"
          alt="Testing prompts with variables and different models"
        />
      </video>
    </Frame>
  </Step>

  <Step title="Run Prompt with AI Gateway">
    Use your prompt instantly by referencing its ID in your [configured AI Gateway](/ai-gateway/config#helicone-add-ons). No code changes, no rebuilds.

    <CodeGroup>
      ```typescript TypeScript
      import { OpenAI } from "openai";

      const openai = new OpenAI({
        baseURL: "http://localhost:8080/ai",
        apiKey: "api-key",
      });

      const response = await openai.chat.completions.create({
        model: "openai/gpt-4o-mini",
        prompt_id: "abc123", // Reference your saved prompt
        inputs: {
          customer_name: "John Doe",
          product: "AI Gateway"
        }
      });
      ```

      ```python Python
      import openai

      openai.api_base = "http://localhost:8080/ai"
      openai.api_key = "api-key"

      response = openai.chat.completions.create(
          model="openai/gpt-4o-mini",
          prompt_id="abc123",  # Reference your saved prompt
          inputs={
              "customer_name": "John Doe",
              "product": "AI Gateway"
          }
      )
      ```

      ```bash cURL
      curl http://localhost:8080/ai/chat/completions \
        -H "Content-Type: application/json" \
        -d '{
          "model": "openai/gpt-4o-mini",
          "prompt_id": "abc123",
          "inputs": {
            "customer_name": "John Doe",
            "product": "AI Gateway"
          }
        }'
      ```
    </CodeGroup>
  </Step>
</Steps>

<Tip>
  Your prompt is automatically compiled with the provided inputs and sent to your chosen model. Update prompts in the dashboard and changes take effect immediately!
</Tip>

## Prompt Assembly Process

When you make an LLM call with a prompt ID, the AI Gateway will compile your saved prompt alongside runtime parameters you provide.

### Parameter Priority

Saved prompts store all the configuration you set in the playground - temperature, max tokens, response format, system messages, and more. At runtime, these saved parameters are used as defaults, but any parameters you specify in your API call will override them.

<CodeGroup>
  ```json Saved Prompt Configuration
  {
    "model": "gpt-4o-mini",
    "temperature": 0.6,
    "max_tokens": 1000,
    "messages": [
      {
        "role": "system", 
        "content": "You are a helpful customer support agent for {{hc:company:string}}."
      },
      {
        "role": "user",
        "content": "Hello, I need help with my account."
      }
    ]
  }
  ```

  ```typescript Runtime API Call
  const response = await openai.chat.completions.create({
    prompt_id: "abc123",
    temperature: 0.4, // Overrides saved temperature of 0.6
    inputs: {
      company: "Acme Corp"
    },
    messages: [
      {
        "role": "user",
        "content": "Actually, I want to cancel my subscription."
      }
    ]
  });
  ```

  ```json Final Compiled Request
  {
    "model": "gpt-4o-mini",
    "temperature": 0.4, // Runtime value used
    "max_tokens": 1000, // Saved value used
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful customer support agent for Acme Corp."
      },
      {
        "role": "user", 
        "content": "Hello, I need help with my account."
      },
      {
        "role": "user",
        "content": "Actually, I want to cancel my subscription."
      }
    ]
  }
  ```
</CodeGroup>

### Message Handling

Messages work differently than other parameters. Instead of overriding, runtime messages are **appended** to the saved prompt messages. This allows you to:

- Define consistent system prompts and example conversations in your saved prompt
- Add dynamic user messages at runtime
- Build multi-turn conversations that maintain context

<Warning>
  Runtime messages are always appended to the end of your saved prompt messages. Make sure your saved prompt structure accounts for this behavior.
</Warning>

### Override Examples

<Tabs>
  <Tab title="Temperature Override">
    ```typescript
    // Saved prompt has temperature: 0.8
    const response = await openai.chat.completions.create({
      prompt_id: "abc123",
      temperature: 0.2, // Uses 0.2, not 0.8
      inputs: { topic: "AI safety" }
    });
    ```
  </Tab>

  <Tab title="Max Tokens Override">
    ```typescript
    // Saved prompt has max_tokens: 500
    const response = await openai.chat.completions.create({
      prompt_id: "abc123", 
      max_tokens: 1500, // Uses 1500, not 500
      inputs: { complexity: "detailed" }
    });
    ```
  </Tab>

  <Tab title="Response Format Override">
    ```typescript
    // Saved prompt has no response format
    const response = await openai.chat.completions.create({
      prompt_id: "abc123",
      response_format: { type: "json_object" }, // Adds JSON formatting
      inputs: { data_type: "user_preferences" }
    });
    ```
  </Tab>
</Tabs>

<Note>
  This compilation approach gives you the flexibility to have consistent prompt templates while still allowing runtime customization for specific use cases.
</Note>

## Variables

Variables make your prompts dynamic and reusable. Define them once in your prompt template, then provide different values at runtime without changing your code.

### Variable Syntax

Variables use the format `{{hc:name:type}}` where:
- `name` is your variable identifier
- `type` defines the expected data type

<CodeGroup>
  ```text Basic Examples
  {{hc:customer_name:string}}
  {{hc:age:number}}
  {{hc:is_premium:boolean}}
  {{hc:context:any}}
  ```

  ```text In Prompt Templates
  You are a helpful assistant for {{hc:company:string}}.

  The customer {{hc:customer_name:string}} is {{hc:age:number}} years old.
  Premium status: {{hc:is_premium:boolean}}

  Additional context: {{hc:context:any}}
  ```
</CodeGroup>

### Supported Types

| Type | Description | Example Values | Validation |
|------|-------------|----------------|------------|
| `string` | Text values | `"John Doe"`, `"Hello world"` | None |
| `number` | Numeric values | `25`, `3.14`, `-10` | AI Gateway type-checking |
| `boolean` | True/false values | `true`, `false`, `"yes"`, `"no"` | AI Gateway type-checking |
| `your_type_name` | Any data type | Objects, arrays, strings | None |

<Warning>
  Only `number` and `boolean` types are validated by the Helicone AI Gateway, which will accept strings for any input as long as they can be converted to valid values.
</Warning>

Boolean variables accept multiple formats:
- `true` / `false` (boolean)
- `"yes"` / `"no"` (string)
- `"true"` / `"false"` (string)

### Schema Variables

Variables can be used within JSON schemas for tools and response formatting. This enables dynamic schema generation based on runtime inputs.

<CodeGroup>
  ```json Response Schema Example
  {
    "name": "moviebot_response",
    "strict": true,
    "schema": {
      "type": "object",
      "properties": {
        "markdown_response": {
          "type": "string"
        },
        "tools_used": {
          "type": "array",
          "items": {
            "type": "string",
            "enum": "{{hc:tools:array}}"
          }
        },
        "user_tier": {
          "type": "string",
          "enum": "{{hc:tiers:array}}"
        }
      },
      "required": [
        "markdown_response",
        "tools_used",
        "user_tier"
      ],
      "additionalProperties": false
    }
  }
  ```

  ```json Runtime Input
  {
    "tools": ["search", "calculator", "weather"],
    "tiers": ["basic", "premium", "enterprise"]
  }
  ```

  ```json Compiled Schema
  {
    "name": "moviebot_response",
    "strict": true,
    "schema": {
      "type": "object",
      "properties": {
        "markdown_response": {
          "type": "string"
        },
        "tools_used": {
          "type": "array",
          "items": {
            "type": "string",
            "enum": ["search", "calculator", "weather"]
          }
        },
        "user_tier": {
          "type": "string",
          "enum": ["basic", "premium", "enterprise"]
        }
      },
      "required": [
        "markdown_response",
        "tools_used",
        "user_tier"
      ],
      "additionalProperties": false
    }
  }
  ```
</CodeGroup>

#### Replacement Behavior

**Value Replacement**: When a variable tag is the only content in a string, it gets replaced with the actual data type:
```json
"enum": "{{hc:tools:array}}" → "enum": ["search", "calculator", "weather"]
```

**String Substitution**: When variables are part of a larger string, normal regex replacement occurs:
```json
"description": "Available for {{hc:name:string}} users" → "description": "Available for premium users"
```

**Keys and Values**: Variables work in both JSON keys and values throughout tool schemas and response schemas.

## TypeScript SDK Helpers

For TypeScript users, we provide helper types that extend the OpenAI SDK with full type safety at compile-time.

```bash
npm install @helicone/helpers
```

| Type | Description | Use Case |
|------|-------------|----------|
| `HeliconeChatCreateParams` | Standard chat completions with prompts | Non-streaming requests |
| `HeliconeChatCreateParamsStreaming` | Streaming chat completions with prompts | Streaming requests |

Both types extend the OpenAI SDK's chat completion parameters and add:
- `prompt_id` - Your saved prompt identifier
- `version_id` - Optional specific version (defaults to production version)
- `inputs` - Variable values.

### Usage Examples

<CodeGroup>
  ```typescript Non-Streaming
  import OpenAI from 'openai';
  import { HeliconeChatCreateParams } from '@helicone/helpers';

  const openai = new OpenAI({
    baseURL: "http://localhost:8080/ai",
    apiKey: "api-key",
  });

  async function generateResponse() {
    const response = await openai.chat.completions.create({
      prompt_id: "abc123",
      model: "openai/gpt-4o-mini",
      inputs: {
        customer_name: "Alice Johnson",
        product: "AI Gateway",
        tier: "premium"
      }
    } as HeliconeChatCreateParams);
    
    console.log(response.choices[0].message.content);
  }
  ```

  ```typescript Streaming
  import OpenAI from 'openai';
  import { HeliconeChatCreateParamsStreaming } from '@helicone/helpers';

  const openai = new OpenAI({
    baseURL: "http://localhost:8080/ai", 
    apiKey: "api-key",
  });

  async function streamResponse() {
    const stream = await openai.chat.completions.create({
      prompt_id: "abc123",
      model: "openai/gpt-4o-mini",
      stream: true,
      inputs: {
        topic: "artificial intelligence",
        audience: "beginners",
        length: "short"
      }
    } as HeliconeChatCreateParamsStreaming);

    for await (const chunk of stream) {
      process.stdout.write(chunk.choices[0].delta.content || '');
    }
  }
  ```

  ```typescript With Version Control
  import OpenAI from 'openai';
  import { HeliconeChatCreateParams } from '@helicone/helpers';

  const openai = new OpenAI({
    baseURL: "http://localhost:8080/ai",
    apiKey: "api-key",
  });

  async function useSpecificVersion() {
    const response = await openai.chat.completions.create({
      prompt_id: "abc123",
      version_id: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx", // Use specific version
      model: "openai/gpt-4o-mini",
      inputs: {
        user_query: "How does caching work?",
        context: "technical documentation"
      }
    } as HeliconeChatCreateParams);
    
    return response.choices[0].message.content;
  }
  ```
</CodeGroup>

<Note>
  The helper types are fully compatible with all OpenAI SDK features including function calling, response formats, and advanced parameters.
</Note>