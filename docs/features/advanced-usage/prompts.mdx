---
title: "Prompt Management"
sidebarTitle: "Prompt Management"
description: "Compose and iterate prompts, then easily deploy them in any LLM call with the AI Gateway."
"twitter:title": "Prompt Management | Helicone OSS LLM Observability"
---

## Why Prompt Management?

Traditional prompt development involves hardcoded prompts in application code, messy string substitution, and frustrating and rebuilding deployments for every iteration. This creates friction that slows down experimentation and your team's ability to ship.

<CardGroup cols={2}>
<Card title="Iterate Without Code Changes" icon="rocket">
  Test and deploy prompt changes instantly without rebuilding or redeploying your application
</Card>
<Card title="Version Control Built-In" icon="code-branch">
  Track every change, compare versions, and rollback instantly if something goes wrong
</Card>
<Card title="Dynamic Variables" icon="brackets-curly">
  Use variables anywhere - system prompts, messages, even tool schemas - for truly reusable prompts
</Card>
<Card title="Environment Management" icon="server">
  Deploy different versions to production, staging, and development environments independently
</Card>
</CardGroup>

## Quickstart

<Steps>
  <Step title="Create a Prompt">
    Build a prompt in the Playground. Save any prompt with clear commit histories and tags.

    <Frame>
      <video width="100%" autoPlay loop muted playsInline>
        <source
          src="https://marketing-assets-helicone.s3.us-west-2.amazonaws.com/Prompts2025-A.mp4"
          type="video/mp4"
        />
        <img
          src="/static/features/prompts/create_prompt_placeholder.png"
          alt="Creating a prompt in Helicone dashboard"
        />
      </video>
    </Frame>
  </Step>

  <Step title="Test and Iterate">
    Experiment with different variables, inputs, and models until you reach desired output. Variables can be used anywhere, even in tool schemas.

    <Frame>
      <video width="100%" autoPlay loop muted playsInline>
        <source
          src="https://marketing-assets-helicone.s3.us-west-2.amazonaws.com/Prompts2025-B.mp4"
          type="video/mp4"
        />
        <img
          src="/static/features/prompts/test_prompt_placeholder.png"
          alt="Testing prompts with variables and different models"
        />
      </video>
    </Frame>
  </Step>

  <Step title="Run Prompt with AI Gateway">
    Use your prompt instantly by referencing its ID in your [AI Gateway](/features/advanced-usage/cloud-ai-gateway-overview). No code changes, no rebuilds.

    <CodeGroup>
      ```typescript TypeScript
      import { OpenAI } from "openai";
      import { HeliconeChatCreateParams } from "@helicone/helpers";

      const openai = new OpenAI({
        baseURL: "https://ai-gateway.helicone.ai",
        apiKey: process.env.HELICONE_API_KEY,
      });

      const response = await openai.chat.completions.create({
        model: "gpt-4o-mini",
        prompt_id: "abc123", // Reference your saved prompt
        environment: "production", // Optional: specify environment
        messages: [
          {
            role: "user",
            content: "Hello there!"
          }
        ], // optional: saved prompt also provides messages
        inputs: {
          customer_name: "John Doe",
          product: "AI Gateway"
        }
      } as HeliconeChatCreateParams);
      ```

      ```typescript TypeScript (with satisfies)
      import { OpenAI } from "openai";
      import { HeliconeChatCreateParams } from "@helicone/helpers";

      const openai = new OpenAI({
        baseURL: "https://ai-gateway.helicone.ai",
        apiKey: process.env.HELICONE_API_KEY,
      });

      const body = {
        model: "gpt-4o-mini",
        prompt_id: "abc123",
        environment: "production", // Optional: specify environment
        messages: [
          {
            role: "user",
            content: "Hello there!"
          }
        ], // optional: saved prompt also provides messages
        inputs: {
          customer_name: "John Doe",
          product: "AI Gateway"
        }
      } satisfies HeliconeChatCreateParams;

      const response = await openai.chat.completions.create(body);
      ```

      ```python Python
      import openai
      import os

      client = openai.OpenAI(
          base_url="https://ai-gateway.helicone.ai",
          api_key=os.environ.get("HELICONE_API_KEY")
      )

      response = client.chat.completions.create(
          model="gpt-4o-mini",
          prompt_id="abc123",  # Reference your saved prompt
          environment="production",  # Optional: specify environment
          inputs={
              "customer_name": "John Doe",
              "product": "AI Gateway"
          }
      )
      ```

      ```bash cURL
      curl https://ai-gateway.helicone.ai/chat/completions \
        -H "Content-Type: application/json" \
        -H "Authorization: Bearer $HELICONE_API_KEY" \
        -d '{
          "model": "gpt-4o-mini",
          "prompt_id": "abc123",
          "environment": "production",
          "inputs": {
            "customer_name": "John Doe",
            "product": "AI Gateway"
          }
        }'
      ```
    </CodeGroup>
  </Step>
</Steps>

<Tip>
  Your prompt is automatically compiled with the provided inputs and sent to your chosen model. Update prompts in the dashboard and changes take effect immediately!
</Tip>

## Prompt Assembly Process

When you make an LLM call with a prompt ID, the AI Gateway will compile your saved prompt alongside runtime parameters you provide.

### Version Selection

The AI Gateway automatically determines which prompt version to use based on the parameters you provide:

<ParamField body="environment" type="string">
  Uses the version deployed to that environment (e.g., production, staging, development)
</ParamField>

<ParamField body="version_id" type="string">
  Uses a specific version directly by its ID
</ParamField>

<Note>
  **Default behavior**: If neither parameter is provided, the production version is used. Environment takes precedence over version_id if both are specified.
</Note>

### Parameter Priority

Saved prompts store all the configuration you set in the playground - temperature, max tokens, response format, system messages, and more. At runtime, these saved parameters are used as defaults, but any parameters you specify in your API call will override them.

<CodeGroup>
  ```json Saved Prompt Configuration
  {
    "model": "gpt-4o-mini",
    "temperature": 0.6,
    "max_tokens": 1000,
    "messages": [
      {
        "role": "system", 
        "content": "You are a helpful customer support agent for {{hc:company:string}}."
      },
      {
        "role": "user",
        "content": "Hello, I need help with my account."
      }
    ]
  }
  ```

  ```typescript Runtime API Call
  const response = await openai.chat.completions.create({
    prompt_id: "abc123",
    temperature: 0.4, // Overrides saved temperature of 0.6
    inputs: {
      company: "Acme Corp"
    },
    messages: [
      {
        "role": "user",
        "content": "Actually, I want to cancel my subscription."
      }
    ]
  });
  ```

  ```json Final Compiled Request
  {
    "model": "gpt-4o-mini",
    "temperature": 0.4, // Runtime value used
    "max_tokens": 1000, // Saved value used
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful customer support agent for Acme Corp."
      },
      {
        "role": "user", 
        "content": "Hello, I need help with my account."
      },
      {
        "role": "user",
        "content": "Actually, I want to cancel my subscription."
      }
    ]
  }
  ```
</CodeGroup>

### Message Handling

Messages work differently than other parameters. Instead of overriding, runtime messages are **appended** to the saved prompt messages. This allows you to:

- Define consistent system prompts and example conversations in your saved prompt
- Add dynamic user messages at runtime
- Build multi-turn conversations that maintain context

Since your saved prompts contain the required messages, the `messages` parameter becomes optional in API calls when using Helicone prompts. However, if your prompt template is empty or lacks messages, you'll need to provide them at runtime.

<Warning>
  Runtime messages are always appended to the end of your saved prompt messages. Make sure your saved prompt structure accounts for this behavior.
</Warning>

### Override Examples

<Tabs>
  <Tab title="Temperature Override">
    ```typescript
    // Saved prompt has temperature: 0.8
    const response = await openai.chat.completions.create({
      prompt_id: "abc123",
      temperature: 0.2, // Uses 0.2, not 0.8
      inputs: { topic: "AI safety" }
    });
    ```
  </Tab>

  <Tab title="Max Tokens Override">
    ```typescript
    // Saved prompt has max_tokens: 500
    const response = await openai.chat.completions.create({
      prompt_id: "abc123", 
      max_tokens: 1500, // Uses 1500, not 500
      inputs: { complexity: "detailed" }
    });
    ```
  </Tab>

  <Tab title="Response Format Override">
    ```typescript
    // Saved prompt has no response format
    const response = await openai.chat.completions.create({
      prompt_id: "abc123",
      response_format: { type: "json_object" }, // Adds JSON formatting
      inputs: { data_type: "user_preferences" }
    });
    ```
  </Tab>
</Tabs>

<Note>
  This compilation approach gives you the flexibility to have consistent prompt templates while still allowing runtime customization for specific use cases.
</Note>

## Managing Environments

You can easily manage different deployment environments for your prompts directly in the Helicone dashboard. Create and deploy prompts to production, staging, development, or any custom environment you need.

<Frame>
  <video width="100%" autoPlay loop muted playsInline>
    <source
      src="https://marketing-assets-helicone.s3.us-west-2.amazonaws.com/Prompts2025-Environment.mp4"
      type="video/mp4"
    />
    <img
      src="/static/features/prompts/environment_placeholder.png"
      alt="Managing prompt environments in Helicone dashboard"
    />
  </video>
</Frame>

## Variables

Variables make your prompts dynamic and reusable. Define them once in your prompt template, then provide different values at runtime without changing your code.

### Variable Syntax

Variables use the format `{{hc:name:type}}` where:
- `name` is your variable identifier
- `type` defines the expected data type

<CodeGroup>
  ```text Basic Examples
  {{hc:customer_name:string}}
  {{hc:age:number}}
  {{hc:is_premium:boolean}}
  {{hc:context:any}}
  ```

  ```text In Prompt Templates
  You are a helpful assistant for {{hc:company:string}}.

  The customer {{hc:customer_name:string}} is {{hc:age:number}} years old.
  Premium status: {{hc:is_premium:boolean}}

  Additional context: {{hc:context:any}}
  ```
</CodeGroup>

### Supported Types

| Type | Description | Example Values | Validation |
|------|-------------|----------------|------------|
| `string` | Text values | `"John Doe"`, `"Hello world"` | None |
| `number` | Numeric values | `25`, `3.14`, `-10` | AI Gateway type-checking |
| `boolean` | True/false values | `true`, `false`, `"yes"`, `"no"` | AI Gateway type-checking |
| `your_type_name` | Any data type | Objects, arrays, strings | None |

<Warning>
  Only `number` and `boolean` types are validated by the Helicone AI Gateway, which will accept strings for any input as long as they can be converted to valid values.
</Warning>

Boolean variables accept multiple formats:
- `true` / `false` (boolean)
- `"yes"` / `"no"` (string)
- `"true"` / `"false"` (string)

### Schema Variables

Variables can be used within JSON schemas for tools and response formatting. This enables dynamic schema generation based on runtime inputs.

<CodeGroup>
  ```json Response Schema Example
  {
    "name": "moviebot_response",
    "strict": true,
    "schema": {
      "type": "object",
      "properties": {
        "markdown_response": {
          "type": "string"
        },
        "tools_used": {
          "type": "array",
          "items": {
            "type": "string",
            "enum": "{{hc:tools:array}}"
          }
        },
        "user_tier": {
          "type": "string",
          "enum": "{{hc:tiers:array}}"
        }
      },
      "required": [
        "markdown_response",
        "tools_used",
        "user_tier"
      ],
      "additionalProperties": false
    }
  }
  ```

  ```json Runtime Input
  {
    "tools": ["search", "calculator", "weather"],
    "tiers": ["basic", "premium", "enterprise"]
  }
  ```

  ```json Compiled Schema
  {
    "name": "moviebot_response",
    "strict": true,
    "schema": {
      "type": "object",
      "properties": {
        "markdown_response": {
          "type": "string"
        },
        "tools_used": {
          "type": "array",
          "items": {
            "type": "string",
            "enum": ["search", "calculator", "weather"]
          }
        },
        "user_tier": {
          "type": "string",
          "enum": ["basic", "premium", "enterprise"]
        }
      },
      "required": [
        "markdown_response",
        "tools_used",
        "user_tier"
      ],
      "additionalProperties": false
    }
  }
  ```
</CodeGroup>

#### Replacement Behavior

**Value Replacement**: When a variable tag is the only content in a string, it gets replaced with the actual data type:
```json
"enum": "{{hc:tools:array}}" → "enum": ["search", "calculator", "weather"]
```

**String Substitution**: When variables are part of a larger string, normal regex replacement occurs:
```json
"description": "Available for {{hc:name:string}} users" → "description": "Available for premium users"
```

**Keys and Values**: Variables work in both JSON keys and values throughout tool schemas and response schemas.

## SDK Helpers

We provide SDKs for both TypeScript and Python that offer two ways to use Helicone prompts:

1. **[AI Gateway Integration](/gateway/prompt-integration)** - Use prompts through the Helicone AI Gateway
2. **Direct SDK Integration** - Pull prompts directly via SDK

Prompts through the AI Gateway come with several benefits:
- **Cleaner code**: Automatically performs compilation and substitution in the router.
- **Input traces**: Traces inputs on each request for better observability in Helicone requests.
- **Faster TTFT**: The AI Gateway adds significantly less latency compared to the SDK.

<Tabs>
  <Tab title="TypeScript">
    ```bash
    npm install @helicone/helpers
    ```
  </Tab>
  
  <Tab title="Python">
    ```bash
    pip install helicone-helpers openai
    ```
    
    **Note:** The OpenAI Python SDK is required for prompt management features.
  </Tab>
</Tabs>

### Types and Classes

<Tabs>
  <Tab title="TypeScript">
    The SDK provides types for both integration methods when using the OpenAI SDK:

    | Type | Description | Use Case |
    |------|-------------|----------|
    | `HeliconeChatCreateParams` | Standard chat completions with prompts | Non-streaming requests |
    | `HeliconeChatCreateParamsStreaming` | Streaming chat completions with prompts | Streaming requests |

    Both types extend the OpenAI SDK's chat completion parameters and add:
    - `prompt_id` - Your saved prompt identifier
    - `environment` - Optional environment to target (e.g., "production", "staging")
    - `version_id` - Optional specific version (defaults to production version)
    - `inputs` - Variable values
    
    **Important**: These types make `messages` optional because Helicone prompts are expected to contain the required message structure. If your prompt template is empty or doesn't include messages, you'll need to provide them at runtime.

    For direct SDK integration:
    ```typescript
    import { HeliconePromptManager } from '@helicone/helpers';

    const promptManager = new HeliconePromptManager({
      apiKey: "your-helicone-api-key"
    });
    ```
  </Tab>
  
  <Tab title="Python">
    The SDK provides types that extend OpenAI's official types:

    | Type | Description | Use Case |
    |------|-------------|----------|
    | `HeliconeChatParams` | Chat completion parameters with prompt support (includes environment) | All prompt requests |
    | `PromptCompilationResult` | Result with body and validation errors | Error handling |

    The `HeliconeChatParams` type includes all OpenAI parameters plus:
    - `prompt_id` - Your saved prompt identifier
    - `environment` - Optional environment to target (e.g., "production", "staging")
    - `version_id` - Optional specific version (defaults to production version)
    - `inputs` - Variable values for template substitution
    
    **Important**: Similar to TypeScript, `messages` becomes optional when using prompts since your saved prompt template should contain the necessary message structure.

    The main class for direct SDK integration:
    ```python
    from helicone_helpers import HeliconePromptManager

    prompt_manager = HeliconePromptManager(
        api_key="your-helicone-api-key"
    )
    ```
  </Tab>
</Tabs>

### Methods

Both SDKs provide the `HeliconePromptManager` with these main methods:

| Method | Description | Returns |
|--------|-------------|---------|
| `pullPromptVersion()` | Determine which prompt version to use | Prompt version object |
| `pullPromptBody()` | Fetch raw prompt from storage | Raw prompt body |
| `pullPromptBodyByVersionId()` | Fetch prompt by specific version ID | Raw prompt body |
| `mergePromptBody()` | Merge prompt with inputs and validation | Compilation result |
| `getPromptBody()` | Complete compile process with inputs | Compiled body + validation errors |

### Usage Examples

<Tabs>
  <Tab title="TypeScript">
    <CodeGroup>
      ```typescript Basic Usage
      import OpenAI from 'openai';
      import { HeliconePromptManager } from '@helicone/helpers';

      const openai = new OpenAI({
        baseURL: "https://ai-gateway.helicone.ai",
        apiKey: process.env.HELICONE_API_KEY,
      });

      const promptManager = new HeliconePromptManager({
        apiKey: "your-helicone-api-key"
      });

      async function generateWithPrompt() {
        // Get compiled prompt with variable substitution
        const { body, errors } = await promptManager.getPromptBody({
          prompt_id: "abc123",
          model: "gpt-4o-mini",
          inputs: {
            customer_name: "Alice Johnson",
            product: "AI Gateway"
          }
        });

        // Check for validation errors
        if (errors.length > 0) {
          console.warn("Validation errors:", errors);
        }

        // Use compiled prompt with OpenAI SDK
        const response = await openai.chat.completions.create(body);
        console.log(response.choices[0].message.content);
      }
      ```

      ```typescript With Environment Control
      import OpenAI from 'openai';
      import { HeliconePromptManager } from '@helicone/helpers';

      const openai = new OpenAI({
        baseURL: "https://ai-gateway.helicone.ai",
        apiKey: process.env.HELICONE_API_KEY,
      });

      const promptManager = new HeliconePromptManager({
        apiKey: "your-helicone-api-key"
      });

      async function useEnvironmentVersion() {
        const { body, errors } = await promptManager.getPromptBody({
          prompt_id: "abc123",
          environment: "staging", // Use staging environment
          model: "gpt-4o-mini",
          inputs: {
            user_query: "How does caching work?",
            context: "technical documentation"
          },
          messages: [
            { role: "user", content: "Follow up question..." }
          ]
        });

        if (errors.length > 0) {
          console.warn("Variable validation failed:", errors);
        }

        return await openai.chat.completions.create(body);
      }
      ```

      ```typescript With Specific Version
      async function useSpecificVersion() {
        const { body, errors } = await promptManager.getPromptBody({
          prompt_id: "abc123",
          version_id: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx",
          model: "gpt-4o-mini",
          inputs: {
            user_query: "How does caching work?",
            context: "technical documentation"
          }
        });

        if (errors.length > 0) {
          console.warn("Variable validation failed:", errors);
        }

        return await openai.chat.completions.create(body);
      }
      ```

      ```typescript Error Handling
      import OpenAI from 'openai';
      import { HeliconePromptManager } from '@helicone/helpers';

      const promptManager = new HeliconePromptManager({
        apiKey: "your-helicone-api-key"
      });

      async function handleValidationErrors() {
        const { body, errors } = await promptManager.getPromptBody({
          prompt_id: "abc123",
          model: "gpt-4o-mini",
          inputs: {
            age: "not-a-number", // This will cause a validation error
            is_premium: "maybe"   // This will cause a validation error
          }
        });

        // Handle validation errors
        if (errors.length > 0) {
          errors.forEach(error => {
            console.error(`Variable "${error.variable}" validation failed:`);
            console.error(`  Expected: ${error.expected}`);
            console.error(`  Received: ${JSON.stringify(error.value)}`);
          });
          
          // Decide how to handle: throw error, use defaults, prompt user, etc.
          throw new Error(`Prompt validation failed: ${errors.length} errors`);
        }

        // Proceed with valid prompt
        const openai = new OpenAI({ 
          baseURL: "https://ai-gateway.helicone.ai",
          apiKey: process.env.HELICONE_API_KEY,
        });
        return await openai.chat.completions.create(body);
      }
      ```
    </CodeGroup>
  </Tab>

  <Tab title="Python">
    <CodeGroup>
      ```python Basic Usage
      import openai
      import os
      from helicone_helpers import HeliconePromptManager

      client = openai.OpenAI(
          base_url="https://ai-gateway.helicone.ai",
          api_key=os.environ.get("HELICONE_API_KEY")
      )

      prompt_manager = HeliconePromptManager(
          api_key="your-helicone-api-key"
      )

      def generate_with_prompt():
          # Get compiled prompt with variable substitution
          result = prompt_manager.get_prompt_body({
              "prompt_id": "abc123",
              "model": "gpt-4o-mini",
              "inputs": {
                  "customer_name": "Alice Johnson",
                  "product": "AI Gateway"
              }
          })

          # Check for validation errors
          if result["errors"]:
              print("Validation errors:", result["errors"])

          # Use compiled prompt with OpenAI SDK
          response = client.chat.completions.create(**result["body"])
          print(response.choices[0].message.content)
      ```

      ```python With Environment Control
      import openai
      import os
      from helicone_helpers import HeliconePromptManager

      client = openai.OpenAI(
          base_url="https://ai-gateway.helicone.ai",
          api_key=os.environ.get("HELICONE_API_KEY")
      )

      prompt_manager = HeliconePromptManager(
          api_key="your-helicone-api-key"
      )

      def use_environment_version():
          result = prompt_manager.get_prompt_body({
              "prompt_id": "abc123",
              "environment": "staging",  # Use staging environment
              "model": "gpt-4o-mini",
              "inputs": {
                  "user_query": "How does caching work?",
                  "context": "technical documentation"
              },
              "messages": [
                  {"role": "user", "content": "Follow up question..."}
              ]
          })

          if result["errors"]:
              print("Variable validation failed:", result["errors"])

          return client.chat.completions.create(**result["body"])
      ```

      ```python With Specific Version
      def use_specific_version():
          result = prompt_manager.get_prompt_body({
              "prompt_id": "abc123",
              "version_id": "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx",
              "model": "gpt-4o-mini",
              "inputs": {
                  "user_query": "How does caching work?",
                  "context": "technical documentation"
              }
          })

          if result["errors"]:
              print("Variable validation failed:", result["errors"])

          return client.chat.completions.create(**result["body"])
      ```

      ```python Error Handling
      import openai
      import os
      from helicone_helpers import HeliconePromptManager

      prompt_manager = HeliconePromptManager(
          api_key="your-helicone-api-key"
      )

      def handle_validation_errors():
          result = prompt_manager.get_prompt_body({
              "prompt_id": "abc123",
              "model": "gpt-4o-mini",
              "inputs": {
                  "age": "not-a-number",  # This will cause a validation error
                  "is_premium": "maybe"   # This will cause a validation error
              }
          })

          # Handle validation errors
          if result["errors"]:
              for error in result["errors"]:
                  print(f'Variable "{error.variable}" validation failed:')
                  print(f"  Expected: {error.expected}")
                  print(f"  Received: {error.value}")
              
              # Decide how to handle: throw error, use defaults, prompt user, etc.
              raise ValueError(f'Prompt validation failed: {len(result["errors"])} errors')

          # Proceed with valid prompt
          client = openai.OpenAI(
              base_url="https://ai-gateway.helicone.ai",
              api_key=os.environ.get("HELICONE_API_KEY")
          )
          return client.chat.completions.create(**result["body"])
      ```
    </CodeGroup>
  </Tab>
</Tabs>

The Helicone AI Gateway is the recommended way to interact with prompts, as it offers a fully OpenAI-compatible router, caching, rate limits, and more alongside prompts usage. However, the SDK is a great option for users that need direct interaction with compiled prompt bodies.

<Note>
  Both approaches are fully compatible with all OpenAI SDK features including function calling, response formats, and advanced parameters. The `HeliconePromptManager`, while not providing input traces, will provide validation error handling.
</Note>