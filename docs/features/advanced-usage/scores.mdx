---
title: "Scores"
sidebarTitle: "Scores"
description: "Add custom scoring metrics to your LLM requests and experiments. Evaluate prompt performance, compare results across datasets, and quantify model outputs for continuous improvement."
"twitter:title": "Scores - Helicone OSS LLM Observability"
---

import QuestionsSection from "/snippets/questions-section.mdx";

<Info>
  **Who can use this feature**: Anyone on any
  [plan](https://www.helicone.ai/pricing).
</Info>

## Introduction

Helicone's scores [API](https://docs.helicone.ai/rest/request/post-v1request-score) allows you to score your requests and experiments. You can use this feature to evaluate the performance of your prompts and compare different experiments and datasets. E.g., if you are building an image classification application, you might need a variety of scores to help you determine how accurate the outputs are compared to what you expect. For example, an image classification app might have one score that tells you how accurate the model classifies images into the correct categories, and another that measures the confidence level of the model's predictions.

<Frame caption="Example: Experiment scores. ">
  <img
    src="/images/scores/example-experiment-scores.png"
    alt="Example of experiment scores compared to the original scores displayed in Helicone."
  />
</Frame>

### Why Scores

Scoring request allows you:

- Evaluate the performance of your prompts.
- Compare the scores of different experiments and datasets.

<Info>
  We are currently not supporting autoscoring, but you can write your own logic
  and submit scores via our
  [API](https://docs.helicone.ai/rest/request/post-v1request-score).
</Info>

## Quick Start

### Option 1: Using the Request Page

You can add scores to your `requests` directly from the request page:

<Frame caption="Example: Adding scores on the request page. ">
  <img
    src="/images/scores/example-scores-request-page.png"
    alt="Example of adding scores post-request on the Requests page in Helicone."
  />
</Frame>

### Option 2: Setting up your own Scoring Webhook

You can set up your own scoring webhook to score your requests. Here's an example of how you can do this with Cloudflare Workers:

<Tabs>
  <Tab title="Example">
    <Steps>
    <Step title="Deploy Worker">
    First, deploy Cloudflare Worker template by clicking the button above. [![Deploy with Workers](https://deploy.workers.cloudflare.com/button)](https://deploy.workers.cloudflare.com/?url=https://github.com/Helicone/helicone/tree/main/examples/worker-helicone-scores)
    </Step>
      <Step title="Create a Webhook">
     Create a webhook to be able receive request and response data [create a webhook](/features/webhooks) with your Scoring Worker URL.
      </Step>

      <Step title="Customize your scoring logic">
      You can can customize the scoring logic in the `index.js` file in your Scoring Worker.

      ```tsx
      // You can customize the scoring function below and add more scores as needed.
      function calculateScore(data: HeliconeRequest): Record<string, number> {
        if (data.response_body) {
          return {
            vocabulary_diversity: calculateVocabularyDiversity(data.response_body),
            // Add more scores here
          };
        }

        return {};
      }
      ```


      </Step>
    </Steps>

  </Tab>
</Tabs>

## Local Testing

If you want to test your scoring logic locally you'll need to use wrangler secrets to add appropriate value for `HELICONE_AUTH`.

```sh
$ wrangler secret put HELICONE_AUTH
```

And run Scoring Webhook locally:

```sh
$ wrangler deploy
```

<QuestionsSection />
