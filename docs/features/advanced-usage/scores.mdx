---
title: "Eval Scores"
sidebarTitle: "Eval Scores"
---

When running evaluation frameworks to measure model performance, you need visibility into how well your AI applications are performing across different metrics. Scores let you report evaluation results from any framework to Helicone, providing centralized observability for accuracy, hallucination rates, helpfulness, and custom metrics.

## Why use Scores

- **Centralize evaluation results**: Report scores from any evaluation framework for unified monitoring and analysis
- **Track model performance over time**: Visualize how accuracy, hallucination rates, and other metrics evolve
- **Compare experiments side-by-side**: Evaluate different prompts, models, or configurations with consistent metrics

## Quick Start

<Steps>
<Step title="Run your evaluation">
Execute your evaluation framework to assess model outputs:

```typescript
// Example using your evaluation framework
const evalResult = await evaluateLLMResponse({
  prompt: "Explain quantum computing",
  response: llmResponse,
  expectedBehavior: "accurate, technical, no hallucinations"
});

// Returns scores like:
// {accuracy: 92, hallucination: 8, helpfulness: 85}
```
</Step>

<Step title="Report scores to Helicone">
Send evaluation results using the Helicone API:

```typescript
// Get the request ID from response headers
const requestId = response.headers.get("helicone-id");

// Report evaluation scores
await fetch(`https://api.helicone.ai/v1/request/${requestId}/score`, {
  method: "POST",
  headers: {
    "Authorization": `Bearer ${HELICONE_API_KEY}`,
    "Content-Type": "application/json"
  },
  body: JSON.stringify({
    scores: {
      "accuracy": 92,        // Integer values required
      "hallucination": 8,    // Converted to integers (0.08 * 100)
      "helpfulness": 85,
      "is_safe": true        // Booleans supported
    }
  })
});
```

<Accordion title="Alternative: Add scores via dashboard">
You can also add scores directly in the Helicone dashboard on the request details page. This is useful for manual evaluation or quick testing.
</Accordion>
</Step>

<Step title="View score analytics">
Analyze evaluation results in the Helicone dashboard to track performance trends, compare experiments, and identify areas for improvement.
</Step>
</Steps>

## API Format

### Request Structure

The scores API expects this exact format:

| Field | Type | Description | Required | Example |
|-------|------|-------------|----------|---------|
| `scores` | `object` | Key-value pairs of evaluation metrics | ✅ Yes | `{"accuracy": 92}` |

### Score Values

| Type | Description | Example |
|------|-------------|---------|
| `integer` | Numeric scores (no decimals) | `92`, `85`, `0` |
| `boolean` | Pass/fail or true/false metrics | `true`, `false` |

**Important:** Float values like `0.92` are rejected. Convert to integers: `0.92` → `92`

#### Detailed Explanations

<AccordionGroup>
<Accordion title="Batch Scoring">
Report multiple evaluation metrics in a single API call:

```typescript
// Report all evaluation metrics at once
await fetch(`https://api.helicone.ai/v1/request/${requestId}/score`, {
  method: "POST",
  headers: {
    "Authorization": `Bearer ${HELICONE_API_KEY}`,
    "Content-Type": "application/json"
  },
  body: JSON.stringify({
    scores: {
      "accuracy": 92,             // Convert decimals to integers
      "hallucination_rate": 8,    // 0.08 * 100
      "response_relevance": 87,   // 0.87 * 100
      "context_precision": 90,    // 0.90 * 100
      "faithfulness": 85          // 0.85 * 100
    }
  })
});
```
</Accordion>

<Accordion title="Score Metadata">
Include context about your evaluation for better analysis:

```typescript
// Note: Scores API currently doesn't support metadata in request body
// Store evaluation context in your application or use Custom Properties:
const response = await openai.chat.completions.create({
  model: "gpt-4",
  messages: [{ role: "user", content: "Test message" }]
}, {
  headers: {
    "Helicone-Auth": `Bearer ${HELICONE_API_KEY}`,
    "Helicone-Property-Framework": "custom-eval-v2",
    "Helicone-Property-Dataset": "golden-test-set",
    "Helicone-Property-Environment": "production"
  }
});

// Then report scores using the API
await fetch(`https://api.helicone.ai/v1/request/${requestId}/score`, {
  method: "POST",
  headers: {
    "Authorization": `Bearer ${HELICONE_API_KEY}`,
    "Content-Type": "application/json"
  },
  body: JSON.stringify({ scores })
});
```
</Accordion>
</AccordionGroup>

## Use Cases

<Tabs>
<Tab title="RAG Accuracy Evaluation">
Evaluate retrieval-augmented generation for accuracy and hallucination:

<CodeGroup>
```python Python
import requests
from ragas import evaluate
from ragas.metrics import Faithfulness, ResponseRelevancy
from datasets import Dataset

# Run RAG evaluation
def evaluate_rag_response(question, answer, contexts, ground_truth, requestId):
    # Initialize RAGAS metrics
    metrics = [Faithfulness(), ResponseRelevancy()]
    
    # Create dataset in RAGAS format
    data = {
        "question": [question],
        "answer": [answer], 
        "contexts": [contexts],
        "ground_truth": [ground_truth]
    }
    dataset = Dataset.from_dict(data)
    
    # Run evaluation
    result = evaluate(dataset, metrics=metrics)
    
    # Extract scores (RAGAS returns 0-1 values)
    faithfulness_score = result['faithfulness'] if 'faithfulness' in result else 0
    relevancy_score = result['answer_relevancy'] if 'answer_relevancy' in result else 0
    
    # Report to Helicone (convert to 0-100 scale)
    response = requests.post(
        f"https://api.helicone.ai/v1/request/{requestId}/score",
        headers={
            "Authorization": f"Bearer {HELICONE_API_KEY}",
            "Content-Type": "application/json"
        },
        json={
            "scores": {
                "faithfulness": int(faithfulness_score * 100),
                "answer_relevancy": int(relevancy_score * 100)
            }
        }
    )
    
    return result

# Example usage
scores = evaluate_rag_response(
    question="What is the capital of France?",
    answer="The capital of France is Paris.",
    contexts=["France is a country in Europe. Paris is its capital."],
    ground_truth="Paris",
    requestId="your-request-id-here"
)
```

```typescript TypeScript
// RAG evaluation with custom metrics
async function evaluateRAGResponse(
  question: string,
  answer: string,
  contexts: string[],
  requestId: string
) {
  // Custom evaluation logic
  const scores = {
    relevance: calculateRelevance(answer, question),
    groundedness: checkGroundedness(answer, contexts),
    completeness: measureCompleteness(answer, question),
    hallucination: detectHallucination(answer, contexts)
  };
  
  // Report to Helicone
  await fetch(`https://api.helicone.ai/v1/request/${requestId}/score`, {
    method: "POST",
    headers: {
      "Authorization": `Bearer ${HELICONE_API_KEY}`,
      "Content-Type": "application/json"
    },
    body: JSON.stringify({ scores })
  });
  
  // Alert on poor performance
  if (scores.hallucination > 0.2) {
    console.warn("High hallucination detected:", scores);
  }
  
  return scores;
}
```
</CodeGroup>
</Tab>

<Tab title="Code Generation Quality">
Evaluate code generation for correctness, style, and functionality:

```typescript
// Evaluate generated code quality
async function evaluateCodeGeneration(
  prompt: string,
  generatedCode: string,
  requestId: string
) {
  const scores = {
    // Syntax validity
    syntax_valid: await validateSyntax(generatedCode) ? 1.0 : 0.0,
    
    // Test pass rate
    test_pass_rate: await runTests(generatedCode),
    
    // Code quality metrics
    complexity: calculateCyclomaticComplexity(generatedCode),
    readability: assessReadability(generatedCode),
    
    // Security checks
    security_score: await runSecurityScan(generatedCode),
    
    // Performance benchmarks
    performance: await benchmarkCode(generatedCode)
  };
  
  // Report comprehensive evaluation
  await fetch(`https://api.helicone.ai/v1/request/${requestId}/score`, {
    method: "POST",
    headers: {
      "Authorization": `Bearer ${HELICONE_API_KEY}`,
      "Content-Type": "application/json"
    },
    body: JSON.stringify({
      scores: {
        ...scores,
        // Convert any decimal scores to integers
        test_pass_rate: Math.round(scores.test_pass_rate * 100)
      }
    })
  });
  
  return scores;
}
```
</Tab>

<Tab title="Helpfulness & Safety">
Evaluate model outputs for helpfulness, safety, and alignment:

```python
# Multi-dimensional evaluation for chatbots
async def evaluate_chat_response(user_query, assistant_response, requestId):
    # Use LLM as judge for subjective metrics
    eval_prompt = f"""
    Rate the following assistant response on these criteria (0-1):
    - Helpfulness: How well does it address the user's question?
    - Safety: Is the response safe and appropriate?
    - Accuracy: Is the information correct?
    - Clarity: Is the response clear and well-structured?
    
    User: {user_query}
    Assistant: {assistant_response}
    """
    
    # Get evaluation from judge model
    eval_scores = await llm_judge(eval_prompt)
    
    # Add objective metrics
    scores = {
        **eval_scores,
        "response_length": len(assistant_response),
        "reading_level": calculate_reading_level(assistant_response),
        "contains_refusal": "I cannot" in assistant_response or "I won't" in assistant_response
    }
    
    # Report all scores (convert decimals to integers)
    integer_scores = {
        key: int(value * 100) if isinstance(value, float) and 0 <= value <= 1 else value
        for key, value in scores.items()
    }
    
    response = requests.post(
        f"https://api.helicone.ai/v1/request/{requestId}/score",
        headers={
            "Authorization": f"Bearer {HELICONE_API_KEY}",
            "Content-Type": "application/json"
        },
        json={"scores": integer_scores}
    )
    
    return scores
```
</Tab>
</Tabs>

## Understanding Scores

### Score Timing and Delays

**Important considerations:**
- Scores are processed with a **10 minute delay** by default for analytics aggregation
- Real-time access available via PostgreSQL, analytics via ClickHouse after delay
- Multiple scores can be reported in a single API call for efficiency

**Workflow example:**
```typescript
// 1. Make LLM request
const response = await openai.chat.completions.create({
  model: "gpt-4",
  messages: [{ role: "user", content: prompt }]
}, {
  headers: { "Helicone-Auth": `Bearer ${HELICONE_API_KEY}` }
});

const requestId = response.headers.get("helicone-id");

// 2. Run evaluation (can be async)
const scores = await evaluateResponse(response);

// 3. Report scores immediately
await fetch(`https://api.helicone.ai/v1/request/${requestId}/score`, {
  method: "POST",
  headers: {
    "Authorization": `Bearer ${HELICONE_API_KEY}`,
    "Content-Type": "application/json"
  },
  body: JSON.stringify({ scores })
});

// 4. Scores appear in analytics dashboard after ~10 minutes
```


## Related Features

<CardGroup cols={2}>
<Card title="Experiments" icon="flask" href="/features/experiments">
Compare different configurations with consistent scoring
</Card>

<Card title="Sessions" icon="link" href="/features/sessions">
Evaluate multi-turn conversations and workflows
</Card>

<Card title="Custom Properties" icon="tag" href="/features/advanced-usage/custom-properties">
Tag requests for segmented evaluation analysis
</Card>

<Card title="Webhooks" icon="webhook" href="/features/webhooks">
Trigger evaluations automatically when requests complete
</Card>
</CardGroup>

<Snippet file="questions-section.mdx" />