---
title: "LLM Caching"
sidebarTitle: "Caching"
"twitter:title": "LLM Cache - Helicone OSS LLM Observability"
---

import QuestionsSection from "/snippets/questions-section.mdx";

When developing and testing LLM applications, you often make the same requests repeatedly during debugging and iteration. Caching stores responses on the edge using Cloudflare Workers, eliminating redundant API calls and reducing both latency and costs.

## Why use Caching

- **Save money during development**: Avoid repeated charges for identical requests while testing and debugging
- **Reduce response latency**: Serve cached responses instantly instead of waiting for LLM providers
- **Handle traffic spikes**: Protect against rate limits and maintain performance during high usage

<Frame caption="Dashboard view of cache hits, cost and time saved">
  <img
    src="/images/example-cache.png"
    alt="Helicone Dashboard showing the number of cache hits, cost, and time saved."
  />
</Frame>

<iframe
  width="100%"
  height="420"
  src="https://www.youtube.com/embed/qIOq_NbeQ28?autoplay=1&mute=1"
></iframe>

## Quick Start

<Steps>
<Step title="Enable caching">
Add the `Helicone-Cache-Enabled` header to your requests:

```typescript
{
  "Helicone-Cache-Enabled": "true"
}
```
</Step>

<Step title="Make your request">
Execute your LLM request - the first call will be cached:

```typescript
const response = await openai.chat.completions.create(
  {
    model: "gpt-4o-mini",
    messages: [{ role: "user", content: "Hello world" }]
  },
  {
    headers: {
      "Helicone-Cache-Enabled": "true"
    }
  }
);
```
</Step>

<Step title="Verify caching works">
Make the same request again - it should return instantly from cache:

```typescript
// This exact same request will return a cached response
const cachedResponse = await openai.chat.completions.create(
  {
    model: "gpt-4o-mini", 
    messages: [{ role: "user", content: "Hello world" }]
  },
  {
    headers: {
      "Helicone-Cache-Enabled": "true"
    }
  }
);
```
</Step>
</Steps>

## Configuration Options

### Basic Settings

Control caching behavior with these headers:

| Header | Type | Description | Default | Example |
|--------|------|-------------|---------|---------|
| `Helicone-Cache-Enabled` | `string` | Enable or disable caching | N/A | `"true"` |
| `Cache-Control` | `string` | Set cache duration using max-age | `"max-age=604800"` (7 days) | `"max-age=3600"` (1 hour) |

### Advanced Settings

| Header | Type | Description | Default | Example |
|--------|------|-------------|---------|---------|
| `Helicone-Cache-Bucket-Max-Size` | `string` | Number of responses to store per cache bucket | `"1"` | `"3"` |
| `Helicone-Cache-Seed` | `string` | Create separate cache namespaces | N/A | `"user-123"` |
| `Helicone-Cache-Ignore-Keys` | `string` | Comma-separated JSON keys to exclude from cache key generation | N/A | `"request_id,timestamp"` |

<Info>
All header values must be strings. For example, `"Helicone-Cache-Bucket-Max-Size": "10"`.
</Info>

<AccordionGroup>
<Accordion title="Cache Duration">
Set how long responses stay cached using the `Cache-Control` header:

```typescript
{
  "Cache-Control": "max-age=3600"  // 1 hour
}
```

**Common durations:**
- 1 hour: `max-age=3600`
- 1 day: `max-age=86400`
- 7 days: `max-age=604800` (default)
- 30 days: `max-age=2592000`

<Note>Maximum cache duration is 365 days (`max-age=31536000`)</Note>
</Accordion>

<Accordion title="Bucket Size">
Control how many different responses are stored for the same request:

```typescript
{
  "Helicone-Cache-Bucket-Max-Size": "3"
}
```

With bucket size 3, the same request can return one of 3 different cached responses randomly:

```
openai.completion("give me a random number") -> "42"  # Cache Miss
openai.completion("give me a random number") -> "47"  # Cache Miss  
openai.completion("give me a random number") -> "17"  # Cache Miss

openai.completion("give me a random number") -> "42" | "47" | "17"  # Cache Hit
```

<Note>Maximum bucket size is 20. Enterprise plans support larger buckets.</Note>
</Accordion>

<Accordion title="Cache Seeds">
Create separate cache namespaces using seeds:

```typescript
{
  "Helicone-Cache-Seed": "user-123"
}
```

Different seeds maintain separate cache states:

```
# Seed: "user-123"
openai.completion("random number") -> "42"
openai.completion("random number") -> "42"  # Same response

# Seed: "user-456"  
openai.completion("random number") -> "17"  # Different response
openai.completion("random number") -> "17"  # Consistent per seed
```

<Tip>Change the seed value to effectively clear your cache for testing.</Tip>
</Accordion>

<Accordion title="Ignore Keys">
Exclude specific JSON fields from cache key generation:

```typescript
{
  "Helicone-Cache-Ignore-Keys": "request_id,timestamp,session_id"
}
```

When these fields are ignored, requests with different values for these fields will still hit the same cache entry:

```typescript
// First request
const response1 = await openai.chat.completions.create(
  {
    model: "gpt-4o-mini",
    messages: [{ role: "user", content: "Hello" }],
    request_id: "req-123",
    timestamp: "2024-01-01T00:00:00Z"
  },
  {
    headers: {
      "Helicone-Cache-Enabled": "true",
      "Helicone-Cache-Ignore-Keys": "request_id,timestamp"
    }
  }
);

// Second request with different request_id and timestamp
// This will hit the cache despite different values
const response2 = await openai.chat.completions.create(
  {
    model: "gpt-4o-mini",
    messages: [{ role: "user", content: "Hello" }],
    request_id: "req-456",  // Different ID
    timestamp: "2024-02-02T00:00:00Z"  // Different timestamp
  },
  {
    headers: {
      "Helicone-Cache-Enabled": "true",
      "Helicone-Cache-Ignore-Keys": "request_id,timestamp"
    }
  }
);
// response2 returns cached response from response1
```

<Note>This feature only works with JSON request bodies. Non-JSON bodies will use the original text for cache key generation.</Note>

**Common use cases:**
- Ignore tracking IDs that don't affect the response
- Exclude timestamps for time-independent queries
- Remove session or user metadata when caching shared content
- Ignore `prompt_cache_key` when using OpenAI prompt caching alongside Helicone caching
</Accordion>
</AccordionGroup>

## Use Cases

<Tabs>
<Tab title="Development Testing">
Avoid repeated charges while debugging and iterating on prompts:

<CodeGroup>
```typescript Node.js
import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
  baseURL: "https://oai.helicone.ai/v1",
  defaultHeaders: {
    "Helicone-Auth": `Bearer ${process.env.HELICONE_API_KEY}`,
    "Helicone-Cache-Enabled": "true",
    "Cache-Control": "max-age=86400" // Cache for 1 day during development
  },
});

// This request will be cached
const response = await openai.chat.completions.create({
  model: "gpt-4o-mini",
  messages: [{ role: "user", content: "Explain quantum computing" }]
});

// Subsequent identical requests return cached response instantly
```

```python Python
import openai

client = openai.OpenAI(
    api_key=os.environ.get("OPENAI_API_KEY"),
    base_url="https://oai.helicone.ai/v1",
    default_headers={
        "Helicone-Auth": f"Bearer {os.environ.get('HELICONE_API_KEY')}",
        "Helicone-Cache-Enabled": "true",
        "Cache-Control": "max-age=86400"  # Cache for 1 day
    }
)

response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Explain quantum computing"}]
)
```
</CodeGroup>
</Tab>

<Tab title="Code Explanation Bot">
Cache explanations for commonly asked code snippets across your team:

```typescript
// Use a consistent identifier for the code snippet
const codeIdentifier = `code-${codeSnippet.length}-${codeSnippet.slice(0, 20)}`;

const response = await openai.chat.completions.create(
  {
    model: "gpt-4o-mini",
    messages: [{ 
      role: "user", 
      content: `Explain this code:\n\n${codeSnippet}` 
    }]
  },
  {
    headers: {
      "Helicone-Cache-Enabled": "true",
      "Helicone-Cache-Seed": codeIdentifier,    // Same code = same cache
      "Cache-Control": "max-age=604800"         // Cache for 1 week
    }
  }
);

// Multiple developers asking about the same function get instant responses
```
</Tab>

<Tab title="Documentation Q&A">
Cache answers to frequently asked questions about your API or product:

```typescript
const response = await openai.chat.completions.create(
  {
    model: "gpt-4o-mini",
    messages: [{ 
      role: "user", 
      content: "How do I authenticate with the API?" 
    }]
  },
  {
    headers: {
      "Helicone-Cache-Enabled": "true",
      "Cache-Control": "max-age=86400",          // Cache for 24 hours
      "Helicone-Cache-Bucket-Max-Size": "1"     // Consistent answers
    }
  }
);

// Common documentation questions get instant, consistent responses
// Perfect for chatbots, help widgets, and FAQ systems
```
</Tab>

<Tab title="OpenAI Prompt Caching">
Combine Helicone caching with OpenAI's native prompt caching by ignoring the `prompt_cache_key` parameter:

```typescript
const response = await openai.chat.completions.create(
  {
    model: "gpt-4o-mini",
    messages: [{ 
      role: "user", 
      content: "Analyze this large document with cached context..." 
    }],
    prompt_cache_key: `doc-analysis-${documentId}` // Different per document
  },
  {
    headers: {
      "Helicone-Cache-Enabled": "true",
      "Helicone-Cache-Ignore-Keys": "prompt_cache_key", // Ignore this for Helicone cache
      "Cache-Control": "max-age=3600"                   // Cache for 1 hour
    }
  }
);

// Requests with the same message but different prompt_cache_key values 
// will hit Helicone's cache, while still leveraging OpenAI's prompt caching
// for improved performance and cost savings on both sides
```

This approach:
- Uses OpenAI's prompt caching for faster processing of repeated context
- Uses Helicone's caching for instant responses to identical requests  
- Ignores `prompt_cache_key` so Helicone cache works across different OpenAI cache entries
- Maximizes cost savings by combining both caching strategies
</Tab>
</Tabs>

## Understanding Caching

### How Cache Keys Work
Helicone generates cache keys by hashing:
- **Cache seed**: If specified (for namespacing)
- **Request URL**: The full endpoint URL
- **Request body**: Complete request payload including all parameters
- **Relevant headers**: Authorization and Helicone-specific cache headers
- **Bucket index**: For multi-response caching

**What triggers cache hits:**
```typescript
// ✅ Cache hit - identical requests
const request1 = { model: "gpt-4o-mini", messages: [{ role: "user", content: "Hello" }] };
const request2 = { model: "gpt-4o-mini", messages: [{ role: "user", content: "Hello" }] };

// ❌ Cache miss - different content  
const request3 = { model: "gpt-4o-mini", messages: [{ role: "user", content: "Hi" }] };

// ❌ Cache miss - different parameters
const request4 = { model: "gpt-4o-mini", messages: [{ role: "user", content: "Hello" }], temperature: 0.5 };
```

### Cache Response Headers
Check cache status in response headers:

```typescript
const response = await openai.chat.completions.create(
  { /* your request */ },
  { 
    headers: { "Helicone-Cache-Enabled": "true" }
  }
);

// Access raw response to check headers
const chatCompletion = await client.chat.completions.with_raw_response.create(
  { /* your request */ },
  { 
    headers: { "Helicone-Cache-Enabled": "true" }
  }
);

const cacheStatus = chatCompletion.http_response.headers.get('Helicone-Cache');
console.log(cacheStatus); // "HIT" or "MISS"

const bucketIndex = chatCompletion.http_response.headers.get('Helicone-Cache-Bucket-Idx');
console.log(bucketIndex); // Index of cached response used
```

### Cache Bucket Behavior
Cache buckets store multiple responses for the same request:

**Bucket Size 1 (default):**
- Same request always returns same cached response
- Deterministic behavior

**Bucket Size > 1:**
- Same request can return different cached responses
- Useful for creative prompts where variety is desired
- Response chosen randomly from bucket

### Cache Limitations
- **Maximum duration**: 365 days
- **Maximum bucket size**: 20 (enterprise plans support more)
- **Cache key sensitivity**: Any parameter change creates new cache entry
- **Storage location**: Cached in Cloudflare Workers KV (edge-distributed), not your infrastructure

## Related Features

<CardGroup cols={2}>
<Card title="Custom Properties" icon="tag" href="/features/advanced-usage/custom-properties">
Add metadata to cached requests for better filtering and analysis
</Card>

<Card title="Rate Limiting" icon="clock" href="/features/advanced-usage/custom-rate-limits">
Control request frequency and combine with caching for cost optimization
</Card>

<Card title="User Metrics" icon="chart-line" href="/features/advanced-usage/user-metrics">
Track cache hit rates and savings per user or application
</Card>

<Card title="Webhooks" icon="webhook" href="/features/webhooks">
Get notified about cache performance and optimization opportunities
</Card>
</CardGroup>

<QuestionsSection />