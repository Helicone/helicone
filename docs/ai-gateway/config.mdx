---
title: "Configuration Reference"
sidebarTitle: "Configuration"
description: "Complete reference for configuring your LLM Gateway"
---

The AI Gateway is configured through a `ai-gateway-config.yaml` file that defines how requests are routed, load balanced, and processed across different LLM providers.

## Routers

Each Helicone AI Gateway deployment can configure multiple independent routing policies for different use cases. Each router operates with its own load balancing strategy, provider set, and configuration.

<ParamField path="routers" type="object">
  Define one or more routers. Each router name becomes part of the URL path when making requests.

  ```yaml
  routers:
    production:
      load-balance:
        chat:
          strategy: latency
          providers:
            - openai
            - anthropic
    
    development:
      load-balance:
        chat:
          strategy: weighted
          providers:
            - provider: anthropic
              weight: '0.9'
            - provider: openai
              weight: '0.1'
  ```

  **Usage:** Set your OpenAI SDK baseURL to `http://localhost:8080/production` or `http://localhost:8080/experimental`
</ParamField>

## Load Balancing

Distribute requests across multiple providers to optimize performance, costs, and reliability. The gateway supports latency-based and weighted strategies for different use cases.

[Learn more](/ai-gateway/concepts/load-balancing)

### Latency Strategy

<ParamField path="routers.[name].load-balance.chat.strategy" type="string">
  Use `latency` for automatic load balancing that routes to the provider with the lowest latency.

  ```yaml
  routers:
    production:
      load-balance:
        chat:
          strategy: latency
          providers:
            - openai
            - anthropic
            - gemini
            - ollama
  ```
</ParamField>

### Weighted Strategy

<ParamField path="routers.[name].load-balance.chat.strategy" type="string">
  Use `weighted` to distribute requests based on specific percentages.

  ```yaml
  routers:
    production:
      load-balance:
        chat:
          strategy: weighted
          providers:
            - provider: anthropic
              weight: '0.95'
            - provider: openai
              weight: '0.05'
  ```

  **Important:** Weights must sum to exactly `1.0`
</ParamField>

### Load Balancing Providers

<ParamField path="routers.[name].load-balance.chat.providers" type="array">
  List of target providers for load balancing.

  **For Latency Strategy:**
  ```yaml
  providers:
    - openai
    - anthropic
    - gemini
  ```

  **For Weighted Strategy:**
  ```yaml
  providers:
    - provider: anthropic
      weight: '0.7'
    - provider: openai
      weight: '0.3'
  ```
</ParamField>

## Caching

Store and reuse LLM responses for identical requests to dramatically reduce costs and improve response times. Cache directives control response freshness and staleness tolerance.

[Learn more](/ai-gateway/concepts/caching)

### Cache Store

<ParamField path="cache-store" type="object">
  Define the cache storage backend. Must be configured at the top level.

<Tabs>
  <Tab title="In-Memory">
  **In-Memory Storage (Default):**
  ```yaml
  cache-store:
    type: "in-memory"
  ```
  **With custom max-size:**
  ```yaml
  cache-store:
    type: "in-memory"
    max-size: 10000  # Number of cached entries
  ```

  **Options:**
  - Default: `268435456` (256MB worth of entries)
  - Small: `1000` (good for development)
  - Medium: `10000` (good for moderate traffic)
  - Large: `536870912` (512MB worth for high traffic)
  </Tab>

  <Tab title="Redis">
  **Redis Storage:**
  ```yaml
  cache-store:
    type: "redis"
  ```

  **With custom host-url:**
  ```yaml
  cache-store:
    type: "redis"
    host-url: "redis://localhost:6379"
  ```
  **Options:**
  - `host-url`: Redis connection string (required)
  - Supports standard Redis URL format: `redis://[username:password@]host[:port][/database]`
  </Tab>
</Tabs>


</ParamField>

<ParamField path="routers.[name].cache" type="object" optional>
  Configure response caching for a router.

  ```yaml
  cache-store:
    type: "in-memory"

  routers:
    production:
      cache:
        directive: "max-age=3600, max-stale=1800"
        buckets: 10
        seed: "unique-cache-seed"
  ```
</ParamField>

<ParamField path="routers.[name].cache.directive" type="string">
  HTTP cache-control directive string.

  ```yaml
  cache:
    directive: "max-age=3600, max-stale=1800"
  ```

  **How it works:** Defines cache freshness (`max-age`) and staleness tolerance (`max-stale`) in seconds for all requests. Optionally override with `cache-control` request headers.
</ParamField>

<ParamField path="routers.[name].cache.buckets" type="number">
  Number of responses stored per cache key before random selection begins.

  ```yaml
  cache:
    buckets: 10
  ```

  **How it works:** Stores n number of different responses for identical requests, then randomly selects from the stored responses to add variability.
</ParamField>

<ParamField path="routers.[name].cache.seed" type="string">
  Unique seed for cache key generation.

  ```yaml
  cache:
    seed: "unique-cache-seed"
  ```

  **How it works:** Creates isolated cache namespaces - different seeds maintain separate cache spaces for the same requests.
</ParamField>

## Rate Limiting

Control request frequency using GCRA (Generic Cell Rate Algorithm) with burst capacity and smooth rate limiting. Global limits are checked first, then router-specific limits are applied.

<Warning>
  **Authentication Required**: Rate limiting works per-API-key, so you must enable Helicone authentication for rate limiting to function properly. [Set up authentication](/ai-gateway/authentication) first.
</Warning>

[Learn more](/ai-gateway/concepts/rate-limiting)

### Rate Limit Store

<ParamField path="rate-limit-store" type="object">
  Define the rate limit storage backend. Must be configured at the top level.

  <Tabs>
    <Tab title="In-Memory">
    **In-Memory Storage (Default):**
    ```yaml
    rate-limit-store:
      type: "in-memory"
    ```
    </Tab>

    <Tab title="Redis">
    **Redis Storage:**
    ```yaml
    rate-limit-store:
      type: "redis"
    ```

    **With custom host-url:**
    ```yaml
    rate-limit-store:
      type: "redis"
      host-url: "redis://localhost:6379"
    ```
    **Options:**
    - `host-url`: Redis connection string
      - Supports standard Redis URL format: `redis://[username:password@]host[:port][/database]`
    - `connection-timeout`: Connection timeout in seconds (default: 5)
    </Tab>
  </Tabs>
</ParamField>

### Global Rate Limiting

<ParamField path="global.rate-limit" type="object">
  Configure application-wide rate limits that apply to all requests.

  ```yaml
  global:
    rate-limit:
      per-api-key:
        capacity: 500
        refill-frequency: 1s
  ```

  **How it works:** These limits are checked first for every request across all routers.
</ParamField>

### Router-Level Rate Limiting

<ParamField path="routers.[name].rate-limit" type="object">
  Configure additional rate limiting specific to this router (applied after global limits).

  ```yaml
  routers:
    production:
      rate-limit:
        per-api-key:
          capacity: 100
          refill-frequency: 1m
  ```

  **How it works:** If global limits are configured, they're checked first. Then these router-specific limits are applied as an additional layer.
</ParamField>

<ParamField path="routers.[name].rate-limit.store" type="string" default="in-memory">
  You can choose to override the rate limit set in the `rate-limit-store` configuration.

  ```yaml
  routers:
    production:
      rate-limit:
        store:
          type: in-memory
        per-api-key:
          ...
  ```

  **Options:** Refer to the [Rate Limit Store](#rate-limit-store) section for more information.
</ParamField>

### Rate Limit Configuration Fields

The following fields are available for both global and router-level rate limiting:

<ParamField path="[context].rate-limit.per-api-key" type="object">
  Rate limits applied per API key.

  ```yaml
  per-api-key:
    capacity: 500
    refill-frequency: 1s
  ```
</ParamField>

<ParamField path="[context].rate-limit.per-api-key.capacity" type="integer" default="500">
  Maximum number of requests in the bucket (burst capacity).

  ```yaml
  per-api-key:
    capacity: 1000
  ```

  **How it works:** This is the maximum number of requests that can be made instantly before rate limiting kicks in.
</ParamField>

<ParamField path="[context].rate-limit.per-api-key.refill-frequency" type="duration" default="1s">
  Time to completely refill the capacity bucket.

  ```yaml
  per-api-key:
    refill-frequency: 1s
  ```

  **How it works:** With capacity=500 and refill-frequency=1s, you get 500 requests per second sustained rate.
</ParamField>

<ParamField path="[context].rate-limit.cleanup-interval" type="duration" default="5m">
  How often to clean up expired rate limit entries.

  ```yaml
  cleanup-interval: 5m
  ```

  **Note:** Only available for global rate limiting configuration.
</ParamField>

## Retries

Automatically retry transient errors from AI providers using configurable strategies. Retries use smart failure detection with configurable maximum attempts and backoff timing.

[Learn more](/ai-gateway/concepts/retries)

### Global Retry Configuration

<ParamField path="global.retries" type="object">
  Configure an application-wide retry policy that applies to all requests.

  ```yaml
  global:
    retries:
      strategy: "constant"
      delay: "50ms"
      max-retries: 3
  ```

  **How it works:** If a transient error is encountered on any endpoint, the request will be retried with the given policy.
</ParamField>

### Router-Level Retry Configuration

<ParamField path="routers.[name].retries" type="object">
  Configure retry policy specific to this router (overrides global retry settings).

  ```yaml
  routers:
    production:
      retries:
        strategy: "exponential"
        min-delay: "100ms"
        max-delay: "30s"
        max-retries: 5
        factor: 2.0
  ```

  **How it works:** Router-specific retry settings take precedence over global retry configuration.
</ParamField>

### Retry Strategy Fields

The following fields are available for both global and router-level retry configuration:

<ParamField path="[context].retries.strategy" type="string" required>
  Retry backoff strategy.

  ```yaml
  retries:
    strategy: "constant"
  ```

  **Options:**
  - `constant` - Fixed delay between retry attempts with jitter
  - `exponential` - Exponentially increasing delays with jitter and configurable bounds
</ParamField>

#### Constant Strategy Fields

<ParamField path="[context].retries.delay" type="duration" default="1s">
  Fixed delay between retry attempts (for constant strategy).

  ```yaml
  retries:
    strategy: "constant"
    delay: "50ms"
  ```

  **How it works:** Each retry waits exactly this duration (plus jitter) before attempting again.
</ParamField>

<ParamField path="[context].retries.max-retries" type="integer" default="2">
  Maximum number of retry attempts.

  ```yaml
  retries:
    strategy: "constant"
    max-retries: 3
  ```

  **How it works:** After this many failed attempts, the request fails permanently.
</ParamField>

#### Exponential Strategy Fields

<ParamField path="[context].retries.min-delay" type="duration" default="1s">
  Minimum delay for exponential backoff strategy.

  ```yaml
  retries:
    strategy: "exponential"
    min-delay: "100ms"
  ```

  **How it works:** First retry waits this duration, then each subsequent retry increases exponentially.
</ParamField>

<ParamField path="[context].retries.max-delay" type="duration" default="30s">
  Maximum delay cap for exponential backoff strategy.

  ```yaml
  retries:
    strategy: "exponential"
    max-delay: "60s"
  ```

  **How it works:** Delays will never exceed this value, even with exponential growth.
</ParamField>

<ParamField path="[context].retries.max-retries" type="integer" default="2">
  Maximum number of retry attempts.

  ```yaml
  retries:
    strategy: "exponential"
    max-retries: 5
  ```

  **How it works:** After this many failed attempts, the request fails permanently.
</ParamField>

<ParamField path="[context].retries.factor" type="number" default="2.0">
  Exponential backoff multiplication factor.

  ```yaml
  retries:
    strategy: "exponential"
    factor: 1.5
  ```

  **How it works:** Each retry delay is multiplied by this factor. With factor=2.0: 100ms → 200ms → 400ms → 800ms.
</ParamField>



## Helicone Add-ons

Configure integration with the Helicone platform for authentication and observability.

[Learn more about Authentication](/ai-gateway/authentication) | [Learn more about Observability](/ai-gateway/observability)

<ParamField path="helicone.features" type="string" default="none" optional>
  Enable Helicone features such as LLM observability and authentication for secure API access.

  ```yaml
  helicone:
    # Set to `features: observability` to enable observability and auth
    features: auth
  ```

  Enable authentication (required) and request logging to your Helicone dashboard.

  ```yaml
  helicone:
    features: observability
  ```

  **When enabled:** Must set the `HELICONE_CONTROL_PLANE_API_KEY` environment variable with your Helicone API key when deploying
  the AI Gateway when using Helicone add ons.
</ParamField>

<ParamField path="helicone.base-url" type="string" default="https://api.helicone.ai">
  Helicone API endpoint URL.

  ```yaml
  helicone:
    base-url: "https://api.helicone.ai"
  ```

  **Note:** Only change this if you're self-hosting Helicone. Use the default for Helicone Cloud.
</ParamField>

<ParamField path="helicone.websocket-url" type="string" default="wss://api.helicone.ai/ws/v1/router/control-plane">
  WebSocket URL for control plane connection.

  ```yaml
  helicone:
    websocket-url: "wss://api.helicone.ai/ws/v1/router/control-plane"
  ```

  **Note:** Only change this if you're self-hosting Helicone. Use the default for Helicone Cloud.
</ParamField>

## Provider Configuration

Configure LLM providers, their endpoints, and available models. 

<Note>
The gateway ships with [comprehensive defaults](https://github.com/Helicone/helicone-router/blob/main/llm-proxy/config/embedded/providers.yaml) for all major providers. Most users will not need to configure this section, [this guide](/ai-gateway/concepts/providers) will walk you through when you might need to.
</Note>

<ParamField path="providers" type="object">
  Configure provider settings to override defaults.

  ```yaml
  providers:
    anthropic:
      base-url: "https://api.anthropic.com"
      version: "2023-06-01"
      models:
        - claude-3-5-haiku
    
    ollama:
      base-url: "http://192.168.1.100:11434"
      models:
        - llama3.2
        - deepseek-r1
        - custom-fine-tuned-model
    
    bedrock:
      base-url: "https://bedrock-runtime.us-west-2.amazonaws.com"
      models:
        - anthropic.claude-3-5-sonnet-20241022-v2:0
        - anthropic.claude-3-haiku-20240307-v1:0
  ```
</ParamField>

<ParamField path="providers.[name].base-url" type="string" required>
  API endpoint URL for the provider.

  ```yaml
  providers:
    openai:
      base-url: "https://api.openai.com"
  ```
</ParamField>

<ParamField path="providers.[name].models" type="array" required>
  List of supported models for this provider.

  ```yaml
  providers:
    openai:
      models:
        - gpt-4
        - gpt-4o
        - gpt-4o-mini
  ```
</ParamField>

<ParamField path="providers.[name].version" type="string" optional>
  API version (required for some providers like Anthropic).

  ```yaml
  providers:
    anthropic:
      version: "2023-06-01"
  ```
</ParamField>

## Model Mapping

Define equivalencies between models from different providers for seamless switching and load balancing. 

<Note>
The Gateway ships with [comprehensive defaults](https://github.com/Helicone/helicone-router/blob/main/llm-proxy/config/embedded/model-mapping.yaml) for all major providers. Most users will not need to configure this section, [this guide](/ai-gateway/concepts/model-mapping) will walk you through when you might need to.
</Note>

<ParamField path="routers.[name].model-mappings" type="object" optional>
  Router-specific model mappings for fallback when requested model isn't available.

  ```yaml
  routers:
    production:
      model-mappings:
        gpt-4o: claude-3-opus
        claude-3-5-sonnet: gemini-1.5-pro
        gpt-4o-mini: claude-3-5-sonnet
  ```
</ParamField>

<ParamField path="default-model-mapping" type="object">
  Global fallback mappings used when router-specific mappings aren't defined.

  ```yaml
  default-model-mapping:
    gpt-4o: claude-3-opus
    gpt-4o-mini: claude-3-5-sonnet
    claude-3-5-sonnet: gemini-1.5-pro
  ```
</ParamField>

## Telemetry

Configure OpenTelemetry monitoring for the AI Gateway application's health and performance.

<Note>
  Monitor the AI Gateway's health and performance with OpenTelemetry. We provide Docker Compose for local testing and Grafana dashboard configs for production. 
  
  [Learn more](/ai-gateway/debugging/telemetry)
</Note>

<ParamField path="telemetry.level" type="string" default="info">
  Logging level in env logger format.

  ```yaml
  telemetry:
    level: "info"
  ```

  **Common patterns:**
  - `"info"` - General information for all modules, recommended for production
  - `"info,ai_gateway=debug"` - Debug for dependencies, info for gateway, recommended for development
</ParamField>

<ParamField path="telemetry.exporter" type="string" default="stdout">
  Telemetry data export destination.

  ```yaml
  telemetry:
    exporter: "otlp"
  ```

  **Options:**
  - `stdout` - Export telemetry data to standard output (default)
  - `otlp` - Export telemetry data to OTLP collector endpoint
  - `both` - Export to both stdout and OTLP collector
</ParamField>

<ParamField path="telemetry.otlp-endpoint" type="string" default="http://localhost:4317/v1/metrics">
  OTLP collector endpoint URL.

  ```yaml
  telemetry:
    otlp-endpoint: "http://localhost:4317"
  ```
</ParamField>

## Response Headers

Control which headers are returned to provide visibility into the gateway's routing decisions and processing.

<ParamField path="response-headers.provider" type="boolean" default="true">
  Add `helicone-provider` header showing which provider handled the request.

  ```yaml
  response-headers:
    provider: true
  ```

  **When enabled:** Responses include header like `helicone-provider: openai` or `helicone-provider: anthropic`.
</ParamField>

<ParamField path="response-headers.provider-request-id" type="boolean" default="true">
  Add `helicone-provider-req-id` header showing the provider's request ID.

  ```yaml
  response-headers:
    provider-request-id: true
  ```

  **When enabled:** Responses include header like `helicone-provider-req-id: req-12345` for request tracing.
</ParamField>

## Health Monitoring

Configure how the AI Gateway monitors provider health and automatically removes failing providers from load balancing rotation.

<ParamField path="discover.monitor.health.type" type="string">
  Health monitoring strategy.

  ```yaml
  discover:
    monitor:
      health:
        ratio: 0.1
        window: 60s
        grace-period:
          min-requests: 20
  ```

  **Options:**
  - `error-ratio` - Monitor based on error rate thresholds (only option currently)
</ParamField>

<ParamField path="discover.monitor.health.ratio" type="number" default="0.1">
  Error ratio threshold (0.0-1.0) that triggers provider removal.

  ```yaml
  discover:
    monitor:
      health:
        ratio: 0.15
  ```

  **How it works:** If errors/requests exceeds this ratio, provider is marked unhealthy and removed from load balancing.
</ParamField>

<ParamField path="discover.monitor.health.window" type="duration" default="60s">
  Time window for measuring error ratios.

  ```yaml
  discover:
    monitor:
      health:
        window: 60s
  ```

  **How it works:** Rolling window size for calculating error rates.
</ParamField>

<ParamField path="discover.monitor.health.grace-period.min-requests" type="integer" default="20">
  Minimum requests required before health monitoring takes effect.

  ```yaml
  discover:
    monitor:
      health:
        grace-period:
          min-requests: 20
  ```

  **How it works:** Providers won't be marked unhealthy until they've handled at least this many requests.
</ParamField>