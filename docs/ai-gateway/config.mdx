---
title: "Configuration Reference"
sidebarTitle: "Configuration"
description: "Complete reference for configuring your LLM Gateway"
---

The AI Gateway is configured through a `ai-gateway-config.yaml` file that defines how requests are routed, load balanced, and processed across different LLM providers.

## Routers

Each Helicone AI Gateway deployment can configure multiple independent routing policies for different use cases. Each router operates with its own load balancing strategy, provider set, and configuration.

<ParamField path="routers" type="object">
  Define one or more routers. Each router name becomes part of the URL path when making requests.

  ```yaml
  routers:
    production:
      load-balance:
        chat:
          strategy: latency
          providers:
            - openai
            - anthropic
    
    experimental:
      load-balance:
        chat:
          strategy: weighted
          providers:
            - provider: anthropic
              weight: '0.9'
            - provider: openai
              weight: '0.1'
  ```

  **Usage:** Set your OpenAI SDK baseURL to `http://localhost:8080/production` or `http://localhost:8080/experimental`
</ParamField>

## Load Balancing

Distribute requests across multiple providers to optimize performance, costs, and reliability. The gateway supports latency-based and weighted strategies for different use cases.

[Learn more](/ai-gateway/features/loadbalancing)

### Latency Strategy

<ParamField path="routers.[name].load-balance.chat.strategy" type="string">
  Use `latency` for automatic load balancing that routes to the provider with the lowest latency.

  ```yaml
  routers:
    production:
      load-balance:
        chat:
          strategy: latency
          providers:
            - openai
            - anthropic
            - google
            - ollama
  ```
</ParamField>

### Weighted Strategy

<ParamField path="routers.[name].load-balance.chat.strategy" type="string">
  Use `weighted` to distribute requests based on specific percentages.

  ```yaml
  routers:
    production:
      load-balance:
        chat:
          strategy: weighted
          providers:
            - provider: anthropic
              weight: '0.95'
            - provider: openai
              weight: '0.05'
  ```

  **Important:** Weights must sum to exactly `1.0`
</ParamField>

### Load Balancing Providers

<ParamField path="routers.[name].load-balance.chat.providers" type="array">
  List of target providers for load balancing.

  **For Latency Strategy:**
  ```yaml
  providers:
    - openai
    - anthropic
    - google
  ```

  **For Weighted Strategy:**
  ```yaml
  providers:
    - provider: anthropic
      weight: '0.7'
    - provider: openai
      weight: '0.3'
  ```
</ParamField>

## Caching

Store and reuse LLM responses for identical requests to dramatically reduce costs and improve response times. Cache directives control response freshness and staleness tolerance.

[Learn more](/ai-gateway/features/cache)

<ParamField path="routers.[name].cache" type="object" optional>
  Configure response caching for a router.

  ```yaml
  routers:
    production:
      cache:
        enabled: true
        directive: "max-age=3600, max-stale=1800"
        buckets: 10
        seed: "unique-cache-seed"
  ```
</ParamField>

<ParamField path="routers.[name].cache.store" type="string" default="in-memory">
  Define where to store cached responses.

  ```yaml
  cache:
    store: in-memory
  ```

  **Options:**
  - `in-memory` - Local memory storage (default)
  - `redis` - Redis storage (coming soon...) 
</ParamField>

<ParamField path="routers.[name].cache.directive" type="string">
  HTTP cache-control directive string.

  ```yaml
  cache:
    directive: "max-age=3600, max-stale=1800"
  ```

  **How it works:** Defines cache freshness (`max-age`) and staleness tolerance (`max-stale`) in seconds for all requests. Optionally override with `cache-control` request headers.
</ParamField>

<ParamField path="routers.[name].cache.buckets" type="number">
  Number of responses stored per cache key before random selection begins.

  ```yaml
  cache:
    buckets: 10
  ```

  **How it works:** Stores n number of different responses for identical requests, then randomly selects from the stored responses to add variability.
</ParamField>

<ParamField path="routers.[name].cache.seed" type="string">
  Unique seed for cache key generation.

  ```yaml
  cache:
    seed: "unique-cache-seed"
  ```

  **How it works:** Creates isolated cache namespaces - different seeds maintain separate cache spaces for the same requests.
</ParamField>

## Rate Limiting

Control request frequency using GCRA (Generic Cell Rate Algorithm) with burst capacity and smooth rate limiting. Global limits are checked first, then router-specific limits are applied.

[Learn more](/ai-gateway/features/rate-limiting)

### Global Rate Limiting

<ParamField path="global.rate-limit" type="object">
  Configure application-wide rate limits that apply to all requests.

  ```yaml
  global:
    rate-limit:
      store: in-memory
      per-api-key:
        capacity: 500
        refill-frequency: 1s
  ```

  **How it works:** These limits are checked first for every request across all routers.
</ParamField>

### Router-Level Rate Limiting

<ParamField path="routers.[name].rate-limit" type="object">
  Configure additional rate limiting specific to this router (applied after global limits).

  ```yaml
  routers:
    production:
      rate-limit:
        per-api-key:
          capacity: 100
          refill-frequency: 1m
  ```

  **How it works:** If global limits are configured, they're checked first. Then these router-specific limits are applied as an additional layer.
</ParamField>

### Rate Limit Configuration Fields

The following fields are available for both global and router-level rate limiting:

<ParamField path="[context].rate-limit.store" type="string" default="in-memory">
  Storage backend for rate limit counters.

  ```yaml
  store: in-memory
  ```

  **Options:**
  - `in-memory` - Local memory storage (default)
  - `redis` - Redis storage (coming soon...) 
</ParamField>

<ParamField path="[context].rate-limit.per-api-key" type="object">
  Rate limits applied per API key.

  ```yaml
  per-api-key:
    capacity: 500
    refill-frequency: 1s
  ```
</ParamField>

<ParamField path="[context].rate-limit.per-api-key.capacity" type="integer" default="500">
  Maximum number of requests in the bucket (burst capacity).

  ```yaml
  per-api-key:
    capacity: 1000
  ```

  **How it works:** This is the maximum number of requests that can be made instantly before rate limiting kicks in.
</ParamField>

<ParamField path="[context].rate-limit.per-api-key.refill-frequency" type="duration" default="1s">
  Time to completely refill the capacity bucket.

  ```yaml
  per-api-key:
    refill-frequency: 1s
  ```

  **How it works:** With capacity=500 and refill-frequency=1s, you get 500 requests per second sustained rate.
</ParamField>

<ParamField path="[context].rate-limit.cleanup-interval" type="duration" default="5m">
  How often to clean up expired rate limit entries.

  ```yaml
  cleanup-interval: 5m
  ```

  **Note:** Only available for global rate limiting configuration.
</ParamField>

## Helicone Observability

Configure integration with the Helicone platform for authentication and/or observability. When enabled, requests must include a valid Helicone API key.

[Learn more](/ai-gateway/features/helicone-integration)

### Authentication

If you plan on enabling authentication, you'll need to register your router with Helicone first.

<ParamField path="helicone-observability.enable" type="boolean" default="false">
  Enable Helicone platform integration and authentication.

  ```yaml
  helicone-observability:
    enable: true
  ```

  **When enabled:** Requests must include a valid Helicone API key, and observability data is automatically sent to Helicone.
</ParamField>

<ParamField path="helicone-observability.base-url" type="string" default="https://api.helicone.ai">
  Helicone API endpoint URL.

  ```yaml
  helicone-observability:
    base-url: "https://api.helicone.ai"
  ```
</ParamField>


## Provider Configuration

Configure LLM providers, their endpoints, and available models. 

<Note>
The gateway ships with [comprehensive defaults](https://github.com/Helicone/helicone-router/blob/main/llm-proxy/config/embedded/providers.yaml) for all major providers. Most users will not need to configure this section, [this guide](/ai-gateway/features/providers) will walk you through when you might need to.
</Note>

<ParamField path="providers" type="object">
  Configure provider settings to override defaults.

  ```yaml
  providers:
    anthropic:
      base-url: "https://api.anthropic.com"
      version: "2023-06-01"
      models:
        - claude-3-5-haiku
    
    ollama:
      base-url: "http://192.168.1.100:11434"
      models:
        - llama3.2
        - deepseek-r1
        - custom-fine-tuned-model
    
    bedrock:
      base-url: "https://bedrock-runtime.us-west-2.amazonaws.com"
      models:
        - anthropic.claude-3-5-sonnet-20241022-v2:0
        - anthropic.claude-3-haiku-20240307-v1:0
  ```
</ParamField>

<ParamField path="providers.[name].base-url" type="string" required>
  API endpoint URL for the provider.

  ```yaml
  providers:
    openai:
      base-url: "https://api.openai.com"
  ```
</ParamField>

<ParamField path="providers.[name].models" type="array" required>
  List of supported models for this provider.

  ```yaml
  providers:
    openai:
      models:
        - gpt-4
        - gpt-4o
        - gpt-4o-mini
  ```
</ParamField>

<ParamField path="providers.[name].version" type="string" optional>
  API version (required for some providers like Anthropic).

  ```yaml
  providers:
    anthropic:
      version: "2023-06-01"
  ```
</ParamField>

## Model Mapping

Define equivalencies between models from different providers for seamless switching and load balancing. 

<Note>
The Gateway ships with [comprehensive defaults](https://github.com/Helicone/helicone-router/blob/main/llm-proxy/config/embedded/model-mapping.yaml) for all major providers. Most users will not need to configure this section, [this guide](/ai-gateway/features/model-mapping) will walk you through when you might need to.
</Note>

<ParamField path="routers.[name].model-mappings" type="object" optional>
  Router-specific model mappings for fallback when requested model isn't available.

  ```yaml
  routers:
    production:
      model-mappings:
        gpt-4o: claude-3-opus
        claude-3-5-sonnet: gemini-1.5-pro
        gpt-4o-mini: claude-3-5-sonnet
  ```
</ParamField>

<ParamField path="default-model-mapping" type="object">
  Global fallback mappings used when router-specific mappings aren't defined.

  ```yaml
  default-model-mapping:
    gpt-4o: claude-3-opus
    gpt-4o-mini: claude-3-5-sonnet
    claude-3-5-sonnet: gemini-1.5-pro
  ```
</ParamField>

## Telemetry

Configure OpenTelemetry monitoring for the AI Gateway application's health and performance.

<Note>
  Monitor the AI Gateway's health and performance with OpenTelemetry. We provide Docker Compose for local testing and Grafana dashboard configs for production. 
  
  [Learn more](/ai-gateway/features/observability)
</Note>

<ParamField path="telemetry.level" type="string" default="info">
  Logging level in env logger format.

  ```yaml
  telemetry:
    level: "info"
  ```

  **Common patterns:**
  - `"info"` - General information for all modules, recommended for production
  - `"info,ai_gateway=debug"` - Debug for dependencies, info for gateway, recommended for development
</ParamField>

<ParamField path="telemetry.exporter" type="string" default="stdout">
  Telemetry data export destination.

  ```yaml
  telemetry:
    exporter: "otlp"
  ```

  **Options:**
  - `stdout` - Export telemetry data to standard output (default)
  - `otlp` - Export telemetry data to OTLP collector endpoint
  - `both` - Export to both stdout and OTLP collector
</ParamField>

<ParamField path="telemetry.otlp-endpoint" type="string" default="http://localhost:4317/v1/metrics">
  OTLP collector endpoint URL.

  ```yaml
  telemetry:
    otlp-endpoint: "http://localhost:4317"
  ```
</ParamField>

## Response Headers

Control which headers are returned to provide visibility into the gateway's routing decisions and processing.

<ParamField path="response-headers.provider" type="boolean" default="true">
  Add `helicone-provider` header showing which provider handled the request.

  ```yaml
  response-headers:
    provider: true
  ```

  **When enabled:** Responses include header like `helicone-provider: openai` or `helicone-provider: anthropic`.
</ParamField>

<ParamField path="response-headers.provider-request-id" type="boolean" default="true">
  Add `helicone-provider-req-id` header showing the provider's request ID.

  ```yaml
  response-headers:
    provider-request-id: true
  ```

  **When enabled:** Responses include header like `helicone-provider-req-id: req-12345` for request tracing.
</ParamField>

## Health Monitoring

Configure how the AI Gateway monitors provider health and automatically removes failing providers from load balancing rotation.

<ParamField path="discover.monitor.health.type" type="string">
  Health monitoring strategy.

  ```yaml
  discover:
    monitor:
      health:
        ratio: 0.1
        window: 60s
        grace-period:
          min-requests: 20
  ```

  **Options:**
  - `error-ratio` - Monitor based on error rate thresholds (only option currently)
</ParamField>

<ParamField path="discover.monitor.health.ratio" type="number" default="0.1">
  Error ratio threshold (0.0-1.0) that triggers provider removal.

  ```yaml
  discover:
    monitor:
      health:
        ratio: 0.15
  ```

  **How it works:** If errors/requests exceeds this ratio, provider is marked unhealthy and removed from load balancing.
</ParamField>

<ParamField path="discover.monitor.health.window" type="duration" default="60s">
  Time window for measuring error ratios.

  ```yaml
  discover:
    monitor:
      health:
        window: 60s
  ```

  **How it works:** Rolling window size for calculating error rates.
</ParamField>

<ParamField path="discover.monitor.health.grace-period.min-requests" type="integer" default="20">
  Minimum requests required before health monitoring takes effect.

  ```yaml
  discover:
    monitor:
      health:
        grace-period:
          min-requests: 20
  ```

  **How it works:** Providers won't be marked unhealthy until they've handled at least this many requests.
</ParamField>