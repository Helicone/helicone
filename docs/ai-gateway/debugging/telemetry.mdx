---
title: "Telemetry"
sidebarTitle: "Telemetry"
description: "Monitor AI Gateway application performance, routing decisions, and system health with OpenTelemetry"
---

The AI Gateway provides comprehensive application monitoring through OpenTelemetry (OTel), enabling deep visibility into how the gateway processes requests, makes routing decisions, and performs under load. With built-in trace propagation and multiple exporter options, you can seamlessly integrate with your existing monitoring stack.

## Getting Started

### Why Monitor the AI Gateway?

AI Gateway application monitoring provides:
- **Routing decision visibility** - See which providers were selected and why
- **Gateway performance tracking** - Monitor request processing latency and throughput  
- **System health insights** - Track application health, errors, and resource usage

<Info>
  **Built-in Instrumentation** - The AI Gateway automatically instruments its own application performance with distributed tracing, metrics collection, and structured logging using OpenTelemetry.
</Info>

<Note>
  For complete configuration options, see the [Configuration Reference](/ai-gateway/config#telemetry).
</Note>

### Telemetry Levels

The recommended telemetry level is `info,ai_gateway=debug` for development and `info` for production.

### Configuration Examples

<Tabs>
  <Tab title="Development - Console Output">
    **Use case:** Local development with console output for immediate debugging.

    ```yaml
    telemetry:
      level: "info,ai_gateway=debug" # Recommended for development
      otlp-endpoint: "http://localhost:4317"
    ```
    
    **Result:** Pretty-printed logs to console with full trace information and no external dependencies.
  </Tab>

  <Tab title="Production - Full Stack">
    **Use case:** Production deployment with complete observability stack integration.

    ```yaml
    telemetry:
      level: "info" # Recommended for production
      otlp-endpoint: "https://otel-collector.yourcompany.com:4317"
    ```
    
    **Result:** OTLP export to collector with structured telemetry data and cross-service correlation.
  </Tab>

  <Tab title="Local Testing - Docker Stack">
    **Use case:** Local testing with the provided Grafana + Prometheus + Tempo stack.

    ```yaml
    telemetry:
      level: "info,ai_gateway=debug"
      otlp-endpoint: "http://localhost:4317"
    ```
    
    **Setup:**
    ```bash
    cd infrastructure
    docker-compose up -d
    # Access Grafana at http://localhost:3010
    ```
  </Tab>
</Tabs>

## Reference

### Grafana Stack (Included)

Use the included Docker Compose setup for complete local observability:

```bash
# Start the full observability stack
cd infrastructure
docker-compose up -d

# Access services
open http://localhost:3010  # Grafana dashboard
open http://localhost:9090  # Prometheus metrics
```

**Included services:**
- **Grafana** (port 3010) - Dashboards and visualization
- **Prometheus** (port 9090) - Metrics storage and querying
- **Loki** (port 3100) - Log aggregation and search
- **OpenTelemetry Collector** (port 4317) - Telemetry ingestion

<Note>
  **Pre-built Dashboard:** The setup includes a [comprehensive Grafana dashboard](https://github.com/Helicone/helicone-router/blob/main/infrastructure/grafana/dashboards/helicone-router.json) for AI Gateway monitoring that you can import into your own Grafana instance.
</Note>

### Custom Monitoring Stack

The AI Gateway can integrate with any OpenTelemetry-compatible monitoring solution. Simply configure the telemetry endpoint in your [configuration file](/ai-gateway/config#telemetry) to point to your existing monitoring infrastructure.