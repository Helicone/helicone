---
title: "AI Gateway Overview"
sidebarTitle: "Overview"
description: "Complete guide to Helicone's AI Gateway - routing, load balancing, and monitoring for 100+ LLM providers"
---

Helicone AI Gateway is a lightweight, Rust-based proxy that provides intelligent routing, load balancing, and monitoring for over 100 LLM providers. Whether you're building AI applications or managing production workloads, the Gateway simplifies LLM operations with enterprise-grade features.

## Getting Started

<CardGroup cols={2}>
<Card title="Introduction" icon="book-open" href="/ai-gateway/introduction">
  Learn about the AI Gateway's features and benefits
</Card>
<Card title="Quickstart" icon="rocket" href="/ai-gateway/quickstart">
  Get started in 1 minute with your first AI Gateway deployment
</Card>
<Card title="Create Your First Router" icon="route" href="/ai-gateway/router-quickstart">
  Build custom routers with load balancing, caching, and multiple environments
</Card>
<Card title="Configuration Reference" icon="gear" href="/ai-gateway/config">
  Complete YAML configuration guide for all Gateway features
</Card>
</CardGroup>

## Integration & Observability

<CardGroup cols={2}>
<Card title="Helicone Integration" icon="chart-line" href="/ai-gateway/helicone-integration">
  Enable authentication and observability with Helicone platform
</Card>
<Card title="Application Health" icon="heart-pulse" href="/ai-gateway/debugging/telemetry">
  Monitor Gateway performance with OpenTelemetry integration
</Card>
</CardGroup>

## Core Concepts

<CardGroup cols={3}>
<Card title="Load Balancing" icon="scale-balanced" href="/ai-gateway/concepts/loadbalancing">
  Distribute requests across providers for optimal performance
</Card>
<Card title="Caching" icon="database" href="/ai-gateway/concepts/caching">
  Store and reuse responses to reduce costs and latency
</Card>
<Card title="Rate Limiting" icon="gauge-high" href="/ai-gateway/concepts/rate-limiting">
  Control request frequency with GCRA algorithm
</Card>
<Card title="Multiple Routers" icon="server" href="/ai-gateway/concepts/multi-router">
  Configure separate routers for different environments and use cases
</Card>
<Card title="Model Mapping" icon="arrows-left-right" href="/ai-gateway/concepts/model-mapping">
  Define equivalencies between models from different providers
</Card>
<Card title="Providers" icon="server-cog" href="/ai-gateway/concepts/providers">
  Configure custom LLM provider endpoints and models
</Card>
</CardGroup>

## Deployment Options

<CardGroup cols={2}>
<Card title="Self-hosted Deployment" icon="server" href="/ai-gateway/self-host">
  Deploy on your own infrastructure with full control
</Card>
<Card title="Deployment Guides" icon="book" href="/ai-gateway/deployment">
  Platform-specific deployment instructions and best practices
</Card>
</CardGroup>

