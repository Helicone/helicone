---
title: "Quickstart"
sidebarTitle: "Quickstart"
description: "Get started with Helicone AI Gateway in 1 minute"
---

<Note>
  Helicone AI Gateway is currently only available as a self-hosted solution. 
  Our cloud-based solution is coming soon.
</Note>

<Steps>
    <Step title="Configure provider secrets">
        In order to deploy the Gateway, you'll need to configure the provider
        secrets for all the providers you wish to use.

        Set your OpenAI API Key as an environment variable, for example
        with a `.env` file.

        ```bash
        export OPENAI_API_KEY=your-api-key
        ```

        Using other providers? Check how to add them [here](/ai-gateway/config/#provider-configuration).
    </Step>
    <Step title="Set up your AI Gateway">
    ```bash
    npx @helicone/ai-gateway
    ```

    The Gateway will be running on `http://localhost:8080` and has
    three routes:
    - `/ai` for a Unified API endpoint compatible with the OpenAI endpoint
    - `/router/{router-name}` for load balancing with multiple providers
    - `/{provider}` for a direct gateway to a provider

    </Step>
    <Step title="Make your first request">
    <CodeGroup>
    ```typescript Typescript
    import { OpenAI } from "openai";

    const openai = new OpenAI({
      baseURL: "http://localhost:8080/ai"
    });

    const response = await openai.chat.completions.create({
      model: "openai/gpt-4o-mini", // 100+ models available
      messages: [{ role: "user", content: "Hello, world!" }],
    });

    console.log(response);
    ```
    ```python Python
    import openai

    openai.api_base = "http://localhost:8080"
    openai.api_key = "fake-key-as-real-key-is-managed-by-gateway"

    response = openai.ChatCompletion.create(
        model="openai/gpt-4o-mini",  # 100+ models available
        messages=[{"role": "user", "content": "Hello, world!"}]
    )
    ```
    ```bash Bash
    curl http://localhost:8080/ai/v1/chat/completions \
      -H "Authorization: Bearer $HELICONE_API_KEY" \
      -H "Content-Type: application/json" \
      -d '{
        "model": "openai/gpt-4o-mini",
        "messages": [
          { "role": "user", "content": "Hello, world!" }
        ]
      }'
    ```
    </CodeGroup>

    You're all set! ðŸŽ‰ 
    
    Your AI Gateway is now ready to handle requests across 100+ AI models!
    </Step>
    <Step title="Optional: Enable Helicone observability">
    Gain detailed tracing and insights into your AI usage directly from your Gateway.
    
    Just add the following environment variables to your Gateway configuration:

    ```bash
    export HELICONE_CONTROL_PLANE_API_KEY=your-api-key
    ```
    </Step>
</Steps>

## Next step:

Great job getting your Gateway started! The next step is making it work exactly how you want.

Interested in adding new providers, balancing request loads, or caching responses for efficiency?

<Card title="Configuration" horizontal icon="gear" href="/ai-gateway/config">
  Dive into our configuration guides to build the perfect Gateway for your use case.
</Card>