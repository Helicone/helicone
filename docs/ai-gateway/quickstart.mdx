---
title: "Quickstart"
sidebarTitle: "Quickstart"
description: "Get started with Helicone AI Gateway in 1 minute"
---

<Note>
  Helicone AI Gateway is currently only available as a self-hosted solution. Our cloud-based solution is coming soon. 
  
  If you're using our existing cloud AI Gateway, find the legacy documentation [here](/features/advanced-usage/cloud-ai-gateway-overview).
</Note>

<Steps>
  <Step title="Configure provider secrets">
    To get started, you'll need to configure the provider secrets for the providers you want to use.

    Just add your API keys to the `.env` file:

    ```bash
    OPENAI_API_KEY=your-api-key
    ANTHROPIC_API_KEY=your-api-key
    GEMINI_API_KEY=your-api-key
    ```

    All supported models and providers are available [here](https://github.com/Helicone/helicone-router/blob/main/ai-gateway/config/embedded/providers.yaml)

    Using unsupported providers or custom endpoints? Check how to configure them [here](/ai-gateway/config/#provider-configuration).
  </Step>
  <Step title="Start the Gateway">
    ```bash
    npx @helicone/ai-gateway
    ```

    The Gateway will be running on `http://localhost:8080` and has three routes:

    - `/ai` for a standard OpenAI-compatible Unified API that works out of the box
    - `/router/{router-name}` for advanced Unified API with custom routing logic and load balancing
    - `/{provider-name}` for direct access to a specific provider without routing
  </Step>

    <Step title="Make your first request">
    Let's start with a simple request to the pre-configured `/ai` route. Don't worry, we'll show you how to create custom routers next!

    <CodeGroup>
    ```typescript Typescript
    import { OpenAI } from "openai";

    const openai = new OpenAI({
      baseURL: "http://localhost:8080/ai",
      apiKey: "fake-api-key", // Required by SDK, but gateway handles real auth
    });

    const response = await openai.chat.completions.create({
      model: "openai/gpt-4o-mini", // 100+ models available
      messages: [{ role: "user", content: "Hello, world!" }],
    });

    console.log(response);
    ```
    ```python Python
    import openai

    openai.api_base = "http://localhost:8080/ai"
    openai.api_key = "fake-api-key" # Required by SDK, but gateway handles real auth

    response = openai.ChatCompletion.create(
        model="openai/gpt-4o-mini",  # 100+ models available
        messages=[{"role": "user", "content": "Hello, world!"}]
    )
    ```
    ```bash cURL
    curl http://localhost:8080/ai/v1/chat/completions \
      -H "Content-Type: application/json" \
      -d '{
        "model": "openai/gpt-4o-mini", # 100+ models available
        "messages": [
          { "role": "user", "content": "Hello, world!" }
        ]
      }'
    ```
    </CodeGroup>

    You're all set! ðŸŽ‰ 
    
    Your AI Gateway is now ready to handle requests across 100+ AI models!
    </Step>
    <Step title="Optional: Enable Helicone observability">
    Gain detailed tracing and insights into your AI usage directly from your Gateway.
    
    Just add the following environment variables to your Gateway configuration:

    ```bash
    export HELICONE_CONTROL_PLANE_API_KEY=your-api-key
    ```
    </Step>
</Steps>

## Next step:

Great job getting your Gateway started! The next step is making it work exactly how you want.

Interested in adding new providers, balancing request loads, or caching responses for efficiency?

<Card title="Router Quickstart" href="/ai-gateway/router-quickstart">
    Build custom routers with load balancing, caching, and multiple environments in 5 minutes
</Card>