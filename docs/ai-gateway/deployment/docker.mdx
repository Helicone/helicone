---
title: "Deploy with Docker"
sidebarTitle: "Docker"
description: "Deploy Helicone AI Gateway using Docker in 2 minutes"
---

Deploy the AI Gateway using Docker for easy containerized deployment to any cloud provider or local environment.

## Quick Start

<Steps>
  <Step title="Configure environment">
    Create a `.env` file with your provider API keys. You can use our template as a starting point:

    ```bash
    # Download the template
    curl -o .env https://raw.githubusercontent.com/Helicone/ai-gateway/main/.env.template
    ```

    Then edit the `.env` file with your actual API keys:

    ```bash
    # Required for server binding
    AI_GATEWAY__SERVER__ADDRESS=0.0.0.0

    # Add your provider API keys
    OPENAI_API_KEY=sk-your-openai-key
    ANTHROPIC_API_KEY=sk-ant-your-anthropic-key
    # Add other provider keys as needed
    ```

    <Note>
      The `AI_GATEWAY__SERVER__ADDRESS=0.0.0.0` is required for Docker to work properly.
    </Note>
  </Step>

  <Step title="Run the container">
    Start the AI Gateway container:

    ```bash
    docker run -it --rm --name ai-gateway -p 8080:8080 --env-file .env helicone/ai-gateway:main
    ```

    The Gateway will be running on `http://localhost:8080` with these routes:

    - `/ai` - Unified API that works out of the box
    - `/router/{router-name}` - Custom routing with load balancing  
    - `/{provider-name}` - Direct provider access
  </Step>

  <Step title="Test your deployment">
    Make a test request to verify everything works:

    <CodeGroup>
    ```typescript TypeScript
    import { OpenAI } from "openai";

    const openai = new OpenAI({
      baseURL: "http://localhost:8080/ai",
      apiKey: "placeholder-key", // Gateway handles API keys
    });

    const response = await openai.chat.completions.create({
      model: "openai/gpt-4o-mini",
      messages: [{ role: "user", content: "Hello from Docker!" }],
    });

    console.log(response);
    ```
    ```python Python
    import openai

    client = openai.OpenAI(
        base_url="http://localhost:8080/ai",
        api_key="placeholder-key"  # Gateway handles API keys
    )

    response = client.chat.completions.create(
        model="openai/gpt-4o-mini",
        messages=[{"role": "user", "content": "Hello from Docker!"}]
    )

    print(response)
    ```
    ```bash cURL
    curl http://localhost:8080/ai/v1/chat/completions \
      -H "Content-Type: application/json" \
      -d '{
        "model": "openai/gpt-4o-mini",
        "messages": [
          { "role": "user", "content": "Hello from Docker!" }
        ]
      }'
    ```
    </CodeGroup>

    You should see a response from the AI model! ðŸŽ‰
  </Step>
</Steps>

## Using a Configuration File

For custom routing and advanced features, create a `config.yaml` file:

<Steps>
  <Step title="Create config file">
    Create a `config.yaml` file with your routing configuration:

    ```yaml
    routers:
      production:
        load-balance:
          chat:
            strategy: latency
            providers:
              - openai
              - anthropic
    ```
  </Step>

  <Step title="Mount config and run">
    Run the container with your configuration file:

    ```bash
    docker run -it --rm --name ai-gateway -p 8080:8080 --env-file .env \
      -v $(pwd)/config.yaml:/helicone-config/config.yaml \
      helicone/ai-gateway:main ai-gateway -c /helicone-config/config.yaml
    ```
  </Step>

  <Step title="Test your router">
    Test your custom router:

    <CodeGroup>
    ```typescript TypeScript
    import { OpenAI } from "openai";

    const openai = new OpenAI({
      baseURL: "http://localhost:8080/router/production",
      apiKey: "placeholder-key", // Gateway handles API keys
    });

    const response = await openai.chat.completions.create({
      model: "openai/gpt-4o-mini",
      messages: [{ role: "user", content: "Hello from my custom router!" }],
    });

    console.log(response);
    ```
    ```python Python
    import openai

    client = openai.OpenAI(
        base_url="http://localhost:8080/router/production",
        api_key="placeholder-key"  # Gateway handles API keys
    )

    response = client.chat.completions.create(
        model="openai/gpt-4o-mini",
        messages=[{"role": "user", "content": "Hello from my custom router!"}]
    )

    print(response)
    ```
    ```bash cURL
    curl http://localhost:8080/router/production/v1/chat/completions \
      -H "Content-Type: application/json" \
      -d '{
        "model": "openai/gpt-4o-mini",
        "messages": [
          { "role": "user", "content": "Hello from my custom router!" }
        ]
      }'
    ```
    </CodeGroup>
  </Step>
</Steps>

## Cloud Deployment Options

The containerized AI Gateway can be deployed to various cloud platforms:

<CardGroup cols={2}>
  <Card title="AWS ECS" href="/ai-gateway/deployment/aws">
    One-click deployment to AWS ECS with Terraform
  </Card>
  <Card title="Fly.io" href="/ai-gateway/deployment/fly">
    Simple container deployment platform
  </Card>
  <Card title="Porter" href="/ai-gateway/deployment/porter">
    Container registry and app platform
  </Card>
    <Card title="Kubernetes" href="/ai-gateway/deployment/kubernetes">
    Deploy using Helm charts for production workloads
  </Card>
</CardGroup>