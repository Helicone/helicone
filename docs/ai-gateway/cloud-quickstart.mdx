---
title: "Cloud Quickstart"
sidebarTitle: "Cloud Quickstart"
description: "Get started with Helicone's managed AI Gateway in 30 seconds"
---

Get instant access to intelligent routing for 100+ LLM providers with zero infrastructure setup. Our cloud-hosted AI Gateway handles scaling, monitoring, and reliability automatically.

<Steps>
  <Step title="Sign up for Helicone">
    Create your free account at [helicone.ai](https://helicone.ai).
  </Step>

  <Step title="Add your LLM Provider keys to Helicone">
    Go to the [Provider](https://us.helicone.ai/providers) page in Helicone to add your provider API keys
    for all the providers you wish to use.
  </Step>

  <Step title="Create a Helicone API Key">
    Go to the [API Key](https://us.helicone.ai/settings/api-keys) page in Helicone to create a Helicone API
    key.

    You will be required to include a valid Helicone API Key as an authorization header in every request.
  </Step>

  <Step title="Configure your AI client">
    Simply point your existing AI SDK to the Helicone gateway's unified API endpoint. No other changes needed!

    <CodeGroup>
    ```typescript TypeScript
    import { OpenAI } from "openai";

    const openai = new OpenAI({
      baseURL: "https://ai-gateway.helicone.ai/ai",
      apiKey: process.env.HELICONE_API_KEY,
    });

    const response = await openai.chat.completions.create({
      model: "openai/gpt-4o-mini",
      messages: [{ role: "user", content: "Hello, world!" }],
    });
    ```

    ```python Python
    from openai import OpenAI

    client = OpenAI(
        base_url="https://ai-gateway.helicone.ai/ai",
        api_key=os.getenv("HELICONE_API_KEY")
    )

    response = client.chat.completions.create(
        model="openai/gpt-4o-mini",
        messages=[{"role": "user", "content": "Hello, world!"}]
    )
    ```

    ```bash cURL
    curl https://ai-gateway.helicone.ai/ai/chat/completions \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer $HELICONE_API_KEY" \
      -d '{
        "model": "openai/gpt-4o-mini",
        "messages": [
          { "role": "user", "content": "Hello, world!" }
        ]
      }'
    ```
    </CodeGroup>

    <Tip>
      **Works with any model!** Simply prefix the model with the provider name.
      You may find the full list of supported providers [here](https://github.com/Helicone/helicone-router/blob/main/llm-proxy/config/embedded/providers.yaml)
    </Tip>
  </Step>

  <Step title="View your requests">
    Head to your [Helicone dashboard](https://helicone.ai/dashboard) to see:
    - Real-time request logs
    - Cost tracking across all providers
    - Latency metrics and performance insights
    - Token usage analytics
    
    Everything is automatically tracked - no additional setup needed!
  </Step>
</Steps>

## Next Steps

<CardGroup cols={2}>
<Card title="Routers in depth" icon="book" href="/ai-gateway/concepts/routers">
  Learn about routers, load balancing, and advanced features
</Card>
<Card title="Configuration Reference" icon="code" href="/ai-gateway/config">
  Detailed configuration options and API documentation
</Card>
</CardGroup>
