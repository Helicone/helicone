---
title: "Self-Hosted Quickstart"
sidebarTitle: "Self-Hosted Quickstart"
description: "Deploy Helicone AI Gateway on your own infrastructure"
---

<Info>
  **Looking for the managed solution?** Our cloud-hosted AI Gateway gets you started in 30 seconds with zero infrastructure. [Try Cloud â†’](/ai-gateway/cloud-quickstart)
</Info>

<Accordion title="Still using our legacy cloud AI Gateway?">
      If you're using our existing cloud AI Gateway, find the legacy documentation [here](/features/advanced-usage/cloud-ai-gateway-overview).
</Accordion>

<Steps>
  <Step title="Configure provider secrets">
    To get started, you'll need to configure the provider secrets for the providers you want to use.

    Set up your .env file with your PROVIDER_API_KEYs:

    ```bash
    OPENAI_API_KEY=your_openai_key
    ANTHROPIC_API_KEY=your_anthropic_key
    ```

    <Accordion title="Need more provider options?">
    - **All supported providers:** View the complete list [here](https://github.com/Helicone/helicone-router/blob/main/ai-gateway/config/embedded/providers.yaml)
    - **Custom providers:** Configure unsupported providers or custom endpoints [here](/ai-gateway/config/#provider-configuration)
  </Accordion>
  </Step>
  <Step title="Start the Gateway">
    ```bash
    npx @helicone/ai-gateway@latest
    ```

    The Gateway will be running on `http://localhost:8080` and has three routes:

    - `/ai` for a standard OpenAI-compatible Unified API that works out of the box
    - `/router/{router-id}` for advanced Unified API with custom routing logic and load balancing
    - `/{provider-name}` for direct access to a specific provider without routing
  </Step>

    <Step title="Make your first request">
    Let's start with a simple request to the pre-configured `/ai` route. Don't worry, we'll show you how to create custom routers next!

    <CodeGroup>
    ```typescript Typescript
    import { OpenAI } from "openai";

    const openai = new OpenAI({
      baseURL: "http://localhost:8080/ai",
      apiKey: "fake-api-key", // Required by SDK, but gateway handles real auth
    });

    const response = await openai.chat.completions.create({
      model: "openai/gpt-4o-mini", // 100+ models available
      messages: [{ role: "user", content: "Hello, world!" }],
    });

    console.log(response);
    ```
    ```python Python
    import openai

    openai.api_base = "http://localhost:8080/ai"
    openai.api_key = "fake-api-key" # Required by SDK, but gateway handles real auth

    response = openai.chat.completions.create(
        model="openai/gpt-4o-mini",  # 100+ models available
        messages=[{"role": "user", "content": "Hello, world!"}]
    )
    ```
    ```bash cURL
    curl http://localhost:8080/ai/chat/completions \
      -H "Content-Type: application/json" \
      -d '{
        "model": "openai/gpt-4o-mini",
        "messages": [
          { "role": "user", "content": "Hello, world!" }
        ]
      }'
    ```
    </CodeGroup>

    <Tip>
      **Try switching models!** Simply change the model parameter to `"anthropic/claude-3-5-sonnet"` to use Anthropic instead of OpenAI. Same API, different provider - that's the power of the unified interface!
    </Tip>

    You're all set! ðŸŽ‰ 
    
    Your AI Gateway is now ready to handle requests across 100+ AI models!
    </Step>
    <Step title="Optional: Enable Helicone observability">
    Gain detailed tracing and insights into your AI usage directly from your Gateway.
    
    Just add the following environment variables to your Gateway configuration:

    ```bash
    export HELICONE_CONTROL_PLANE_API_KEY=your-api-key
    ```
    </Step>
</Steps>

## Next steps:

Great job getting your self-hosted Gateway running! Here are some important next steps:

<CardGroup cols={2}>
<Card title="Secure Your Gateway" icon="shield" href="/ai-gateway/self-hosted/authentication" horizontal>
    Set up authentication to protect your provider API keys
</Card>
<Card title="Deploy to Production" icon="server" href="/ai-gateway/self-hosted/deployment/overview" horizontal>
    Platform-specific guides for Docker, Kubernetes, AWS, and more
</Card>
<Card title="Advanced Configuration" icon="gear" href="/ai-gateway/config" horizontal>
    Configure routers, load balancing, caching, and rate limiting
</Card>
<Card title="Enable Observability" icon="chart-line" href="/ai-gateway/self-hosted/observability" horizontal>
    Track usage, costs, and performance with Helicone integration
</Card>
</CardGroup>