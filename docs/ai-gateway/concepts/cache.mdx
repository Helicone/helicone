---
title: "Response Caching"
sidebarTitle: "Caching"
description: "Intelligent LLM response caching to reduce costs and improve latency"
---

The AI Gateway automatically caches LLM responses and reuses them for identical requests, reducing costs by up to 95% and improving response times.

Caching uses exact parameter matching with configurable TTL, staleness policies, and bucketed responses for variety.

**Benefits:**
- **Eliminate CI/test costs** by reusing responses across test runs and development
- **Reduce costs** by eliminating duplicate API calls to providers
- **Improve latency** by serving cached responses instantly
- **Handle high traffic** by reducing load on upstream providers
- **Cross-provider efficiency** by reusing responses across different providers

## Quick Start

<Steps>
  <Step title="Create your configuration">
    Create `ai-gateway-config.yaml` with basic caching (1-hour TTL with 30-minute stale allowance):

    ```yaml
    cache-store:
      type: "in-memory"

    global:
      cache:
        directive: "max-age=3600, max-stale=1800"
        buckets: 1
    ```
  </Step>

  <Step title="Start the gateway">
    ```bash
    npx @helicone/ai-gateway@latest --config ai-gateway-config.yaml
    ```
  </Step>

  <Step title="Test caching">
    Send this request twice and see the second request return instantly from cache with `helicone-cache: HIT` header!

    ```bash
    curl -X POST http://localhost:8080/ai/chat/completions \
      -H "Content-Type: application/json" \
      -d '{
        "model": "openai/gpt-4o-mini",
        "messages": [{"role": "user", "content": "Hello!"}]
      }'
    ```
    
    ✅ The second request returns instantly from cache with `helicone-cache: HIT` header!
  </Step>
</Steps>

<Note>
  For complete configuration options and syntax, see the [Configuration Reference](/ai-gateway/config#caching).
</Note>

## Cache Options

<AccordionGroup>
  <Accordion title="Multiple Responses (Buckets)" icon="bucket">
    **Store multiple responses for the same cache key**
    
    Instead of storing one response per cache key, store multiple responses to provide variety for non-deterministic use cases while still benefiting from caching.
    
    **Best for:** Creative applications where response variety is desired
    
    **Example:**
    ```yaml
    cache-store:
      type: "in-memory"

    global:
      cache:
        buckets: 10  # Store up to 10 different responses
        directive: "max-age=3600"
    ```
  </Accordion>

  <Accordion title="Cache Namespacing (Seeds)" icon="key">
    **Partition cache by seed for multi-tenant isolation**
    
    Each cache entry lives in a namespace derived from a **seed**. You can set the seed once in the router config or override it per-request with the `Helicone-Cache-Seed` header.
    
    **Best for:** SaaS apps and multi-tenant systems that need user-level isolation
    
    **How it works:**
    1. Router config can set a default `seed` value
    2. Incoming requests may override the seed via header
    3. The seed is prefixed to the cache key, creating an isolated namespace
    
    **Example (router config):**
    ```yaml
    cache-store:
      type: "in-memory"

    global:
      cache:
        directive: "max-age=3600, max-stale=1800"
        seed: "tenant-alpha"
    ```
    
    **Example (per-request header):**
    ```bash
    curl -H "Helicone-Cache-Seed: user-123" ...
    ```
  </Accordion>

  <Accordion title="Ignore Keys in Cache Key Generation" icon="filter">
    **Exclude specific JSON fields from cache key generation**
    
    When generating cache keys, you can exclude specific fields from the request body that shouldn't affect caching. This is useful for fields that change between requests but don't affect the response, such as timestamps or request IDs.
    
    **Best for:** Applications with dynamic metadata that shouldn't invalidate cache
    
    **How it works:**
    1. Specify keys to ignore via the `Helicone-Cache-Ignore-Keys` header
    2. These keys are removed from the request body before generating the cache key
    3. The original request with all fields is still sent to the provider
    
    **Example:**
    ```bash
    curl -X POST http://localhost:8080/ai/chat/completions \
      -H "Content-Type: application/json" \
      -H "Helicone-Cache-Ignore-Keys: request_id,timestamp" \
      -d '{
        "model": "openai/gpt-4o-mini",
        "messages": [{"role": "user", "content": "Hello!"}],
        "request_id": "unique-123",
        "timestamp": "2024-01-01T00:00:00Z"
      }'
    ```
    
    In this example, the `request_id` and `timestamp` fields are ignored when generating the cache key, so requests with different values for these fields will still hit the same cache entry.
  </Accordion>
</AccordionGroup>

## Use Cases

<Tabs>
  <Tab title="CI/Test Pipeline - Eliminate Costs">
    **Use case:** CI pipeline or test suite that makes repeated identical requests. Cache for the duration of the test run to eliminate all provider costs.

    ```yaml
    cache-store:
      type: "in-memory"

    global:
      cache:
        directive: "max-age=7200, max-stale=3600"  # 2-hour TTL for test runs
        buckets: 1   # Consistent responses for tests
    ```
  </Tab>

  <Tab title="Production API - Cost Optimization">
    **Use case:** Production API that needs to minimize provider costs while maintaining response freshness for users.

    ```yaml
    cache-store:
      type: "in-memory"

    global:
      cache:
        directive: "max-age=1800, max-stale=900"  # 30-min TTL
        buckets: 1   # Consistent responses
    ```
  </Tab>

  <Tab title="Multi-Environment Setup">
    **Use case:** Different environments with different caching strategies - production optimized for freshness, development for cost savings.

    <Note>
      If using router cache configuration, we suggest not using the global cache configuration due to the merging behavior confusion.
    </Note>

    ```yaml
    cache-store:
      type: "in-memory"

    routers:
      production:
        cache:
          directive: "max-age=1800"  # 30-min TTL for production freshness
          buckets: 1                 # Consistent responses
        load-balance:
          chat:
            strategy: latency
            providers: [openai, anthropic]
      
      development:
        cache:
          directive: "max-age=7200"  # 2-hour TTL to reduce dev costs
          buckets: 5                 # More variety for testing
        load-balance:
          chat:
            strategy: latency
            providers: [openai, anthropic]
    ```
  </Tab>
</Tabs>

## How It Works

**Exact Parameter Matching**

All caching uses exact parameter matching—identical requests (model, messages, temperature, all parameters) return cached responses instantly. Request parameters are hashed to create a unique cache key.

### Request Flow

<Steps>
  <Step title="Request Arrives">
    A request comes in with specific parameters (model, messages, temperature, etc.)
  </Step>
  
  <Step title="Configuration Merge">
    Cache settings are merged in precedence order:
    
    - **Request headers**: Highest priority (can override everything)
    - **Router configuration**: Middle priority
    - **Global configuration**: Lowest priority (fallback defaults)
  </Step>
  
  <Step title="Cache Key Generation">
    Request parameters are hashed to create a unique cache key, optionally prefixed with seed for namespacing
  </Step>
  
  <Step title="Cache Lookup">
    System checks the cache store for an existing response that matches the key and isn't expired
  </Step>
  
  <Step title="Cache Hit or Miss">
    - **Hit**: Returns cached response instantly with `helicone-cache: HIT` header
    - **Miss**: Forwards request to provider, caches response, returns with `helicone-cache: MISS` header
  </Step>
</Steps>

### Configuration Scope

Cache settings are applied in precedence order (highest to lowest priority):

| Level | Description | When Applied |
|-------|-------------|--------------|
| **Request Headers** | Per-request cache control via headers | Overrides all other settings |
| **Router Configuration** | Per-router cache policies | Overrides global defaults |
| **Global Configuration** | Application-wide cache defaults | Used as fallback |

### Available Headers

Control caching behavior per-request with these headers:

- `Helicone-Cache-Enabled: true/false` - Enable/disable caching
- `Cache-Control: "max-age=3600"` - Override cache directive
- `Helicone-Cache-Seed: "custom-seed"` - Set cache namespace
- `Helicone-Cache-Bucket-Max-Size: 5` - Override bucket size
- `Helicone-Cache-Ignore-Keys: "key1,key2"` - Ignore specific JSON keys when generating cache keys

### Cache Response Headers

When caching is enabled, the gateway adds response headers to indicate cache status:

- `helicone-cache: HIT/MISS` - Whether response was served from cache
- `helicone-cache-bucket-idx: 2` - Index of cache bucket used (0-based)

## Storage Backend Options

Cache responses can be stored in different backends depending on your deployment needs:

<AccordionGroup>
  <Accordion title="In-Memory Storage" icon="memory">
    **Local cache storage (Default)**
    
    Cache responses are stored locally in each router instance—no external dependencies, ultra-fast lookup.
    
    ```yaml
    cache-store:
      type: "in-memory" # Uses default max-size

    global:
      cache:
        directive: "max-age=3600"
    ```
    
    **Best for:**
    - Single-instance deployments
    - Development environments
    - High-performance scenarios
  </Accordion>

  <Accordion title="Redis" icon="database">
    **Redis cache storage**
    
    Cache responses are stored in Redis, enabling cache sharing across multiple router instances and persistence across restarts.
    
    ```yaml
    cache-store:
      type: "redis"
      host-url: "redis://localhost:6379"

    global:
      cache:
        directive: "max-age=3600"
    ```
    
    **Best for:**
    - Distributed cache sharing across multiple router instances
    - Cache persistence across restarts
  </Accordion>
</AccordionGroup>

<Note>
  Database storage for caching is coming soon.
</Note>

### Strategy Selection Guide

| Use Case                    | Recommended Approach               |
| --------------------------- | ---------------------------------- |
| **Production APIs**         | 1-hour TTL, buckets 1-3            |
| **Development/Testing**     | 24-hour TTL, buckets 5-10          |
| **Creative applications**   | 30-min TTL, buckets 10+            |
| **High-traffic systems**    | Short TTL (≤2 h), buckets 3-5      |
| **User-specific caching**   | Seeds for namespace isolation      |
| **Single instance**         | In-memory storage                  |
| **Multiple instances** | Redis storage |

<Note>
  For complete configuration options and syntax, see the [Configuration Reference](/ai-gateway/config#caching).
</Note>

## Coming Soon

The following caching features are planned for future releases:

| Feature | Description | Version |
|---------|-------------|---------|
| **Database Storage** | Persistent cache storage with advanced analytics and compliance features | v1 |
