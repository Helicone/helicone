---
title: "Response Caching"
sidebarTitle: "Caching"
description: "Intelligent LLM response caching to reduce costs and improve latency"
---

The AI Gateway automatically caches LLM responses and reuses them for identical requests, reducing costs by up to 95% and improving response times.

Caching uses exact parameter matching with configurable TTL, staleness policies, and bucketed responses for variety.

**Benefits:**
- **Eliminate CI/test costs** by reusing responses across test runs and development
- **Reduce costs** by eliminating duplicate API calls to providers
- **Improve latency** by serving cached responses instantly
- **Handle high traffic** by reducing load on upstream providers
- **Cross-provider efficiency** by reusing responses across different providers

## Quick Start

<Steps>
  <Step title="Create your configuration">
    Create `ai-gateway-config.yaml` with basic caching (1-hour TTL with 30-minute stale allowance):

    ```yaml
    cache-store:
      in-memory: {}

    global:
      cache:
        directive: "max-age=3600, max-stale=1800"
        buckets: 1
    ```
  </Step>

  <Step title="Start the gateway">
    ```bash
    npx @helicone/ai-gateway@latest --config ai-gateway-config.yaml
    ```
  </Step>

  <Step title="Test caching">
    Send this request twice and see the second request return instantly from cache with `helicone-cache: HIT` header!

    ```bash
    curl -X POST http://localhost:8080/ai/v1/chat/completions \
      -H "Content-Type: application/json" \
      -d '{
        "model": "openai/gpt-4o-mini",
        "messages": [{"role": "user", "content": "Hello!"}]
      }'
    ```
    
    ✅ The second request returns instantly from cache with `helicone-cache: HIT` header!
  </Step>
</Steps>

<Note>
  For complete configuration options and syntax, see the [Configuration Reference](/ai-gateway/config#caching).
</Note>

## Cache Options

<AccordionGroup>
  <Accordion title="Multiple Responses (Buckets)" icon="bucket">
    **Store multiple responses for the same cache key**
    
    Instead of storing one response per cache key, store multiple responses to provide variety for non-deterministic use cases while still benefiting from caching.
    
    **Best for:** Creative applications where response variety is desired
    
    **Example:**
    ```yaml
    cache-store:
      in-memory: {}

    global:
      cache:
        buckets: 10  # Store up to 10 different responses
        directive: "max-age=3600"
    ```
  </Accordion>

  <Accordion title="Cache Namespacing (Seeds)" icon="key">
    **Partition cache by seed for multi-tenant isolation**
    
    Each cache entry lives in a namespace derived from a **seed**. You can set the seed once in the router config or override it per-request with the `Helicone-Cache-Seed` header.
    
    **Best for:** SaaS apps and multi-tenant systems that need user-level isolation
    
    **How it works:**
    1. Router config can set a default `seed` value
    2. Incoming requests may override the seed via header
    3. The seed is prefixed to the cache key, creating an isolated namespace
    
    **Example (router config):**
    ```yaml
    cache-store:
      in-memory: {}

    global:
      cache:
        directive: "max-age=3600, max-stale=1800"
        seed: "tenant-alpha"
    ```
    
    **Example (per-request header):**
    ```bash
    curl -H "Helicone-Cache-Seed: user-123" ...
    ```
  </Accordion>
</AccordionGroup>

## Use Cases

<Tabs>
  <Tab title="CI/Test Pipeline - Eliminate Costs">
    **Use case:** CI pipeline or test suite that makes repeated identical requests. Cache for the duration of the test run to eliminate all provider costs.

    ```yaml
    cache-store:
      in-memory: {}

    global:
      cache:
        directive: "max-age=7200, max-stale=3600"  # 2-hour TTL for test runs
        buckets: 1   # Consistent responses for tests
    ```
  </Tab>

  <Tab title="Production API - Cost Optimization">
    **Use case:** Production API that needs to minimize provider costs while maintaining response freshness for users.

    ```yaml
    cache-store:
      in-memory: {}

    global:
      cache:
        directive: "max-age=1800, max-stale=900"  # 30-min TTL
        buckets: 1   # Consistent responses
    ```
  </Tab>

  <Tab title="Multi-Environment Setup">
    **Use case:** Different environments with different caching strategies - production optimized for freshness, development for cost savings.

    <Note>
      If using router cache configuration, we suggest not using the global cache configuration due to the merging behavior confusion.
    </Note>

    ```yaml
    cache-store:
      in-memory: {}

    routers:
      production:
        cache:
          directive: "max-age=1800"  # 30-min TTL for production freshness
          buckets: 1                 # Consistent responses
        load-balance:
          chat:
            strategy: latency
            providers: [openai, anthropic]
      
      development:
        cache:
          directive: "max-age=7200"  # 2-hour TTL to reduce dev costs
          buckets: 5                 # More variety for testing
        load-balance:
          chat:
            strategy: latency
            providers: [openai, anthropic]
    ```
  </Tab>
</Tabs>

## How It Works

**Exact Parameter Matching**

All caching uses exact parameter matching—identical requests (model, messages, temperature, all parameters) return cached responses instantly. Request parameters are hashed to create a unique cache key.

### Request Flow

<Steps>
  <Step title="Request Arrives">
    A request comes in with specific parameters (model, messages, temperature, etc.)
  </Step>
  
  <Step title="Configuration Merge">
    Cache settings are merged in precedence order:
    
    - **Request headers**: Highest priority (can override everything)
    - **Router configuration**: Middle priority
    - **Global configuration**: Lowest priority (fallback defaults)
  </Step>
  
  <Step title="Cache Key Generation">
    Request parameters are hashed to create a unique cache key, optionally prefixed with seed for namespacing
  </Step>
  
  <Step title="Cache Lookup">
    System checks the cache store for an existing response that matches the key and isn't expired
  </Step>
  
  <Step title="Cache Hit or Miss">
    - **Hit**: Returns cached response instantly with `helicone-cache: HIT` header
    - **Miss**: Forwards request to provider, caches response, returns with `helicone-cache: MISS` header
  </Step>
</Steps>

### Configuration Scope

Cache settings are applied in precedence order (highest to lowest priority):

| Level | Description | When Applied |
|-------|-------------|--------------|
| **Request Headers** | Per-request cache control via headers | Overrides all other settings |
| **Router Configuration** | Per-router cache policies | Overrides global defaults |
| **Global Configuration** | Application-wide cache defaults | Used as fallback |

### Available Headers

Control caching behavior per-request with these headers:

- `Helicone-Cache-Enabled: true/false` - Enable/disable caching
- `Cache-Control: "max-age=3600"` - Override cache directive
- `Helicone-Cache-Seed: "custom-seed"` - Set cache namespace
- `Helicone-Cache-Bucket-Max-Size: 5` - Override bucket size

### Cache Response Headers

When caching is enabled, the gateway adds response headers to indicate cache status:

- `helicone-cache: HIT/MISS` - Whether response was served from cache
- `helicone-cache-bucket-idx: 2` - Index of cache bucket used (0-based)

## Storage Backend Options

Cache responses can be stored in different backends depending on your deployment needs:

<AccordionGroup>
  <Accordion title="In-Memory Storage" icon="memory">
    **Local cache storage (Default and Only Option)**
    
    Cache responses are stored locally in each router instance—no external dependencies, ultra-fast lookup.
    
    ```yaml
    cache-store:
      in-memory: {}  # Uses default max-size

    global:
      cache:
        directive: "max-age=3600"
    ```
    
    **Best for:**
    - Single-instance deployments
    - Development environments
    - High-performance scenarios
  </Accordion>
</AccordionGroup>

<Note>
  In-memory storage is currently the only available option. Additional storage backends (Redis and Database) are [coming soon](#coming-soon) for distributed caching and advanced analytics.
</Note>

### Strategy Selection Guide

| Use Case                    | Recommended Approach               |
| --------------------------- | ---------------------------------- |
| **Production APIs**         | 1-hour TTL, buckets 1-3            |
| **Development/Testing**     | 24-hour TTL, buckets 5-10          |
| **Creative applications**   | 30-min TTL, buckets 10+            |
| **High-traffic systems**    | Short TTL (≤2 h), buckets 3-5      |
| **User-specific caching**   | Seeds for namespace isolation      |
| **Single instance**         | In-memory storage                  |

<Note>
  For complete configuration options and syntax, see the [Configuration Reference](/ai-gateway/config#caching).
</Note>

## Coming Soon

The following caching features are planned for future releases:

| Feature | Description | Version |
|---------|-------------|---------|
| **Redis Storage** | Distributed cache sharing across multiple router instances with persistence across restarts | v1 |
| **Database Storage** | Persistent cache storage with advanced analytics and compliance features | v1 |
