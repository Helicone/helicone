---
title: "Model Mapping"
sidebarTitle: "Model Mapping"
description: "Intelligent model equivalencies for seamless switching and load balancing"
---

The AI Gateway includes intelligent model mapping that automatically handles equivalencies between models from different providers. This enables seamless switching, load balancing, and fallback scenarios without changing your application code.

**Most users never need to configure model mappings manually** - the gateway ships with comprehensive defaults that work out-of-the-box.

## Getting Started

### What is Model Mapping?

Model mapping defines equivalencies between models from different providers, enabling the gateway to automatically substitute similar models when:
- **Load balancing** across providers with different model names
- **Provider fallback** when your preferred provider is unavailable  
- **Cost optimization** by routing to cheaper equivalent models
- **Performance tuning** by selecting faster equivalent models

The gateway includes [comprehensive defaults](https://github.com/Helicone/helicone-router/blob/main/llm-proxy/config/embedded/model-mapping.yaml) with intelligent mappings between all major providers.

<Note>
  For complete configuration options and syntax, see the [Configuration Reference](/ai-gateway/config#model-mapping).
</Note>

### How Model Mapping Works

The gateway uses a **3-tier mapping system** with automatic fallback. When load balancing routes a request to a different provider, the system checks for model equivalencies in this order:

<Steps>
  <Step title="Direct Support">
    **First, check if the target provider offers the requested model directly.**
    
    If you request `gpt-4o` and load balancing routes to OpenAI, no mapping needed - OpenAI supports `gpt-4o` natively.
    
    ```yaml
    # No mapping needed - direct support
    User requests: gpt-4o → Load balancer routes to OpenAI → Uses gpt-4o
    ```
  </Step>
  
  <Step title="Router-Specific Mapping">
    **Next, check for custom mappings defined in your specific router configuration.**
    
    These override both direct support and global defaults, giving you fine-grained control per router.
    
    ```yaml
    routers:
      production:
        model-mappings:
          gpt-4o: claude-3-opus    # Custom mapping for this router
    
    # Result: gpt-4o request → Routes to Anthropic → Uses claude-3-opus
    ```
  </Step>
  
  <Step title="Global Default Mapping">
    **Check organization-wide default mappings you've configured.**
    
    These apply to all routers unless overridden by router-specific mappings.
    
    ```yaml
    default-model-mapping:
      gpt-4o: claude-3-5-sonnet   # Global preference
    
    # Result: gpt-4o request → Routes to Anthropic → Uses claude-3-5-sonnet
    ```
  </Step>
  
  <Step title="Built-in Intelligent Mapping">
    **Finally, fall back to the gateway's built-in intelligent mappings.**
    
    These are maintained automatically and updated as new models are released.
    
    ```yaml
    # Built-in mapping (from embedded configuration)
    gpt-4o: [claude-3-5-sonnet, gemini-1.5-pro]
    
    # Result: gpt-4o request → Routes to Anthropic → Uses claude-3-5-sonnet
    ```
  </Step>
</Steps>

### Configuration Examples

<Note>
  The gateway's default mappings prioritize equivalent quality and capability. You only need custom mappings when you disagree with these equivalencies for business reasons.
</Note>

<Tabs>
  <Tab title="Better Equivalents - Your Preference">
    **Use case:** You believe different models are more equivalent than the defaults suggest.

    ```yaml
    routers:
      custom-equivalents:
        model-mappings:
          # Default maps gpt-4o to claude-3-5-sonnet
          # But you think claude-3-opus is more equivalent for your use case
          gpt-4o: claude-3-opus
          
          # Default maps claude-3-5-sonnet to gpt-4o
          # But you think gemini-1.5-pro is more equivalent
          claude-3-5-sonnet: gemini-1.5-pro
        
        load-balance:
          chat:
            strategy: latency
            providers:
              - openai
              - anthropic
              - gemini
    ```

    **What happens:** When load balancer routes a `gpt-4o` request to Anthropic, it uses `claude-3-opus` instead of the default `claude-3-5-sonnet` mapping.
  </Tab>

  <Tab title="Avoid Specific Models">
    **Use case:** Override defaults to avoid specific models you don't want to use, even when they're the logical equivalent.

    ```yaml
    routers:
      production:
        model-mappings:
          # Default maps gpt-4o-mini to claude-3-5-haiku
          # Override because you want to avoid haiku entirely
          gpt-4o-mini: claude-3-5-sonnet
          
          # Default maps gemini-1.5-flash to claude-3-5-haiku  
          # Override to avoid haiku for all requests
          gemini-1.5-flash: claude-3-5-sonnet
        
        load-balance:
          chat:
            strategy: latency
            providers:
              - openai
              - anthropic
              - gemini
    ```

    **What happens:** Even though `claude-3-5-haiku` is the logical equivalent, all requests get mapped to `claude-3-5-sonnet` when routed to Anthropic.
  </Tab>

  <Tab title="Global Overrides - Organization-Wide">
    **Use case:** Set organization-wide model preferences that apply to all routers by default.

    ```yaml
    default-model-mapping:
      # Override built-in defaults for your organization's preferences
      gpt-4o: claude-3-opus              # Your org prefers opus over sonnet
      gpt-4o-mini: claude-3-5-sonnet     # Avoid haiku globally
      claude-3-5-sonnet: gemini-1.5-pro  # Prefer Google for certain tasks

    routers:
      production:
        load-balance:
          chat:
            strategy: latency
            providers:
              - openai
              - anthropic
              - gemini
      
      development:
        # Inherits the global default mappings above
        load-balance:
          chat:
            strategy: weighted
            providers:
              - provider: anthropic
                weight: '0.8'
              - provider: gemini
                weight: '0.2'
    ```

    **What happens:** All routers use these mappings unless they define their own router-specific overrides.
  </Tab>
</Tabs>

## Reference

<Note>
  For complete configuration options and syntax, see the [Configuration Reference](/ai-gateway/config#model-mapping).
</Note>

### Multiple Fallback Options

Define multiple fallback options with automatic selection:

```yaml
default-model-mapping:
  gpt-4o:
    - claude-3-5-sonnet    # First choice
    - gemini-1.5-pro       # Second choice
    - gpt-4                # Final fallback
```

**How it works:** The gateway selects the first available model from the list that the target provider supports.