---
title: "Load Balancing Strategies"
sidebarTitle: "Load Balancing"
description: "Intelligent request routing across providers with latency-based P2C and weighted algorithms"
---

The AI Gateway automatically distributes requests across multiple providers using sophisticated algorithms that consider latency, provider health, and your custom preferences. 

All strategies are **rate-limit aware** and **health-monitored**—unhealthy providers are automatically removed and re-added when they recover. This is enabled by default and [configurable](#health-monitoring).

## Getting Started

### Why Use Load Balancing?

Load balancing helps you:
- **Optimize latency** by routing to the fastest available providers
- **Improve reliability** with automatic failover when providers fail
- **Handle rate limits** by temporarily removing rate-limited providers
- **Control traffic distribution** with custom weights for cost optimization
- **Enable gradual rollouts** and A/B testing across providers

<Note>
  For complete configuration options and syntax, see the [Configuration Reference](/ai-gateway/config#load-balancing).
</Note>

### Available Strategies

<AccordionGroup>
  <Accordion title="Latency-based (P2C + PeakEWMA) - Default" icon="bolt">
    **Power-of-Two-Choices with Peak Exponentially Weighted Moving Average**
    
    Maintains a moving average of each provider's RTT latency, weighted by the number of outstanding requests, to distribute traffic to providers with the least load and optimize for latency.
    
    **Best for:** Production workloads where latency matters most
    
    **How it works:**
    1. Randomly selects 2 providers from the healthy pool
    2. Calculates load using RTT weighted by outstanding requests  
    3. Routes to the provider with lower load score
    4. Updates moving averages with actual response times
    
    **Example:**
    ```yaml
    routers:
      default:
        load-balance:
          chat:
            strategy: latency
            providers:
              - openai
              - anthropic
              - google
    ```
  </Accordion>

  <Accordion title="Weighted Strategy" icon="chart-pie">
    **Custom traffic percentages across providers**
    
    Routes traffic based on arbitrary weights you specify. For example, if you have providers [A, B, C] with weights [0.80, 0.15, 0.05], then A gets 80% of traffic, B gets 15%, and C gets 5%.
    
    **Best for:** Cost optimization, gradual provider migrations, or compliance requirements
    
    **Example:**
    ```yaml
    routers:
      default:
        load-balance:
          chat:
            strategy: weighted
            providers:
              - provider: anthropic
                weight: 0.75
              - provider: openai
                weight: 0.25
    ```
    
    <Callout type="warning">
      Weights must sum to exactly 1.0, or the AI Gateway will reject the configuration.
    </Callout>
  </Accordion>

  <Accordion title="Model-Level Weighted Strategy" icon="bullseye">
    **Provider + model specific weighting** *(Coming in v1)*
    
    Same as weighted strategy over providers, except configurable for provider+model pairs. E.g., [openai/o1, bedrock/claude-3-5-sonnet] with weights [0.90, 0.10].
    
    **Best for:** Fine-grained control over specific model routing
  </Accordion>

  <Accordion title="Cost-Optimized Strategy" icon="dollar-sign">
    **Route to the cheapest equivalent model** *(Coming in v2)*
    
    For a given model, picks the provider that offers that same model or any allowed configured equivalent models for the lowest price.

    Rate limit aware algorithms allow consumption of token budgets with the cheapest provider, automatically removing them when
    limits are hit allowing requests to smoothly flow to the next cheapest provider.
    
    **Best for:** Cost-sensitive workloads where minor latency differences are acceptable
  </Accordion>

  <Accordion title="Tag-based Routing" icon="tag">
    **Header-driven routing decisions** *(Coming in v3)*
    
    Route requests to specific providers and models based on tags passed via request headers.

    TODO: @Cole, actually i think this could be solved with the unified
    api endpoint that doesn't do load balancing. the difference to the user
    is whether they modify the header vs the `model` field in the request body.
    wdyt? should we build this? and pls help me update the right places in the docs that
    talk about this use case
    
    **Best for:** testing, user-specific routing, compliance requirements
  </Accordion>

</AccordionGroup>

### Configuration Examples

<Tabs>
  <Tab title="Production API - Optimize for Speed">
    **Use case:** Customer-facing API where response time is critical. The gateway automatically routes to whichever provider is responding fastest, ensuring optimal user experience.

    ```yaml
    routers:
      production:
        load-balance:
          chat:
            strategy: latency
            providers:
              - openai
              - anthropic
              - google
    ```
  </Tab>

  <Tab title="Cost Optimization - Budget Control">
    **Use case:** Internal tools or batch processing where cost matters more than speed. Route 80% of traffic to the more affordable provider while keeping some capacity on premium providers for quality comparison.

    ```yaml
    routers:
      cost-optimized:
        load-balance:
          chat:
            strategy: weighted
            providers:
              - provider: openai    # o3: $2/$8 per 1M tokens
                weight: 0.80
              - provider: google    # gemini-2.5-pro: $5/$15 per 1M tokens
                weight: 0.20
    ```
  </Tab>

  <Tab title="A/B Testing New Provider">
    **Use case:** Testing a new provider's quality and performance with 10% of traffic before committing to larger rollout. Monitor metrics to compare providers safely.

    ```yaml
    routers:
      experimental:
        load-balance:
          chat:
            strategy: weighted
            providers:
              - provider: openai       # Current stable provider
                weight: 0.90
              - provider: google       # Testing new provider
                weight: 0.10
    ```
  </Tab>

  <Tab title="Safe Provider Migration">
    **Use case:** Gradual migration from OpenAI to Anthropic. Start at 30/70, monitor for issues, then adjust weights weekly until fully migrated. Allows instant rollback if problems occur.

    ```yaml
    routers:
      migration:
        load-balance:
          chat:
            strategy: weighted
            providers:
              - provider: anthropic    # Migrating TO this provider
                weight: 0.70
              - provider: openai       # Migrating FROM this provider
                weight: 0.30
    ```
  </Tab>

  <Tab title="Multi-Environment Setup">
    **Use case:** Development uses free local Ollama models to reduce costs during testing, while production uses cloud providers with latency optimization for real users.

    ```yaml
    routers:
      development:
        load-balance:
          chat:
            strategy: weighted
            providers:
              - provider: ollama       # Local models for dev
                weight: 1.0
      
      production:
        load-balance:
          chat:
            strategy: latency
            providers:
              - openai
              - anthropic
    ```
  </Tab>
</Tabs>

## Reference

### Load Balancing Levels

The AI Gateway supports load balancing at multiple levels of granularity:

| Level                   | Availability | Description                                                                                                                 | Example                                          |
| ----------------------- | ------------ | --------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------ |
| **Providers**           | v0 (live!)           | Balance across different AI companies                                                                                       | OpenAI vs Anthropic vs Google Gemini             |
| **Deployments/Regions** | v0 (live!)           | Balance across regions for providers that support them                                                                     | `us-east-1` vs `us-west-2` vs `eu-west-1`        |
| **Models**              | v1 (coming soon!)     | Balance over (provider, model) pairs. | `openai/gpt-4o` vs `anthropic/claude-3-5-sonnet` |

<Callout type="info">
  Currently, the AI Gateway load balances at the **provider level**. Model-level and regional load balancing are planned for future releases.
</Callout>

### Choosing the Right Strategy

| Use Case                 | Recommended Strategy       | Availability |
| ------------------------ | -------------------------- | ------------ |
| **Production APIs**      | Latency-based (P2C)        | v0           |
| **Cost optimization**    | Weighted → Cost-optimized  | v0 → v2      |
| **Provider migration**   | Weighted                   | v0           |
| **A/B testing**          | Weighted → Tag-based       | v0 → v3      |
| **Fine-grained control** | Model-level weighted       | v2           |
| **Compliance routing**   | [Multiple gateway instances](#compliance-based-routing)                  | v3           |

## Advanced Configuration

### Health Monitoring

All load balancing strategies automatically handle provider failures through intelligent health monitoring:

<CardGroup>
  <Card title="Error rate monitoring">
    Providers with high error rates (default: >10%) are automatically removed
  </Card>
  <Card title="Rate limit detection">
    Rate-limited providers are temporarily removed and re-added when limits reset
  </Card>
  <Card title="Grace period handling">  
    Providers need minimum requests (default: 20) before being considered for removal
  </Card>
  <Card title="Automatic recovery">
    Unhealthy providers are periodically retested and re-added when healthy
  </Card>
</CardGroup>

<Note>
  The AI Gateway monitors provider health every 5 seconds by default. The health check uses a rolling 60-second window with configurable error thresholds.
</Note>

To customize health monitoring behavior, see the [Configuration Reference](/ai-gateway/config#health-monitoring).

### Compliance-Based Routing

For compliance requirements, deploy **multiple router instances** rather than complex routing logic. This provides better isolation, security, and auditability.

#### Common Scenarios

<Tabs>
  <Tab title="Data Sovereignty - GDPR">
    **Use case:** European data must stay in Europe.

    ```bash
    router-eu.company.com   → EU-only providers
    router-us.company.com   → Global providers
    ```

    ```yaml
    routers:
      eu-compliant:
        load-balance:
          chat:
            strategy: latency
            providers: [anthropic-eu, openai-eu]
    ```
  </Tab>

  <Tab title="Healthcare - HIPAA">
    **Use case:** Patient data requires HIPAA-compliant providers.

    ```bash
    hipaa-router.company.com → BAA-covered providers only
    router.company.com       → All providers
    ```

    ```yaml
    routers:
      hipaa-compliant:
        load-balance:
          chat:
            strategy: weighted
            providers:
              - provider: anthropic-baa
                weight: 1.0
    
    helicone:
      enable: true  # Enhanced audit logging
    ```
  </Tab>

  <Tab title="Government/Defense">
    **Use case:** Different security clearance levels.

    ```bash
    unclass-router.company.com → Commercial providers
    restricted-router.local    → On-premise only (air-gapped)
    ```

    ```yaml
    routers:
      restricted:
        load-balance:
          chat:
            strategy: weighted
            providers:
              - provider: ollama
                weight: 1.0
    ```
  </Tab>
</Tabs>

**Benefits:** Separate networks, authentication, audit trails, and certification scope per deployment.
