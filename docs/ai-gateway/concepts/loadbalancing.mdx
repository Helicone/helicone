---
title: "Load Balancing Strategies"
sidebarTitle: "Load Balancing"
description: "Intelligent request routing across providers with latency-based P2C and weighted algorithms"
---

The AI Gateway automatically distributes requests across multiple providers using sophisticated algorithms that consider latency, provider health, and your custom preferences. 

All strategies are **rate-limit aware** and **health-monitored**—unhealthy providers are automatically removed and re-added when they recover.

**Benefits:**
- **Optimize latency** by routing to the fastest available providers
- **Improve reliability** with automatic failover when providers fail
- **Handle rate limits** by temporarily removing rate-limited providers
- **Control traffic distribution** with custom weights for cost optimization
- **Enable gradual rollouts** and A/B testing across providers

## Quick Start

<Steps>
  <Step title="Create your configuration">
    Create `ai-gateway-config.yaml` with latency-based routing (automatically picks the fastest provider):

    ```yaml
    routers:
      default:
        load-balance:
          chat:
            strategy: latency
            providers:
              - openai
              - anthropic
              - gemini
    ```
  </Step>

  <Step title="Ensure your provider API keys are set">
    ```bash
    export OPENAI_API_KEY=sk-your-openai-key
    export ANTHROPIC_API_KEY=sk-ant-your-anthropic-key
    export GEMINI_API_KEY=your-gemini-key
    ```
  </Step>

  <Step title="Start the gateway">
    ```bash
    npx @helicone/ai-gateway --config ai-gateway-config.yaml
    ```
  </Step>

  <Step title="Test load balancing">
    ```bash
    curl -X POST http://localhost:8080/router/default/v1/chat/completions \
      -H "Content-Type: application/json" \
      -d '{
        "model": "gpt-4o-mini",
        "messages": [{"role": "user", "content": "Hello!"}]
      }'
    ```
    
    ✅ The gateway automatically routes to the fastest available provider!
  </Step>
</Steps>

## Available Strategies

<AccordionGroup>
  <Accordion title="Latency-based (P2C + PeakEWMA) - Default" icon="bolt">
    **Power-of-Two-Choices with Peak Exponentially Weighted Moving Average**
    
    Maintains a moving average of each provider's RTT latency, weighted by the number of outstanding requests, to distribute traffic to providers with the least load and optimize for latency.
    
    **Best for:** Production workloads where latency matters most
    
    **How it works:**
    1. Randomly selects 2 providers from the healthy pool
    2. Calculates load using RTT weighted by outstanding requests  
    3. Routes to the provider with lower load score
    4. Updates moving averages with actual response times
    
    **Example:**
    ```yaml
    routers:
      default:
        load-balance:
          chat:
            strategy: latency
            providers:
              - openai
              - anthropic
              - gemini
    ```
  </Accordion>

  <Accordion title="Weighted Strategy" icon="chart-pie">
    **Custom traffic percentages across providers**
    
    Routes traffic based on arbitrary weights you specify. For example, if you have providers [A, B, C] with weights [0.80, 0.15, 0.05], then A gets 80% of traffic, B gets 15%, and C gets 5%.
    
    **Best for:** Cost optimization, gradual provider migrations, or compliance requirements
    
    **Example:**
    ```yaml
    routers:
      default:
        load-balance:
          chat:
            strategy: weighted
            providers:
              - provider: anthropic
                weight: 0.75
              - provider: openai
                weight: 0.25
    ```
    
    <Callout type="warning">
      Weights must sum to exactly 1.0, or the AI Gateway will reject the configuration.
    </Callout>
  </Accordion>


  </AccordionGroup>

<Note>
  Additional load balancing strategies (Cost-Optimized, Model-Level Weighted, Tag-based Routing) are [coming soon](#coming-soon) for advanced routing scenarios.
</Note>

## Use Cases

<Tabs>
  <Tab title="Production API - Optimize for Speed">
    **Use case:** Customer-facing API where response time is critical. The gateway automatically routes to whichever provider is responding fastest, ensuring optimal user experience.

    ```yaml
    routers:
      production:
        load-balance:
          chat:
            strategy: latency
            providers:
              - openai
              - anthropic
              - gemini
    ```
  </Tab>
  
  <Tab title="A/B Testing New Provider">
    **Use case:** Testing a new provider's quality and performance with 10% of traffic before committing to larger rollout. Monitor metrics to compare providers safely.

    ```yaml
    routers:
      production:
        load-balance:
          chat:
            strategy: weighted
            providers:
              - provider: openai       # Current stable provider
                weight: 0.90
              - provider: gemini       # Testing new provider
                weight: 0.10
    ```
  </Tab>

  <Tab title="Safe Provider Migration">
    **Use case:** Gradual migration from OpenAI to Anthropic. Start at 30/70, monitor for issues, then adjust weights weekly until fully migrated. Allows instant rollback if problems occur.

    ```yaml
    routers:
      production:
        load-balance:
          chat:
            strategy: weighted
            providers:
              - provider: anthropic    # Migrating TO this provider
                weight: 0.70
              - provider: openai       # Migrating FROM this provider
                weight: 0.30
    ```
  </Tab>

  <Tab title="Multi-Environment Setup">
    **Use case:** Development uses free local Ollama models to reduce costs during testing, while production uses cloud providers with latency optimization for real users.

    ```yaml
    routers:
      development:
        load-balance:
          chat:
            strategy: weighted
            providers:
              - provider: ollama       # Local models for dev
                weight: 1.0
      
      production:
        load-balance:
          chat:
            strategy: latency
            providers:
              - openai
              - anthropic
    ```
  </Tab>
</Tabs>

## How It Works

### Request Flow

<Steps>
  <Step title="Request Arrives">
    A request comes in for a specific model (e.g., `gpt-4o-mini`)
  </Step>
  
  <Step title="Provider Selection">
    The load balancer identifies which providers can handle this model and applies your chosen strategy:
    
    - **Latency strategy**: Picks 2 random healthy providers, routes to the one with lower load
    - **Weighted strategy**: Routes based on your configured percentages
  </Step>
  
  <Step title="Health Check">
    Before routing, the gateway ensures the selected provider is healthy (not rate-limited, not failing)
  </Step>
  
  <Step title="Request Forwarded">
    The request is sent to the selected provider with the original model name
  </Step>
  
  <Step title="Response & Learning">
    The response is returned to you, and the gateway updates its latency/health metrics for future routing decisions
  </Step>
</Steps>

### What Gets Load Balanced

The AI Gateway distributes requests across **providers** (OpenAI, Anthropic, Google, etc.) that support the requested model.

**Example**: When you request `gpt-4o-mini`:
- ✅ **OpenAI** - Native support for `gpt-4o-mini`
- ✅ **Anthropic** - Via model mapping to `claude-3-5-haiku` 
- ✅ **Ollama** - Via model mapping to `llama3.2`

**Example**: When you request a model not in any mappings:
- ✅ **OpenAI** - If OpenAI natively supports it
- ❌ **Anthropic** - No mapping available
- ❌ **Ollama** - No mapping available

The load balancer only considers providers that can actually handle your request.

### Automatic Health Monitoring

All load balancing strategies automatically handle provider failures through intelligent health monitoring:

<CardGroup>
  <Card title="Error rate monitoring">
    Providers with high error rates (default: >10%) are automatically removed
  </Card>
  <Card title="Rate limit detection">
    Rate-limited providers are temporarily removed and re-added when limits reset
  </Card>
  <Card title="Grace period handling">  
    Providers need minimum requests (default: 20) before being considered for removal
  </Card>
  <Card title="Automatic recovery">
    Unhealthy providers are periodically retested and re-added when healthy
  </Card>
</CardGroup>

<Note>
  The AI Gateway monitors provider health every 5 seconds by default. The health check uses a rolling 60-second window with configurable error thresholds.
</Note>

### Strategy Selection Guide

| Use Case                 | Recommended Strategy       | 
| ------------------------ | -------------------------- |
| **Production APIs**      | Latency-based - Automatically optimizes for speed |
| **Provider migration**   | Weighted - Gradual traffic shifting with instant rollback |
| **A/B testing**          | Weighted - Controlled traffic splits for comparison |
| **Cost optimization**    | Weighted - Route more traffic to cheaper providers |
| **Compliance routing**   | [Multiple AI Gateways](#compliance-based-routing) - Better isolation |

## Compliance-Based Routing

For compliance requirements, deploy **multiple AI Gateway instances** rather than complex routing logic. This provides better isolation, security, and auditability.

### Common Scenarios

<Tabs>
  <Tab title="Data Sovereignty - GDPR">
    **Use case:** European data must stay in Europe.

    ```bash
    router-eu.company.com   → EU-only providers
    router-us.company.com   → Global providers
    ```

    ```yaml
    routers:
      eu-compliant:
        load-balance:
          chat:
            strategy: latency
            providers: [anthropic-eu, openai-eu]
    ```
  </Tab>

  <Tab title="Healthcare - HIPAA">
    **Use case:** Patient data requires HIPAA-compliant providers.

    ```bash
    hipaa-router.company.com → BAA-covered providers only
    router.company.com       → All providers
    ```

    ```yaml
    routers:
      hipaa-compliant:
        load-balance:
          chat:
            strategy: weighted
            providers:
              - provider: anthropic-baa
                weight: 1.0
    
    helicone:
      enable: true  # Enhanced audit logging
    ```
  </Tab>

  <Tab title="Government/Defense">
    **Use case:** Different security clearance levels.

    ```bash
    unclass-router.company.com → Commercial providers
    restricted-router.local    → On-premise only (air-gapped)
    ```

    ```yaml
    routers:
      restricted:
        load-balance:
          chat:
            strategy: weighted
            providers:
              - provider: ollama
                weight: 1.0
    ```
  </Tab>
</Tabs>

**Benefits:** Separate networks, authentication, audit trails, and certification scope per deployment.

<Note>
  For complete configuration options and syntax, see the [Configuration Reference](/ai-gateway/config#load-balancing).
</Note>

## Coming Soon

The following load balancing features are planned for future releases:

| Feature | Description | Version |
|---------|-------------|---------|
| **Cost-Optimized Strategy** | Route to the cheapest equivalent model - picks the provider that offers the same model or configured equivalent models for the lowest price | v2 |
| **Model-Level Weighted Strategy** | Provider + model specific weighting - configure weights for provider+model pairs (e.g., openai/o1 vs bedrock/claude-3-5-sonnet) | v2 |
| **Tag-based Routing** | Header-driven routing decisions - route requests to specific providers and models based on tags passed via request headers | v3 |
