---
title: "Rate Limiting & Spend Controls"
sidebarTitle: "Rate Limiting"
description: "GCRA-based rate limiting with burst capacity and smooth request throttling"
---

The AI Gateway provides flexible rate limiting using GCRA (Generic Cell Rate Algorithm) to help you manage request frequency and prevent abuse. Configure limits globally or per-router with burst capacity and smooth rate limiting.

Rate limiting uses **[configurable storage backends](#storage-backend-options)** and is applied with configurable burst capacity and sustained rates.

## Getting Started

### Why Use Rate Limiting?

Rate limiting helps you:
- **Prevent abuse** by limiting request rates per API key
- **Manage costs** by controlling request frequency
- **Ensure stability** by preventing traffic spikes from overwhelming your system
- **Fair usage** by distributing capacity across different API keys
- **Control your own traffic** based on your business requirements

<Info>
  **Provider rate limits are handled automatically** by the load balancing system. This rate limiting feature is for controlling your own API traffic based on your business requirements.
</Info>

<Note>
  For complete configuration options and syntax, see the [Configuration Reference](/ai-gateway/config#rate-limiting).
</Note>

### Rate Limiting Strategy

**Requests Per Time Period (GCRA Algorithm)**

The AI Gateway uses burst-aware rate limiting with smooth sustained rates. Control request frequency using capacity (burst allowance) and refill frequency (sustained rate). Uses GCRA (Generic Cell Rate Algorithm) for smooth token bucket behavior that's more sophisticated than simple sliding window approaches.

<Note>
  Additional rate limiting features are [coming soon](#coming-soon) for more granular control.
</Note>

#### How it works
1. Each API key gets a virtual token bucket with specified capacity
2. Requests consume tokens from the bucket
3. Tokens refill at the specified rate (refill-frequency / capacity)
4. Requests are allowed if tokens are available, rejected otherwise

**Example:**
```yaml
global:
  rate-limit:
    per-api-key:
      capacity: 500
      refill-frequency: 1s  # 500 requests per second sustained
```

### Configuration Examples

<Tabs>
  <Tab title="Production API - Abuse Prevention">
    **Use case:** Production API that needs to prevent abuse while allowing reasonable burst traffic for legitimate users.

    ```yaml
    global:
      rate-limit:
        store: in-memory
        per-api-key:
          capacity: 1000
          refill-frequency: 1m  # 1000 requests per minute
        cleanup-interval: 5m
    ```
  </Tab>

  <Tab title="Multiple Environments - Different Rate Limits">
    **Use case:** Different environments with different rate limiting strategies - production allowing higher throughput, development with conservative limits for cost safety.

    ```yaml
    routers:
      production:
        rate-limit:
          per-api-key:
            capacity: 1000
            refill-frequency: 1m  # 1000 requests per minute
        load-balance:
          chat:
            strategy: latency
            providers:
              - openai
              - anthropic
      
      development:
        rate-limit:
          per-api-key:
            capacity: 100
            refill-frequency: 1h  # 100 requests per hour for cost safety
        load-balance:
          chat:
            strategy: latency
            providers:
              - openai
              - anthropic
    ```
  </Tab>

  <Tab title="Multi-Tier Service - Different Router Limits">
    **Use case:** Different service tiers with varying rate limits. Premium router gets higher limits than basic router.

    ```yaml
    global:
      rate-limit:
        store: in-memory
        cleanup-interval: 5m
    
    routers:
      premium:
        rate-limit:
          per-api-key:
            capacity: 5000
            refill-frequency: 1m  # 5000 requests per minute
      
      basic:
        rate-limit:
          per-api-key:
            capacity: 100
            refill-frequency: 1m  # 100 requests per minute
    ```
  </Tab>
</Tabs>

<Note>
  For complete configuration options and syntax, see the [Configuration Reference](/ai-gateway/config#rate-limiting).
</Note>

## Reference

### Rate Limiting Levels

**Per-API-Key Rate Limiting**

The AI Gateway currently supports rate limiting at the API key level, where limits are applied to each API key individually. This helps prevent API key abuse and ensures fair usage across different keys.

<Note>
  Additional rate limiting levels (Per-End-User, Per-Team, Per-Team-Member) are [coming soon](#coming-soon) for more granular control.
</Note>

#### Configuration Scope

Rate limits are checked in precedence order when a request comes in:

<Steps>
  <Step title="Global Rate Limits">
    **Application-wide limits**
    
    Applied to all requests across all routers. These limits are checked first and act as a safety net for your entire system.
  </Step>
  
  <Step title="Router-Specific Rate Limits">
    **Individual router limits**
    
    Applied after global limits pass. Each router can have custom limits or opt out of rate limiting entirely.
  </Step>
</Steps>

### Storage Backend Options

Rate limiting counters can be stored in different backends depending on your deployment needs:

<AccordionGroup>
  <Accordion title="In-Memory Storage" icon="memory">
    **Local memory storage**
    
    Rate limiting state is stored locally in each router instance. Fast and simple, but limits are not shared across multiple instances.
    
    ```yaml
    global:
      rate-limit:
        store: in-memory
        cleanup-interval: 5m
    ```
    
    **Best for:**
    - Single instance deployments
    - Development environments
    - High-performance scenarios where cross-instance coordination isn't needed
  </Accordion>


  </AccordionGroup>

<Note>
  Additional storage backends (Redis and Database) are [coming soon](#coming-soon) for distributed rate limiting and advanced analytics.
</Note>

## Coming Soon

The following rate limiting features are planned for future releases:

| Feature | Description | Version |
|---------|-------------|---------|
| **Redis Storage** | Distributed rate limiting state stored in Redis for coordination across multiple router instances | v1 |
| **Database Storage** | Persistent rate limiting state with advanced querying capabilities for analytics and compliance | v2 |
| **Per-End-User Limits** | Rate limits applied to end users via `Helicone-User-Id` header for SaaS user quotas | v1 |
| **Per-Team Limits** | Rate limits applied to teams for budget and governance controls | v2 |
| **Per-Team-Member Limits** | Rate limits applied to individual team members for governance | v2 |
| **Spend Limits** | Cost-based limits that restrict usage based on dollar amounts spent per time period | v2 |
| **Usage Limits** | Token-based limits that restrict usage based on input/output tokens consumed | v2 |