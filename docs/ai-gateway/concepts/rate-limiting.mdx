---
title: "Rate Limiting & Spend Controls"
sidebarTitle: "Rate Limiting"
description: "GCRA-based rate limiting with burst capacity and smooth request throttling"
---

The AI Gateway provides flexible rate limiting using GCRA (Generic Cell Rate Algorithm) to help you manage request frequency and prevent abuse. Configure limits globally or per-router with burst capacity and smooth rate limiting.

Rate limiting uses **[configurable storage backends](#storage-backend-options)** and is applied with configurable burst capacity and sustained rates.

## Getting Started

### Why Use Rate Limiting?

Rate limiting helps you:
- **Prevent abuse** by limiting request rates per API key
- **Manage costs** by controlling request frequency
- **Ensure stability** by preventing traffic spikes from overwhelming your system
- **Fair usage** by distributing capacity across different API keys
- **Control your own traffic** based on your business requirements

<Info>
  **Provider rate limits are handled automatically** by the load balancing system. This rate limiting feature is for controlling your own API traffic based on your business requirements.
</Info>

<Note>
  For complete configuration options and syntax, see the [Configuration Reference](/ai-gateway/config#rate-limiting).
</Note>

### Available Strategies

<AccordionGroup>
  <Accordion title="Requests Per Time Period - Default" icon="gauge">
    **Burst-aware rate limiting with smooth sustained rates**
    
    Control request frequency using capacity (burst allowance) and refill frequency (sustained rate). Uses GCRA (Generic Cell Rate Algorithm) for smooth token bucket behavior that's more sophisticated than simple sliding window approaches.
    
    **Best for:** Production workloads requiring smooth rate limiting with burst tolerance
    
    **How it works:**
    1. Each API key gets a virtual token bucket with specified capacity
    2. Requests consume tokens from the bucket
    3. Tokens refill at the specified rate (refill-frequency / capacity)
    4. Requests are allowed if tokens are available, rejected otherwise
    
    **Example:**
    ```yaml
    rate-limit:
      global:
        per-api-key:
          capacity: 500
          refill-frequency: 1s  # 500 requests per second sustained
    ```
  </Accordion>

  <Accordion title="Rate limiting per end user" icon="window">
    **Rate limit per end user** *(Coming in v1)*
    If you would like to rate limit based on your end user, you can either use the `user` parameter
    with OpenAI or attach the `Helicone-User-Id` request header. See
    [User metrics quickstart](/features/advanced-usage/user-metrics#quick-start)
    for more information.
    
    
    **Best for:** Ensuring one of your users don't exceed usage limits.
  </Accordion>

  <Accordion title="Token count and Count usage based limits" icon="chart-line">
    **Rate limit based off token usage or spend rather than request count** *(Coming in v2)*

    Enforce team or project budgets via spend or token based rate limits.
    
    **Best for:** Spend controls, enforcing end user usage limits
  </Accordion>
</AccordionGroup>

### Configuration Examples

<Tabs>
  <Tab title="Production API - Abuse Prevention">
    **Use case:** Production API that needs to prevent abuse while allowing reasonable burst traffic for legitimate users.

    ```yaml
    rate-limit:
      global:
        store: in-memory
        per-api-key:
          capacity: 1000
          refill-frequency: 1m  # 1000 requests per minute
        cleanup-interval: 5m
    ```
  </Tab>

  <Tab title="Development Environment - Cost Safety">
    **Use case:** Development environment where you want to prevent accidental high costs while allowing reasonable experimentation.

    ```yaml
    rate-limit:
      global:
        store: in-memory
        per-api-key:
          capacity: 100
          refill-frequency: 1h  # 100 requests per hour
        cleanup-interval: 5m
    ```
  </Tab>

  <Tab title="Multi-Tier Service - Different Router Limits">
    **Use case:** Different service tiers with varying rate limits. Premium router gets higher limits than basic router.

    ```yaml
    rate-limit:
      global:
        store: in-memory
        cleanup-interval: 5m
    
    routers:
      premium:
        rate-limit:
          per-api-key:
            capacity: 5000
            refill-frequency: 1m  # 5000 requests per minute
      
      basic:
        rate-limit:
          per-api-key:
            capacity: 100
            refill-frequency: 1m  # 100 requests per minute
    ```
  </Tab>
</Tabs>

## Reference

### Rate Limiting Levels

The AI Gateway supports rate limiting at multiple levels based on **who** gets rate limited:

| Level                   | Availability | Description                                                                 | Example Use Case       |
| ----------------------- | ------------ | --------------------------------------------------------------------------- | ---------------------- |
| **Per-API-Key**         | v0 (live!)   | Limits applied to each API key individually                                | Prevent API key abuse  |
| **Per-End-User**        | v1 (coming)  | Limits applied to end users via `Helicone-User-Id` header                 | SaaS user quotas       |
| **Per-Team**            | v2 (planned) | Limits applied to teams for budget and governance controls                 | Department budgets     |
| **Per-Team-Member**     | v2 (planned) | Limits applied to individual team members for governance                   | Developer quotas       |

#### Configuration Scope

Rate limits are checked in precedence order when a request comes in:

<Steps>
  <Step title="Global Rate Limits">
    **Application-wide limits** *(Available in v0)*
    
    Applied to all requests across all routers. These limits are checked first and act as a safety net for your entire system.
  </Step>
  
  <Step title="Router-Specific Rate Limits">
    **Individual router limits** *(Available in v0)*
    
    Applied after global limits pass. Each router can have custom limits or opt out of rate limiting entirely.
  </Step>
</Steps>

<Note>
  Global and router-specific settings control the **configuration scope**, not who gets limited.
</Note>

### Storage Backend Options

Rate limiting counters can be stored in different backends depending on your deployment needs:

<AccordionGroup>
  <Accordion title="In-Memory Storage" icon="memory">
    **Local memory storage** *(Available in v0)*
    
    Rate limiting state is stored locally in each router instance. Fast and simple, but limits are not shared across multiple instances.
    
    ```yaml
    rate-limit:
      global:
        store: in-memory
        cleanup-interval: 5m
    ```
    
    **Best for:**
    - Single instance deployments
    - Development environments
    - High-performance scenarios where cross-instance coordination isn't needed
  </Accordion>

  <Accordion title="Redis Storage" icon="database">
    **Distributed storage** *(Coming in v1)*
    
    Rate limiting state is stored in Redis, allowing coordination across multiple router instances for consistent limits.
    
    ```yaml
    rate-limit:
      global:
        store: redis
        redis:
          url: "redis://localhost:6379"
        cleanup-interval: 5m
    ```
    
    **Best for:**
    - Multi-instance deployments
    - Load-balanced router setups
    - Consistent rate limiting across a distributed system
  </Accordion>

  <Accordion title="Database Storage" icon="server">
    **Persistent storage** *(Planned for future releases)*
    
    Rate limiting state stored in a database for persistence and advanced querying capabilities.
    
    **Best for:**
    - Long-term rate limiting analytics
    - Compliance requirements
    - Complex rate limiting policies
  </Accordion>
</AccordionGroup>

### Choosing the Right Configuration

| Use Case                    | Recommended Approach          | Availability |
| --------------------------- | ----------------------------- | ------------ |
| **Production APIs**         | Global GCRA with burst tolerance | v0       |
| **Development/Testing**     | Conservative limits with long refill periods | v0 |
| **Multi-tier services**     | Router-specific limit overrides | v0        |
| **High-volume systems**     | High capacity with short refill periods | v0 |
| **Single instance**         | In-memory storage             | v0           |
| **Distributed deployments** | Redis storage                 | v1           |
| **SaaS applications**       | Per-end-user limits           | v1           |
| **Enterprise governance**   | Per-team and per-team-member limits | v2    |