---
title: "LiteLLM Integration"
sidebarTitle: "Callbacks"
description: "Connect Helicone with LiteLLM, a unified interface for multiple LLM providers. Standardize logging and monitoring across various AI models with simple callback or proxy setup."
"twitter:title": "LiteLLM Integration - Helicone OSS LLM Observability"
---

[LiteLLM](https://github.com/BerriAI/litellm) is a model I/O library to standardize API calls to Azure, Anthropic, OpenAI, etc. Here's how you can log your LLM API calls to Helicone from LiteLLM.

<Note>
  **Note:** [Custom
  Properties](https://docs.helicone.ai/features/advanced-usage/custom-properties)
  are available in `metadata` starting with LiteLLM version `1.41.23`.
</Note>

<Tabs>
  <Tab title="Python">

# Approach 1: Use Callbacks

## 1 line integration

Add `HELICONE_API_KEY` to your environment variables.

```bash
export HELICONE_API_KEY=sk-<your-api-key>
# You can also set it in your code (See below)
```

Tell LiteLLM you want to log your data to Helicone

```python
litellm.success_callback=["helicone"]
```

## Complete code

```python
from litellm import completion

## set env variables
os.environ["HELICONE_API_KEY"] = "your-helicone-key"
os.environ["OPENAI_API_KEY"], os.environ["COHERE_API_KEY"] = "", ""

# set callbacks
litellm.success_callback=["helicone"]

#openai call
response = completion(
  model="gpt-3.5-turbo",
  messages=[{"role": "user", "content": "Hi ðŸ‘‹ - i'm openai"}],
  metadata={
    "Helicone-Property-Hello": "World"
  }
)

#cohere call
response = completion(
  model="command-r",
  messages=[{"role": "user", "content": "Hi ðŸ‘‹ - i'm cohere"}],
  metadata={
    "Helicone-Property-Hello": "World"
  }
)

print(response)
```

# Approach 2: [OpenAI + Azure only] Use Helicone as a proxy

Helicone provides advanced functionality like caching, etc. which they support for Azure and OpenAI.

If you want to use Helicone to proxy your OpenAI/Azure requests, then you can -

## 2 line integration

```python
litellm.api_url = ""https://oai.helicone.ai/v1"" # set the base url
litellm.headers = {"Helicone-Auth": f"Bearer {os.getenv('HELICONE_API_KEY')}"} # set your headers
```

## Complete code

```python
import litellm
from litellm import completion

litellm.api_base = "https://oai.helicone.ai/v1"
litellm.headers = {"Helicone-Auth": f"Bearer {os.getenv('HELICONE_API_KEY')}"}

response = litellm.completion(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": "how does a court case get to the Supreme Court?"}],
    metadata={
        "Helicone-Property-Hello": "World"
    }
)

print(response)
```

# Approach 3: Using Gemini with Helicone Proxy (Requires Monkey Patch)

Currently, LiteLLM requires a small monkey patch to correctly route Gemini requests through the Helicone proxy using the Router.

```python
import os
import asyncio
import litellm
from litellm.router import Router
from urllib.parse import urlparse, urlunparse, parse_qs, urlencode
from litellm.llms.vertex_ai.vertex_llm_base import VertexBase

HELICONE_API_KEY = os.getenv("HELICONE_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

if not HELICONE_API_KEY or not GEMINI_API_KEY:
    print("Error: HELICONE_API_KEY and GEMINI_API_KEY must be set.")

MODEL_NAME = "gemini-1.5-flash-latest"

router = Router(
    model_list=[
        {
            "model_name": "gemini-hconeai",
            "litellm_params": {
                "model": f"gemini/{MODEL_NAME}",
                "api_base": f"https://gateway.helicone.ai/v1beta/models/{MODEL_NAME}:generateContent?key={GEMINI_API_KEY}",
                "extra_headers": {
                    "Content-Type": "application/json",
                    "Helicone-Auth": f"Bearer {HELICONE_API_KEY}",
                    "Helicone-Target-Url": "https://generativelanguage.googleapis.com"
                },
                "custom_llm_provider": "gemini"
            }
        }
    ]
)

_original_check_proxy = VertexBase._check_custom_proxy

def fixed_check_custom_proxy(
    self, api_base: str | None, custom_llm_provider: str,
    gemini_api_key: str | None, endpoint: str, stream: bool | None,
    auth_header: str | None, url: str
) -> tuple[str | None, str]:

    is_helicone_gemini_proxy = (
        api_base is not None
        and "gateway.helicone.ai" in api_base
        and custom_llm_provider == "gemini"
    )

    if is_helicone_gemini_proxy:
        final_url = api_base
        auth = None
        if stream:
            parsed = urlparse(final_url)
            qs = parse_qs(parsed.query);
            qs['alt'] = ['sse']
            final_url = urlunparse((parsed.scheme, parsed.netloc, parsed.path, parsed.params, urlencode(qs, doseq=True), parsed.fragment))
        return (auth, final_url)
    elif _original_check_proxy:
        return _original_check_proxy(
            self, api_base, custom_llm_provider, gemini_api_key,
            endpoint, stream, auth_header, url
        )
    else:
        return (auth_header, url)

async def main():
    VertexBase._check_custom_proxy = fixed_check_custom_proxy
    print("Applied Gemini/Helicone patch...")

    response = await router.acompletion(
        model="gemini-hconeai",
        messages=[{"role": "user", "content": "Say 'Hello World'"}]
    )
    print(f"Response: {response.choices[0].message.content}")

if __name__ == "__main__":
    asyncio.run(main())
```

</Tab>
</Tabs>

Feel free to [check it out](https://github.com/BerriAI/litellm) and tell us what you think ðŸ‘‹
