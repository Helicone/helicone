---
title: "LiteLLM Integration with Callbacks"
sidebarTitle: "Callbacks"
description: "Connect Helicone with LiteLLM using callbacks to log and monitor API calls across various AI models."
"twitter:title": "LiteLLM Integration with Callbacks - Helicone OSS LLM Observability"
---

[LiteLLM](https://github.com/BerriAI/litellm) is a model I/O library to standardize API calls to Azure, Anthropic, OpenAI, etc. Here's how you can log your LLM API calls to Helicone from LiteLLM using callbacks.

<Note>
  **Note:** [Custom
  Properties](https://docs.helicone.ai/features/advanced-usage/custom-properties)
  are available in `metadata` starting with LiteLLM version `1.41.23`.
</Note>

## 1 line integration

Add `HELICONE_API_KEY` to your environment variables.

```bash
export HELICONE_API_KEY=sk-<your-api-key>
# You can also set it in your code (See below)
```

Tell LiteLLM you want to log your data to Helicone

```python
litellm.success_callback=["helicone"]
```

## Complete code

```python
from litellm import completion
import os

## set env variables
os.environ["HELICONE_API_KEY"] = "your-helicone-key"
os.environ["OPENAI_API_KEY"], os.environ["COHERE_API_KEY"] = "", ""

# set callbacks
litellm.success_callback=["helicone"]

#openai call
response = completion(
  model="gpt-3.5-turbo",
  messages=[{"role": "user", "content": "Hi ðŸ‘‹ - i'm openai"}],
  metadata={
    "Helicone-Property-Hello": "World"
  }
)

#cohere call
response = completion(
  model="command-r",
  messages=[{"role": "user", "content": "Hi ðŸ‘‹ - i'm cohere"}],
  metadata={
    "Helicone-Property-Hello": "World"
  }
)

print(response)
```

Feel free to [check it out](https://github.com/BerriAI/litellm) and tell us what you think ðŸ‘‹

For proxy-based integration with LiteLLM, see our [LiteLLM Proxy Integration](/getting-started/integration-method/litellm-proxy) guide.
