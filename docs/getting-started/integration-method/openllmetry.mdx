---
title: "OpenLLMetry Async Integration"
sidebarTitle: "OpenLLMetry"
description: "Log LLM traces directly to Helicone, bypassing our proxy, with OpenLLMetry. Supports OpenAI, Anthropic, Azure OpenAI, Cohere, Bedrock, Google AI Platform, and more."
"twitter:title": "OpenLLMetry Async Integration - Helicone OSS LLM Observability"
---

# Overview

Async Integration let's you log events and calls without placing Helicone in your app's critical
path. This ensures that an issue with Helicone will not cause an outage to your app.

<Tabs>
  <Tab title="Node.js">
  ```typescript
  import { HeliconeAsyncLogger } from "@helicone/helicone";
  import OpenAI from "openai";

  const logger = new HeliconeAsyncLogger({
    apiKey: process.env.HELICONE_API_KEY,
    // pass in the providers you want logged
    providers: {
      openAI: OpenAI
      //anthropic: Anthropic,
      //cohere: Cohere
      // ...
    }
  });
  logger.init();

  const openai = new OpenAI();

  async function main() {
    const completion = await openai.chat.completions.create({
      messages: [{"role": "system", "content": "You are a helpful assistant."},
          {"role": "user", "content": "Who won the world series in 2020?"},
          {"role": "assistant", "content": "The Los Angeles Dodgers won the World Series in 2020."},
          {"role": "user", "content": "Where was it played?"}],
      model: "gpt-3.5-turbo",
    });

    console.log(completion.choices[0]);
  }
  main();
  ```
  </Tab>

</Tabs>

# Supported Providers

- [x] OpenAI
- [x] Anthropic
- [x] Azure OpenAI
- [x] Cohere
- [x] Bedrock
- [x] Google AI Platform

# Other Integrations

- [Comparing Proxy vs Async Integration](../proxy-vs-async.mdx)
- [Gateway Integration](/getting-started/integration-method/gateway)
