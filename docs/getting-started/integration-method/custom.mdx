---
title: "Custom Model (Beta)"
---

We recently added support for custom models like llama or GPT-Neo.
This allows you to use your own models with Helicone.
This is currently in beta, so please let us know if you have any issues.

<Tabs>
<Tab title="NodeJS">

Add `HELICONE_API_KEY` to your environment variables.

```bash
export HELICONE_API_KEY=sk-<your-api-key>
# You can also set it in your code (See below)
```

```typescript
import {
  HeliconeAsyncConfiguration,
  HeliconeLogBuilder,
  HeliconeLogger,
  ResponseBody,
} from "@helicone/helicone";

const heliconeApiKey = process.env.HELICONE_API_KEY;

const config = new HeliconeAsyncConfiguration({
  heliconeMeta: {
    apiKey: heliconeApiKey,
  },
});

const logger = new HeliconeLogger(config);

const llmArgs = {
  model: "llama-2",
  prompt: "Say hi!",
};
const builder = new HeliconeLogBuilder(llmArgs);

/*
result = callToLLM(llmArgs)
*/

const result: ResponseBody = {
  text: "This is my response",
  usage: {
    total_tokens: 13,
    prompt_tokens: 5,
    completion_tokens: 8,
  },
};

builder.addResponse(result);
builder.addUser("test-user");
const response = await logger.submit(builder);
if (response.status !== 200) {
  throw new Error(response.data);
}
```

</Tab>

<Tab title="Curl">
The Helicone Async Log Request API is used for logging requests and responses that
go through an endpoint. This is highly useful for auditing, debugging and observing
the behavior of your interactions with the system.

## Request Structure

A typical request will have the following structure:

### Endpoint

```

POST https://api.hconeai.com/custom/v1/log

```

### Headers

| Name          | Value            |
| ------------- | ---------------- |
| Authorization | Bearer {API_KEY} |

Replace `{API_KEY}` with your actual API Key.

### Body

The body of the request should follow the `HeliconeAyncLogRequest` structure:

```ts
export type HeliconeAyncLogRequest = {
  providerRequest: ProviderRequest;
  providerResponse: ProviderResponse;
  timing: Timing;
};

export type ProviderRequest = {
  url: string;
  json: {
    [key: string]: any;
  };
  meta: Record<string, string>;
};

export type ProviderResponse = {
  json: {
    [key: string]: any;
  };
  status: number;
  headers: Record<string, string>;
};

export type Timing = {
  // From Unix epoch in Milliseconds
  startTime: {
    seconds: number;
    milliseconds: number;
  };
  endTime: {
    seconds: number;
    milliseconds: number;
  };
};
```

## Example Usage

Here's an example using curl:

```sh
curl -X POST https://api.hconeai.com/custom/v1/log \
-H 'Authorization: Bearer your_api_key' \
-H 'Content-Type: application/json' \
-d '{
  "providerRequest": {
    "url": "https://example.com",
    "json": {
      "key1": "value1",
      "key2": "value2"
    },
    "meta": {
      "metaKey1": "metaValue1",
      "metaKey2": "metaValue2"
    }
  },
  "providerResponse": {
    "json": {
      "responseKey1": "responseValue1",
      "responseKey2": "responseValue2"
    },
    "status": 200,
    "headers": {
      "headerKey1": "headerValue1",
      "headerKey2": "headerValue2"
    }
  },
  "timing": {
    "startTime": {
      "seconds": 1625686222,
      "milliseconds": 500
    },
    "endTime": {
      "seconds": 1625686244,
      "milliseconds": 750
    }
  }
}'
```

In the curl command above, replace `your_api_key` with your actual API key, and adjust the values in the JSON to fit your actual request and response data and timing.

The response body is a JSON object of the entire response returned by OpenAI, unless it is a streamed request. In that case, it is a JSON object with a key called "streamed_data", which is an array of every single chunk.

</Tab>
</Tabs>
