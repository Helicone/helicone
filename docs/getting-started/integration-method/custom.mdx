---
title: "Custom Model Integration"
sidebarTitle: "Custom Model"
description: "Integrate any custom LLM, including open-source models like Llama and GPT-Neo, with Helicone. Step-by-step guide for both NodeJS and Curl implementations to connect your proprietary or open-source models."
"twitter:title": "Custom Model Integration - Helicone OSS LLM Observability"
---

# Quickstart

<Tabs>
<Tab title="NodeJS">
Logging calls to custom models is currently supported via the Helicone NodeJS SDK.
```typescript
import { HeliconeManualLogger } from "@helicone/helicone";

const logger = new HeliconeManualLogger({
  apiKey: process.env.HELICONE_API_KEY
});

const reqBody = {
  "model": "text-embedding-ada-002",
  "input": "The food was delicious and the waiter was very friendly.",
  "encoding_format": "float"
}

logger.registerRequest(reqBody);

const r = await fetch("https://api.openai.com/v1/embeddings", {
  method: "POST",
  headers: {
    "Content-Type": "application/json",
    Authorization: `Bearer ${process.env.OPENAI_API_KEY}`
  },
  body: JSON.stringify(reqBody)
})

const res = await r.json();
console.log(res);

logger.sendLog(res);
```

## Structure of Objects

### Registering Request

A typical request will have the following structure:

```typescript
type BaseRequest = {
  model: string,
  temperature?: number;
  max_tokens?: number;
  stream?: boolean;
}

// The name and structure of the prompt field depends on the model you are using.
// Eg: for chat models it is named "messages", for embeddings models it is named "input".
// Hence, the only enforced type is `model`, you need still add the respective prompt property for your model.
// You may also add more properties (eg: temperature, stop reason etc)
type ILogRequest = {
  model: string;
} & {
  [key: string]: any;
};
```

### Sending Log
To submit logs, you need to extract the response from the model and send it to Helicone. The
response should follow the below structure:

```typescript
type ILogResponse = {
  id: string;
  object: string;
  created: string;
  model: string;
  choices: Array<{
    index: number;
    finish_reason: string;
    message: {
      role: string;
      content: string;
    };
  }>
  usage?: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  }
}
```
</Tab>

<Tab title="Curl">

## Request Structure
A typical request will have the following structure:

### Endpoint
```
POST https://api.us.hconeai.com/custom/v1/log
```

### Headers

| Name          | Value              |
| ------------- | ------------------ |
| Authorization | Bearer `{API_KEY}` |

Replace `{API_KEY}` with your actual API Key.

### Body
```typescript
export type HeliconeAyncLogRequest = {
  providerRequest: ProviderRequest;
  providerResponse: ProviderResponse;
  timing: Timing;
};

export type ProviderRequest = {
  url: string;
  json: {
    [key: string]: any;
  };
  meta: Record<string, string>;
};

export type ProviderResponse = {
  json: {
    [key: string]: any;
  };
  status: number;
  headers: Record<string, string>;
};

export type Timing = {
  // From Unix epoch in Milliseconds
  startTime: {
    seconds: number;
    milliseconds: number;
  };
  endTime: {
    seconds: number;
    milliseconds: number;
  };
};
```

## Example Usage
```
curl -X POST https://api.us.hconeai.com/custom/v1/log \
-H 'Authorization: Bearer your_api_key' \
-H 'Content-Type: application/json' \
-d '{
  "providerRequest": {
    "url": "https://example.com",
    "json": {
      "key1": "value1",
      "key2": "value2"
    },
    "meta": {
      "metaKey1": "metaValue1",
      "metaKey2": "metaValue2"
    }
  },
  "providerResponse": {
    "json": {
      "responseKey1": "responseValue1",
      "responseKey2": "responseValue2"
    },
    "status": 200,
    "headers": {
      "headerKey1": "headerValue1",
      "headerKey2": "headerValue2"
    }
  },
  "timing": {
    "startTime": {
      "seconds": 1625686222,
      "milliseconds": 500
    },
    "endTime": {
      "seconds": 1625686244,
      "milliseconds": 750
    }
  }
}'
```


</Tab>

</Tabs>
