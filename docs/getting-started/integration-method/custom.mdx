---
title: "Custom Model Integration"
sidebarTitle: "Custom Model"
description: "Integrate any custom LLM, including open-source models like Llama and GPT-Neo, with Helicone. Step-by-step guide for both NodeJS and Curl implementations to connect your proprietary or open-source models."
"twitter:title": "Custom Model Integration - Helicone OSS LLM Observability"
---

# Quickstart

<Tabs>
<Tab title="NodeJS">
Logging calls to custom models is currently supported via the Helicone NodeJS SDK.

<Steps>
  <Step title="To get started, install the `@helicone/helpers` package">
  ```bash
  npm install @helicone/helpers
  ```
  </Step>
  <Step title="Set `HELICONE_API_KEY` as an environment variable">
  ```bash
  export HELICONE_API_KEY=sk-<your-api-key>
  ```
  <Info>You can also set the Helicone API Key in your code (See below)</Info>
  </Step>
  <Step title="Create a new HeliconeManualLogger instance">
  ```typescript
  import { HeliconeManualLogger } from "@helicone/helpers";

  const heliconeLogger = new HeliconeManualLogger({
    apiKey: process.env.HELICONE_API_KEY, // Can be set as env variable
    headers: {} // Additional headers to be sent with the request
  });
  ```
  </Step>
  <Step title="Log your request">
  ```typescript
  const reqBody = {
    model: "text-embedding-ada-002",
    input: "The food was delicious and the waiter was very friendly.",
    encoding_format: "float"
  }
  const res = await heliconeLogger.logRequest(
    reqBody,
    async (resultRecorder) => {
      const r = await fetch("https://api.openai.com/v1/embeddings", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          Authorization: `Bearer ${process.env.OPENAI_API_KEY}`
        },
        body: JSON.stringify(reqBody)
      })
      const resBody = await r.json();
      resultRecorder.appendResults(resBody);
      return results; // this will be returned by the logRequest function
    },
    {
     // Additional headers to be sent with the request
    }
  );
  ```
</Step>
</Steps>

## API Reference

### HeliconeManualLogger

```typescript
class HeliconeManualLogger {
  constructor(opts: IHeliconeManualLogger)
}

type IHeliconeManualLogger = {
  apiKey: string;
  headers?: Record<string, string>;
  loggingEndpoint?: string; // defaults to https://api.hconeai.com/custom/v1/log
};
```

### logRequest

```typescript
logRequest<T>(
    request: HeliconeLogRequest,
    operation: (resultRecorder: HeliconeResultRecorder) => Promise<T>,
    additionalHeaders?: Record<string, string>
  ): Promise<T>
```

#### Parameters

1. `request`: `HeliconeLogRequest` - The request object to log
```typescript
type HeliconeLogRequest = ILogRequest | HeliconeCustomEventRequest; // ILogRequest is the type for the request object for custom model logging

// The name and structure of the prompt field depends on the model you are using.
// Eg: for chat models it is named "messages", for embeddings models it is named "input".
// Hence, the only enforced type is `model`, you need still add the respective prompt property for your model.
// You may also add more properties (eg: temperature, stop reason etc)
type ILogRequest = {
  model: string;
  [key: string]: any;
};
```

2. `operation`: `(resultRecorder: HeliconeResultRecorder) => Promise<T>` - The operation to be executed and logged
```typescript
class HeliconeResultRecorder {
  private results: Record<string, any> = {};

  appendResults(data: Record<string, any>): void {
    this.results = { ...this.results, ...data };
  }

  getResults(): Record<string, any> {
    return this.results;
  }
}
```

3. `additionalHeaders`: `Record<string, string>`
    - Additional headers to be sent with the request
    - This can be used to use features like [session management](/features/sessions), [custom properties](/features/advanced-usage/custom-properties), etc.


</Tab>

{/* ```typescript
import { HeliconeManualLogger } from "@helicone/helicone";

const logger = new HeliconeManualLogger({
  apiKey: process.env.HELICONE_API_KEY
});

const reqBody = {
  "model": "text-embedding-ada-002",
  "input": "The food was delicious and the waiter was very friendly.",
  "encoding_format": "float"
}

logger.registerRequest(reqBody);

const r = await fetch("https://api.openai.com/v1/embeddings", {
  method: "POST",
  headers: {
    "Content-Type": "application/json",
    Authorization: `Bearer ${process.env.OPENAI_API_KEY}`
  },
  body: JSON.stringify(reqBody)
})

const res = await r.json();
console.log(res);

logger.sendLog(res);
```

## Structure of Objects

### Registering Request

A typical request will have the following structure:

```typescript
type BaseRequest = {
  model: string,
  temperature?: number;
  max_tokens?: number;
  stream?: boolean;
}

// The name and structure of the prompt field depends on the model you are using.
// Eg: for chat models it is named "messages", for embeddings models it is named "input".
// Hence, the only enforced type is `model`, you need still add the respective prompt property for your model.
// You may also add more properties (eg: temperature, stop reason etc)
type ILogRequest = {
  model: string;
} & {
  [key: string]: any;
};
```

### Sending Log
To submit logs, you need to extract the response from the model and send it to Helicone. The
response should follow the below structure:

```typescript
type ILogResponse = {
  id: string;
  object: string;
  created: string;
  model: string;
  choices: Array<{
    index: number;
    finish_reason: string;
    message: {
      role: string;
      content: string;
    };
  }>
  usage?: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  }
}
```
</Tab> */}

<Tab title="Curl">

## Request Structure
A typical request will have the following structure:

### Endpoint
```
POST https://api.us.hconeai.com/custom/v1/log
```

### Headers

| Name          | Value              |
| ------------- | ------------------ |
| Authorization | Bearer `{API_KEY}` |

Replace `{API_KEY}` with your actual API Key.

### Body
```typescript
export type HeliconeAsyncLogRequest = {
  providerRequest: ProviderRequest;
  providerResponse: ProviderResponse;
  timing: Timing;
};

export type ProviderRequest = {
  url: "custom-model-nopath";
  json: {
    [key: string]: any;
  };
  meta: Record<string, string>;
};

export type ProviderResponse = {
  json: {
    [key: string]: any;
  };
  status: number;
  headers: Record<string, string>;
};

export type Timing = {
  // From Unix epoch in Milliseconds
  startTime: {
    seconds: number;
    milliseconds: number;
  };
  endTime: {
    seconds: number;
    milliseconds: number;
  };
};
```

## Example Usage
```
curl -X POST https://api.us.hconeai.com/custom/v1/log \
-H 'Authorization: Bearer your_api_key' \
-H 'Content-Type: application/json' \
-d '{
  "providerRequest": {
    "url": "custom-model-nopath",
    "json": {
      "model": "text-embedding-ada-002",
      "input": "The food was delicious and the waiter was very friendly.",
      "encoding_format": "float"
    },
    "meta": {
      "metaKey1": "metaValue1",
      "metaKey2": "metaValue2"
    }
  },
  "providerResponse": {
    "json": {
      "responseKey1": "responseValue1",
      "responseKey2": "responseValue2"
    },
    "status": 200,
    "headers": {
      "headerKey1": "headerValue1",
      "headerKey2": "headerValue2"
    }
  },
  "timing": {
    "startTime": {
      "seconds": 1625686222,
      "milliseconds": 500
    },
    "endTime": {
      "seconds": 1625686244,
      "milliseconds": 750
    }
  }
}'
```


</Tab>

</Tabs>
