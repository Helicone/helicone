---
title: "Platform Overview"
sidebarTitle: "Platform Overview"
description: "Understand how Helicone solves the core challenges of building production LLM applications"
---

Now that your requests are flowing through Helicone, let's explore what you can do with the platform.

<Frame>
  <img
    src="/images/introduction/intro-dashboard.webp"
    alt="Helicone dashboard showing comprehensive LLM observability metrics."
  />
</Frame>

## How It Works

Helicone's **AI Gateway** gives you access to 100+ LLM models through a single unified API:

1. **Add Credits** - Top up your Helicone account (0% markup)
2. **Single Integration** - Point your OpenAI SDK to our gateway URL
3. **Use Any Model** - Switch between providers by just changing the model name
4. **Automatic Observability** - Every request is logged with costs, latency, and errors tracked

No need to sign up for OpenAI, Anthropic, Google, or any other provider. We manage the API keys and you get complete observability built in.

<Accordion title="Advanced: Bring your own provider keys">
  Prefer to use your own API keys? You can configure your own provider keys at [Provider Keys](https://us.helicone.ai/providers) for direct control over billing and provider accounts.
</Accordion>

## The Problems We Solve

<CardGroup cols={2}>
<Card title="Reliability Issues" icon="shield">
  Provider outages break your application. No visibility when requests fail. Manual fallback logic is complex and error-prone.
</Card>
<Card title="Debugging Complexity" icon="bug">
  LLM responses are non-deterministic. Multi-step AI workflows are hard to trace. Errors are difficult to reproduce.
</Card>
<Card title="Cost Uncertainty" icon="dollar-sign">
  Unpredictable spending across providers. No understanding of unit economics. Difficult to optimize without breaking functionality.
</Card>
<Card title="Prompt Management Pain" icon="code">
  Every prompt change requires a deployment. No version control for prompts. Can't iterate quickly based on user feedback.
</Card>
</CardGroup>

## Our Principles

**Best Price Always**
We fight for every penny. 0% markup on credits means you pay exactly what providers charge. No hidden fees, no games.

**Invisible Performance**  
Your app shouldn't slow down for observability. Edge deployment keeps us under 50ms. Always.

**Always Online**  
Your app stays up, period. Providers fail, we fallback. Rate limits hit, we load balance. We don't go down.

**Never Be Surprised**  
No shock bills. No mystery spikes. See every cost as it happens. We believe in radical transparency.

**Find Anything**  
Every request, searchable. Every error, findable. That needle in the haystack? We'll help you find it.

**Built for Your Worst Day**  
When production breaks and everyone's panicking, we're rock solid. Built for when you need us most.

## Real Scenarios

<AccordionGroup>
  <Accordion title="Costs spiked 300% overnight ðŸ“ˆ">
    **What happened:** Your AWS bill shows \$15K in LLM costs this month vs \$5K last month.

    **How Helicone helps:**
    - Instant breakdown by user, feature, or any custom dimension
    - See exactly which user/feature caused the spike
    - Take targeted action in minutes, not days

    **Real example:** An enterprise customer had an API key leaked and racked up over \$1M in LLM spend. With Helicone's user tracking and custom properties, they identified the compromised key within minutes and prevented further damage.
  </Accordion>

  <Accordion title="User says AI gave wrong answer ðŸ¤”">
    **What happened:** Customer support forwards a complaint that your AI chatbot gave incorrect information.

    **How Helicone helps:**
    - View the complete conversation history with session tracking
    - Trace through multi-step workflows to find where it failed
    - Identify the exact prompt that caused the issue
    - Deploy the fix instantly with prompt versioning (no code deploy needed)

    **Real impact:** Traced bad response to outdated prompt version. Fixed and deployed new version in 5 minutes without engineering.
  </Accordion>

  <Accordion title="OpenAI is down ðŸ”´">
    **What happened:** OpenAI API returns 503 errors. Your production app stops working.

    **How Helicone helps:**
    - Configure automatic fallback chains (e.g., GPT-4o: OpenAI â†’ Vertex â†’ Bedrock)
    - Requests automatically route to backup providers when failures occur
    - Users get responses from alternative providers seamlessly
    - Full observability maintained throughout the outage

    **Real impact:** App stayed online during 2-hour OpenAI outage. Users never noticed.
  </Accordion>

  <Accordion title="AI agent workflow is broken ðŸ¤–">
    **What happened:** Your multi-step AI agent isn't completing tasks. Users are frustrated.

    **How Helicone helps:**
    - Session trees visualize the entire workflow across multiple LLM calls
    - Trace exactly where the sequence breaks down
    - See if it's hitting token limits, using wrong context, or failing prompt logic
    - Pinpoint the root cause in the chain of reasoning

    **Real impact:** Discovered agent was hitting context limits on step 3. Adjusted prompt strategy and fixed cascading failures.
  </Accordion>
</AccordionGroup>

## Start Exploring Features

<CardGroup cols={2}>

<Card
  title="AI Gateway" 
  href="/gateway/overview"
  icon="route"
>
  Use 100+ models through one unified API with automatic fallbacks
</Card>

<Card
  title="Agent Debugging"
  href="/features/sessions"
  icon="code-branch"
>
  Debug complex AI agents and multi-step workflows
</Card>

<Card
  title="Prompt Management" 
  href="/gateway/prompt-integration"
  icon="wand-magic-sparkles"
>
  Deploy prompts without code changes
</Card>


<Card
  title="Cost Tracking" 
  href="/guides/cookbooks/cost-tracking"
  icon="dollar-sign"
>
  Track cost and understand the unit economics of your LLM applications
</Card>
</CardGroup>

---

We built Helicone for developers with users depending on them. For the 3am outages. For the surprise bills. For finding that one broken request in millions.