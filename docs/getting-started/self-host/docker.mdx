---
title: "Docker Compose Self-Hosting"
sidebarTitle: "Docker Compose"
description: "Deploy Helicone using Docker Compose. Quick setup guide for running a containerized instance of the LLM observability platform on your local machine or server."
"twitter:title": "Docker Compose Deployment - Helicone OSS LLM Observability"
---

At Helicone we believe that open-source software makes the world a better place. We are committed to open-source and we made a guide to make it easy for you to deploy your own instance of Helicone.

## Running locally

```bash
# Clone repository
git clone https://github.com/Helicone/helicone.git

# Start up the services
docker compose up -d # NOTE: the initial build will take a while ~5 minutes, but subseqent builds will be a lot faster
```

**Default URLs:**

- Helicone Webpage: localhost:3000
- Helicone Worker: localhost:8585/v1/gateway/oai/v1/chat/completions
- Temp mailer: http://localhost:8025/

## Example

```python
from openai import OpenAI
import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

openai_client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    base_url="http://127.0.0.1:8585/v1/gateway/oai/v1",
    default_headers={
        "Helicone-Auth": f"Bearer {os.getenv('HELICONE_API_KEY')}",
    }
)

response = openai_client.completions.create(
    model="gpt-3.5-turbo-instruct",
    prompt="Count to 5",
    stream=False,
)
assert response.choices[0].text is not None

```
