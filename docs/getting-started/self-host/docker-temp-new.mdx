---
title: "Docker Compose Self-Hosting"
sidebarTitle: "Docker Compose"
description: "Deploy Helicone using Docker Compose. Quick setup guide for running a containerized instance of the LLM observability platform on your local machine or server."
"twitter:title": "Docker Compose Deployment - Helicone OSS LLM Observability"
---

At Helicone we believe that open-source software makes the world a better place. We are committed to open-source and we made a guide to make it easy for you to deploy your own instance of Helicone.

## Running locally

```bash
# Clone repository
git clone https://github.com/Helicone/helicone.git

# Close any running containers
docker compose -f docker/docker-compose-slim.yml down -v

# Start up the services
docker compose -f docker/docker-compose-slim.yml up -d

# Run migrations for clickhouse and postgres
flyway migrate -configFiles=db/flyway.conf
python3 clickhouse/ch_hcone.py --upgrade --host localhost --port 18123 --user default --no-password
```

**Default URLs:**

- Helicone Webpage: localhost:3000
- Helicone Worker: localhost:8787 (See known issues below)

## KNOWN ISSUES

The worker container is not very stable. For now we recommend using the proxy built into JAWN, some features are not implemented in the JAWN proxy yet like cache and rate limiting. Basic logging and prompts do work.

Example

```python
from openai import OpenAI
import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

openai_client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    base_url="http://127.0.0.1:8585/v1/gateway/oai/v1",
    default_headers={
        "Helicone-Auth": f"Bearer {os.getenv('HELICONE_API_KEY')}",
    }
)

response = openai_client.completions.create(
    model="gpt-3.5-turbo-instruct",
    prompt="Count to 5",
    stream=False,
)
assert response.choices[0].text is not None

```
