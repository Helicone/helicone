---
title: "Quickstart"
sidebarTitle: "Quickstart"
description: "Helicone is the open-source LLM observability platform for developers to monitor, debug, and improve **production-ready** applications."
"twitter:title": "Quickstart - Helicone Open Source LLM Observability"
---

<Frame>
  <img
    src="/images/introduction/intro-dashboard.webp"
    alt="Helicone dashboard where you can monitor your AI applications."
  />
</Frame>

Log your LLM requests, evaluate, and experiment with prompts, and get instant insights that helps you push changes to production with _confidence_. Helicone is the CI workflow designed for the entire LLM lifecycle.

## Quick Start

Welcome to Helicone! Integrate your preferred provider with Helicone in seconds.

<Steps>

<Step title="Create an account">
  Once you have an [account](https://helicone.ai/), proceed to the next step.
</Step>

  <Step title="Generate an API key">
    <AccordionGroup>
      <Accordion title="Go to Settings" icon="square-1">
        Click on your organization on the top left, then select `Settings` from the dropdown.
        <Frame>
          <img
            src="/images/introduction/key-step-1.webp"
            alt="Find Settings from Helicone's sidebar."
          />
        </Frame>
      </Accordion>

      <Accordion title="Select the API Keys tab" icon="square-2">
        <Frame>
          <img
            src="/images/introduction/key-step-2.webp"
            alt="In Settings, select the API Keys tab."
          />
        </Frame>
      </Accordion>

      <Accordion title="Generate new key" icon="square-3">
        Click on `Generate new key`.

        When creating a new Helicone API key, you have the ability to enable `read` and `write` permissions.
        Write keys can be used through Helicone via our proxy, feedback or any other Helicone service when calling a `POST` or using our gateway.
        <Frame>
          <img
            src="/images/introduction/key-step-3.webp"
            alt="Create a new Helicone API key from Settings."
          />
        </Frame>

        <Note>
          Keys with read permissions will start with `sk-` and keys with write
          permissions will start with `pk-`. For our EU customers, keys are generated
          with the prefix `eu-` this allows our edge workers to know which region to
          route the request to.
        </Note>
      </Accordion>

    </AccordionGroup>

    For more details on Helicone API keys, check out the [Helicone Auth](/helicone-headers/helicone-auth) docs.

  </Step>

  <Step title="Integrate with AI Gateway">

    Helicone's AI Gateway is an OpenAI-compatible, unified API with access to 100+ models, including OpenAI, Anthropic, Vertex, Groq, and more.

    **Bring your own key:** Go to [Provider Keys](https://us.helicone.ai/providers) to securely store API keys for the providers you want to use.

    <Tabs>
      <Tab title="TypeScript">
        ```typescript
        import { OpenAI } from "openai";

        const openai = new OpenAI({
          baseURL: "https://ai-gateway.helicone.ai/v1",
          apiKey: process.env.HELICONE_API_KEY,
        });

        const response = await openai.chat.completions.create({
          model: "gpt-4o-mini/openai", // model-name/provider
          messages: [{ role: "user", content: "Hello, world!" }],
        });
        ```
      </Tab>
      
      <Tab title="Python">
        ```python
        from openai import OpenAI

        client = OpenAI(
            base_url="https://ai-gateway.helicone.ai/v1",
            api_key=os.getenv("HELICONE_API_KEY")
        )

        response = client.chat.completions.create(
            model="claude-3-5-sonnet/anthropic",
            messages=[{"role": "user", "content": "Hello, world!"}]
        )
        ```
      </Tab>

      <Tab title="cURL">
        ```bash
        curl https://ai-gateway.helicone.ai/v1/chat/completions \
          -H "Content-Type: application/json" \
          -H "Authorization: Bearer $HELICONE_API_KEY" \
          -d '{
            "model": "gpt-4o-mini/openai",
            "messages": [
              { "role": "user", "content": "Hello, world!" }
            ]
          }'
        ```
      </Tab>
    </Tabs>

    <Note>
      **Why AI Gateway?** Get immediate, OpenAI-compatible access to models from any major provider, alongside features like rate limiting, caching, prompt versioning, and complete observability - all with zero code changes beyond the base URL and setting up your API keys.
    </Note>
  </Step>

  <Step title="Send your first request ðŸŽ‰">
    Once we receive your requests, you will see them in the `Requests` tab.

  You will also see that your `Dashboard` has been updated with your new request.

  </Step>

</Steps>

## What's next?

We curated a list to help you make the most of Helicone, but you're welcome to explore the product on your own!

<CardGroup cols={2}>
  <Card
    title="Add a custom property"
    href="/features/advanced-usage/custom-properties"
    icon="square-1"
  >
    Label your requests to segment, analyze, and visualize them.
  </Card>
  <Card
    title="Create your first prompt"
    href="/features/advanced-usage/prompts"
    icon="square-2"
  >
    Version your prompt and inputs as they evolve.
  </Card>
  <Card title="Track a session" href="/features/sessions" icon="square-3">
    Group and visualize multi-step LLM interactions (i.e. AI agents).
  </Card>
  <Card
    title="Cache responses"
    href="/features/advanced-usage/caching"
    icon="square-4"
  >
    Cache and watch how much time and cost you saved.
  </Card>
</CardGroup>

## Explore Features

Here are the key features to explore as you build with Helicone:

<CardGroup cols={3}>
  <Card
    title="Prompts"
    href="/features/advanced-usage/prompts"
    icon="square-terminal"
  >
    Manage and version prompts from code or UI.
  </Card>
  <Card
    title="Sessions"
    href="/features/sessions"
    icon="rectangle-history"
  >
    Trace agentic workflows and visualize them.
  </Card>
  <Card
    title="Custom Properties"
    href="/features/advanced-usage/custom-properties"
    icon="tag"
  >
    Label and segment your requests.
  </Card>
  <Card
    title="Caching"
    href="/features/advanced-usage/caching"
    icon="database"
  >
    Save cost and improve latency.
  </Card>
  <Card
    title="Rate Limiting"
    href="/features/advanced-usage/custom-rate-limits"
    icon="user-minus"
  >
    Easily rate-limit power users.
  </Card>
  <Card
    title="User Metrics"
    href="/features/advanced-usage/user-metrics"
    icon="chart-user"
  >
    Get insights into your user's usage.
  </Card>
</CardGroup>

## Alternative Integration Methods

Need a different integration approach? Here are provider-specific and alternative methods:

<CardGroup cols={4}>
  <Card title="OpenAI" href="/integrations/openai/javascript" />
  <Card title="Azure" href="/integrations/azure/javascript" />
  <Card title="Anthropic" href="/integrations/anthropic/javascript" />
  <Card title="Llama" href="/integrations/llama/javascript" />
  <Card title="Ollama" href="/integrations/ollama/javascript" />
  <Card title="AWS Bedrock" href="/integrations/bedrock/javascript" />
  <Card title="Gemini" href="/integrations/gemini/api/javascript" />
  <Card title="Groq" href="/integrations/groq/javascript" />
  <Card title="Instructor" href="/integrations/instructor/javascript" />
  <Card
    title="Vercel AI"
    href="/getting-started/integration-method/vercelai"
  />
  <Card
    title="Anyscale"
    href="/getting-started/integration-method/anyscale"
  />
  <Card
    title="Together AI"
    href="/getting-started/integration-method/together"
  />
  <Card
    title="Hyperbolic"
    href="/getting-started/integration-method/hyperbolic"
  />
  <Card
    title="Deepinfra"
    href="/getting-started/integration-method/deepinfra"
  />
  <Card
    title="OpenRouter"
    href="/getting-started/integration-method/openrouter"
  />
  <Card
    title="LiteLLM"
    href="/getting-started/integration-method/litellm"
  />
  <Card
    title="Fireworks AI"
    href="/getting-started/integration-method/fireworks"
  />
</CardGroup>

<CardGroup cols={2}>
  <Card
    title="OpenLLMetry"
    href="/getting-started/integration-method/openllmetry"
  >
    Log directly to Helicone without going through our proxy.
  </Card>
  <Card title="Gateway" href="/getting-started/integration-method/gateway">
    Custom gateway configurations and legacy documentation.
  </Card>
</CardGroup>

## Further Reading

<CardGroup cols={3}>
  <Card title="Proxy or Async?" href="/getting-started/proxy-vs-async">
    Determine when you should use a proxy or async function in Helicone.
  </Card>
  <Card title="How we calculate costs" href="/faq/how-we-calculate-cost">
    A detailed breakdown of our process to calculate cost per request.
  </Card>
  <Card
    title="Understanding Helicone Headers"
    href="/helicone-headers/header-directory"
  >
    Every header you need to know to access Helicone features.
  </Card>
</CardGroup>

## Questions?

Although we designed the docs to be as self-serving as possible, you are
welcome to join our [Discord](https://discord.com/invite/HwUbV3Q8qz) or
contact [help@helicone.ai](help@helicone.ai) with any questions or feedback
you have.

Interested in deploying Helicone on-prem? [Schedule a call](https://www.helicone.ai/contact) with us.
