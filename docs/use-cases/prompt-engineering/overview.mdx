---
title: "Overview"
sidebarTitle: "Overview"
description: "Prompt engineering is effective crafting prompts to guide Large Language Models (LLMs) like GPT-4 to produce desired outputs. It involves strategically designing input prompts to elicit accurate, relevant, and useful responses from the model."
"twitter:title": "Debugging LLM Applications - Helicone OSS LLM Observability"
---

import QuestionsSection from "/snippets/questions-section.mdx";


## Before prompt engineering

Before you follow along, we assume that you: 
- have a first draft of your prompt
- have a clear idea of the success criteria
- know who your audience are
- have some benchmark to measure prompt improvements
- have some example inputs and desired outputs to test against your prompts

We recommend taking a moment to brainstorm on these to make sure that the following recommendations are effective. 


## How to prompt engineer 
1. [Be specific and clear](/use-cases/prompt-engineering/be-specific-and-clear)
2. [Use structured formats](/use-cases/prompt-engineering/use-structured-formats)
3. [Leverage role-playing](/use-cases/prompt-engineering/leverage-role-playing)
4. [Implement few-shot learning](/use-cases/prompt-engineering/implement-few-shot-learning)
5. [Use constrained outputs](/use-cases/prompt-engineering/use-constrained-outputs)
6. [Use chain-of-thought prompting](/use-cases/prompt-engineering/use-chain-of-thought-prompting)


## When to start prompt engineering? 

- **Start from the beginning.** It's never to early to think about how your prompt will affect the output.
- **When you want to refine model outputs.**
- **When you are expanding features,** prompt engineering ensures that the model adapts to the new use cases.
- **When you are optimizing cost and performance,** an effective prompt can reduce token usage, lower latency, and improve performance.

## Why prompt engineer? 
- Get more accurate and relevant responses.
- Get the response in a specific instructions, styles, or formats.
- Reduce costs by decreasing the number of tokens used, lowering API costs.
- Avoid inappropriate or biased outputs.
- Get consistent and reliable responses across different interactions.
- Improve user experience with more helpful and concise responses.

## FAQ 

<AccordionGroup>
  <Accordion title="How does prompt length affect model responses?">
    The tradeoff is that: 
    - Short prompts can lead to ambiguous or generic answers because of a lack of context.
    - Long prompts provides more detail but can increase token usage and overwhelm the model.

    The optimal balance is aiming for concise prompts that include all necessary information without unnecessary verbosity.
  </Accordion>

  <Accordion title="Can prompt engineering remove biases?">
    Yes. When you carefully crafting prompts to avoid sensitive topics or by instructing the model to follow ethical guidelines, you can reduce the likelihood of biased or inappropriate responses.
  </Accordion>

  <Accordion title="How often should I update my prompts?">
    - Regularly. Especially if you notice changes in the model's performance or after updates to the model. 
    - After use feedback: Incorporate feedback to improve prompt effectiveness.
    - When introducing a new feature: Adjust the prompt to cover new functionalities or use cases.
  </Accordion> 

  <Accordion title="Do I need to be technical to prompt engineer?">
    - For simple prompt adjustment, no extensive technical background is needed.
    - For complex tasks or with the intention to optimize performance, some familiarity with AI concepts is helpful. 
  </Accordion> 

  <Accordion title="What's the difference between prompt engineering vs. finetuning?">
    Prompt engineering is modifying the input prompts to guide the model's responses without changing the model itself. Fine-tuning is training the model on additional data to adjust its internal parameters for specific tasks.
  </Accordion> 

  <Accordion title="Can you cut cost with prompt engineering?">
    Yes. A well-written prompt can minimize the number of tokens required for both the input and output, thereby reducing API usage costs.
  </Accordion> 

  <Accordion title="What are some common mistakes with prompt engineering?">
  - Vague Instructions: Leads to unpredictable outputs.
  - Overcomplicating Prompts: Too much information can confuse the model.
  - Ignoring Model Limitations: Expecting the model to perform tasks beyond its capabilities.
  - Lack of Testing: Not validating prompts with various inputs can result in inconsistent performance.  
  </Accordion> 

</AccordionGroup>

<QuestionsSection />
