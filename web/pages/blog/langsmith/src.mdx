---
title: "A LangSmith Alternative that Takes LLM Observability to the Next Level"
description: "Both Helicone and LangSmith are capable, powerful DevOps platform used by enterprises and developers building LLM applications. But which is better?"
author: "Lina Lam"
date: "Apr 18, 2024"
time: "4 minute read"
icon: "code"
---

![Helicone vs. LangSmith, which is better?](/assets/blog/langsmith-vs-helicone/cover-image.webp)

## Introduction

Both Helicone and LangSmith are capable, powerful DevOps platform used by enterprises and developers to develop, deploy and monitor their LLM applications and gain full visibility into their¬†development. **But which is better?**

With Helicone, the experience of observing and monitoring your LLM is intuitive and integrates well into any LLM observability tech stack. Being a Gateway, we are able to offer caching, prompt threat detection, moderation, vault, rate limiting, customer portal and other useful observability features. As a bonus, integrating with Helicone is as simple as adding two lines of code.

*LangSmith is a great tool and there are some things we would recommend them over Helicone for, such as if you‚Äôre an enterprise that uses LangChain, develops AI agents, or prefers async solutions.*

‚Üí [Try Helicone for Free](https://www.helicone.ai/signup)

---

## Comparing LangSmith and Helicone at a Glance

| Features              | LangSmith | Helicone | 
| :-------------------- | :---------- | :----------- |
| Gateway               | ‚ùå         | ‚úî  
| Dashboards            | ‚úî          | ‚úî 
| Trace logging         | ‚úî          | ‚ùå
| LangChain integration | ‚úî          | ‚úî 
| Caching               | ‚ùå         | ‚úî 
| Open Source           | ‚ùå         | ‚úî 
| Prompts               | ‚ùå         | ‚úî 
| Experiments           | ‚úî          | ‚úî 
| Rate limiting         | ‚ùå          | ‚úî 
| User tracking         | ‚ùå          | ‚úî 
| Vector DB traces      | ‚úî          | ‚ùå
| Flexible pricing      | ‚ùå          | ‚úî  |
| Image support         | ‚ùå          | ‚úî  |
| No payload limitations| ‚ùå          | ‚úî  |

---

## Acting as a Gateway

The biggest difference between LangSmith and Helicone is how we log your data. Helicone is Gateway whereas LangSmith is an async solution. To integrate with Helicone, it‚Äôs as easy as changing the base URL to point to Helicone, and we‚Äôll handle every call you make. As a cherry on top, Helicone exists to fit into any existing tech stack. A minor difference is that LangSmith tracks logs per trace, Helicone tracks logs per request and can support extremely large request bodies. 

![2-line code snippet to integrate with Helicone](/assets/blog/langsmith-vs-helicone/code.png)
 
### Access to Gateway Features

By using Helicone, you get access to [caching](https://docs.helicone.ai/features/advanced-usage/caching), [rate limiting](https://docs.helicone.ai/features/advanced-usage/custom-rate-limits), [API key management](https://docs.helicone.ai/features/advanced-usage/vault), [threat detection](https://docs.helicone.ai/features/advanced-usage/llm-security), [moderations](https://docs.helicone.ai/features/advanced-usage/moderations) and many more. For example, Helicone customers use caching to test and save money by making fewer calls to OpenAI and other models. B2B customers also use us to rate limit their customers and stay compliant by storing OpenAI keys in Helicone vaults. 

### What about latency that comes with being a Gateway? 

We know how much latency matters to our users. We deploy on the edge using Cloudflare Workers to minimize time to response. This adds only ~50 ms from about 95% of the world‚Äôs Internet-connected population (check out [Cloudflare's stats](https://www.cloudflare.com/network/)), for us to bring the additional features and convenience to you. 

Still not sure which one is better? Check out this update on how [Cloudflare selected Helicone as 1 of the 29 startups](https://blog.cloudflare.com/2024-community-update) for the Cloudflare Workers Launchpad this cohort. 

### Just Some Stats

In the last 8 months, Helicone has not had any Gateway incidents with **99.9999% uptime** ‚Äî that‚Äôs pretty good. Whether or not you take that into consideration, we want to give you peace of mind. 

---

## Helicone is Open-source

Helicone is fully open-source and free to start. Companies can also self-host Helicone within their infrastructure. This ensures that you have full control over the application, flexibility and customization tailored to specific business needs.

---

## Which is Cheaper? 

Helicone is also more cost-effective than LangSmith as it operates on a volumetric pricing model. This means companies only pay for what they use, which makes Helicone an easy and flexible platform for businesses to get started and scale their applications. **By the way, the first 100k requests every month are free.**

‚Üí [Find out your cost by usage on Helicone](https://www.helicone.ai/pricing)

---

## Why are companies choosing Helicone over LangSmith? 

Companies that are highly responsive to market changes or opportunities often use Helicone to achieve production quality faster. Helicone simplifies the innovation process, enabling businesses to stay competitive in the fast-paced AI revolution.

Moreover, Helicone can handle a large volume of requests, making it a dependable option for businesses with high traffic. Acting as a Gateway, Helicone offers a suite of both middleware and advanced features such as: (here in cards view + one-liner for each) 

- caching
- prompt threat detection
- moderation
- vault
- rate limiting
- proxy keys
- experiment `advanced` `coming soon`
- fine-tune `advanced` `in beta`

Finally, Helicone places a strong focus on developer experience. Its simple integration, clear pricing coupled with the above features makes Helicone a comprehensive and efficient platform for managing and monitoring your LLM applications. 

---

## Don't Take Our Word for It


<blockquote class="twitter-tweet"><p lang="en" dir="ltr">One of the AI developer tools companies @IrenaCronin and I are talking a lot about is @helicone_ai. Why do I like it? Because I keep hearing of companies building "orchestras" of AI models, think multiple LLMs working against each other, or with each other, partially to find errors. </p>&mdash; Robert Scoble (@Scobleizer) <a href="https://twitter.com/Scobleizer/status/1712000693903733085?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1712000693903733085%7Ctwgr%5E%7Ctwcon%5Es1_&ref_url=notion%3A%2F%2Fwww.notion.so%2Fhelicone%2Faa80460dbd6040768479d967622e1c63%3Fv%3D457c71516b8d473ba09324abf7ff66e5p%3D6922dbe068584c8bbb4b92c6e8964bd3pm%3Ds">October 11, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Big fan of Helicone! It is more than just a logger for LLM requests; for us, it is an IDE for AI and we are using it as much as VS code! If you don't use Helicone yet, you definitely should!!!</p>&mdash; Levan Kvirkvelia (@LevanKvirkvelia) <a href="https://twitter.com/LevanKvirkvelia/status/1778492158984483309?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1778492158984483309%7Ctwgr%5E%7Ctwcon%5Es1_&ref_url=notion%3A%2F%2Fwww.notion.so%2Fhelicone%2Faa80460dbd6040768479d967622e1c63%3Fv%3D457c71516b8d473ba09324abf7ff66e5p%3D6922dbe068584c8bbb4b92c6e8964bd3pm%3Ds">April 11, 2024</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


<blockquote class="twitter-tweet"><p lang="en" dir="ltr">if you are building anything with LLMs and not using 
@helicone_ai you crazy</p>&mdash; austin petersmith (@awwstn) <a href="https://twitter.com/awwstn/status/1771378988734443696?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1771378988734443696%7Ctwgr%5E%7Ctwcon%5Es1_&ref_url=notion%3A%2F%2Fwww.notion.so%2Fhelicone%2Faa80460dbd6040768479d967622e1c63%3Fv%3D457c71516b8d473ba09324abf7ff66e5p%3D6922dbe068584c8bbb4b92c6e8964bd3pm%3Ds">March 22, 2024</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Finally using @helicone_ai in a production app! Currently implementing it on @hyperaide and absolutely loving the experience. It's definitely making my life a lot easier.</p>&mdash; JP (@dqnamo) <a href="https://twitter.com/dqnamo/status/1769756709973819603?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1769756709973819603%7Ctwgr%5E%7Ctwcon%5Es1_&ref_url=notion%3A%2F%2Fwww.notion.so%2Fhelicone%2Faa80460dbd6040768479d967622e1c63%3Fv%3D457c71516b8d473ba09324abf7ff66e5p%3D6922dbe068584c8bbb4b92c6e8964bd3pm%3Ds">March 18, 2024</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Got an LLM app and not using open source @helicone_ai for monitoring? Might as well be driving a sports car with a blindfold on!üòÇ Their free plan is a cheat code minus the guilt! 2 lines of code and bam! They're storing prompt responses, latency, and cost details. Man what a life saver! Their docs are on @mintlify and I just love it!!üî• </p>&mdash; Coffee with One (@coffeewithone) <a href="https://twitter.com/coffeewithone/status/1691218842855538688?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1691218842855538688%7Ctwgr%5E%7Ctwcon%5Es1_&ref_url=notion%3A%2F%2Fwww.notion.so%2Fhelicone%2Faa80460dbd6040768479d967622e1c63%3Fv%3D457c71516b8d473ba09324abf7ff66e5p%3D6922dbe068584c8bbb4b92c6e8964bd3pm%3Ds">August 14, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

---

## Stay Ahead with Helicone. 

‚Üí [Try Helicone for Free](https://www.helicone.ai/signup)

‚Üí [Get in Touch With Us](https://www.helicone.ai/contact)

